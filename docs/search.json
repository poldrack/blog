[
  {
    "objectID": "older/archive.html",
    "href": "older/archive.html",
    "title": "Blog Archive",
    "section": "",
    "text": "{% for tag in site.tags %}\n\n{{ tag[0] }}\n\n\n{% for post in tag[1] %}\n\n{{ post.date | date: “%B %Y” }} - {{ post.title }}\n\n{% endfor %}\n\n{% endfor %}"
  },
  {
    "objectID": "posts/my-workflow-for-writing-papers-or-why-i/index.html",
    "href": "posts/my-workflow-for-writing-papers-or-why-i/index.html",
    "title": "My workflow for writing papers (or, why I switched to LaTeX)",
    "section": "",
    "text": "This was originally posted on blogger here.\nIn the last few years I have changed my workflow for writing papers pretty radically.  Previously, I used Microsoft Word along with Endnote as my primary platform (on the Mac, of course). My decision to change was driven by several factors: I had grown tired of the klunkiness of Endnote and the lags in its integration with new versions of Microsoft Word.  I had grown even more tired of Word’s tendency to crash, or to do crazy things that could only be fixed by starting with a completely new file. I was just starting to work on a book, and I knew that for a large project like that, using Word would be a nightmare. In addition, my coauthors and I wanted to use a source code management system to coordinate changes to the document, and this was not really practical with Word files.In the end, I decided to move to LaTeX as my primary platform for writing papers and books.  For those not familiar with LaTeX, you can think of it as a markup language like HTML, only for writing papers rather than web pages.  Editing a paper in LaTeX is not WYSIWYG - that is, you don’t see the actual layout of the paper as you type.  Rather, you have to typeset the paper in a separate step.  For example, a very short paper might look like this in LaTeX:"
  },
  {
    "objectID": "posts/anatomy-of-coding-error/index.html",
    "href": "posts/anatomy-of-coding-error/index.html",
    "title": "Anatomy of a coding error",
    "section": "",
    "text": "This was originally posted on blogger here.\nA few days ago, one of the students who I collaborate with found a very serious mistake in some code that I had written. The code (which is openly available through my github repo) performed a classification analysis using the data from a number of studies from the openfmri project, and the results are included in a paper that is currently under review. None of us likes to admit mistakes, but it’s clear that they happen often, and the only way to learn from them is to talk about them. This is why I strongly encourage my students to tell me about their mistakes and discuss them in our lab meeting. This particular mistake highlights several important points:Sharing code is good, but only if someone else actually looks at it very closely.You can’t rely on tools to fail when you make a mistake.Classifiers are very good at finding information, even if it’s not the information you had in mind.The code in question is 4_classify_wholebrain.py which reads in the processed data (saved in a numpy file) and classifies each dataset (with about 184K features and 400 observations) into one of 23 different classes (representing different tasks). The code was made publicly available before submitting the paper; while I have no way of knowing whether the reviewers have examined it, it’s fair to say that even if they did, they would most likely not have caught this particular bug unless they were very eagle-eyed. As it happens, a student here was trying to reproduce my analyses independently, and was finding much lower classification accuracies than the ones I had reported. As he dug into my code, it became clear that this difference was driven by a (lazy, in hindsight) coding mistake on my part.The original code can be viewed here - the snippet in question (cleaned up a bit) is:skf=StratifiedKFold(labels,8)if trainsvm: pred=N.zeros(len(labels)) for train,test in skf: clf=LinearSVC() clf.fit(data[train],labels[train]) pred[test]=clf.predict(data[test])Pretty simple - it creates a crossvalidation object using sklearn, then loops through, fitting to the train folds and computing the predicted class for the test fold. Running this, I got about 93% test accuracy on the multiclass problem; had I gotten 100% accuracy I would have been sure that there was a problem, but given that we have previously gotten around 80% for similar problems, I was not terribly shocked by the high accuracy. Here is the problem: In [9]: data.shapeOut[9]: (182609, 400)When I put the data into the numpy object, I had voxels as the first dimension, whereas for classification analysis one would usually put the observations in rows rather than columns. Now, numpy is smart enough that when I give it the train list as an array index, it uses it as an index on the first dimension. However, because of the transposition of the dimensions in the data, the effect was to classify voxels, rather than subjects:In [10]: data[train].shapeOut[10]: (350, 400)In [11]: data[test].shapeOut[11]: (50, 400)When I fix this by using the proper data reference (as in the current revision of the code on the repo), then it looks as it should (i.e. all voxels included for the subjects in the train or test folds):In [12]: data[:,train].T.shapeOut[12]: (350, 182609)In [14]: data[:,test].T.shapeOut[14]: (50, 182609)When I run this with the fixed code I get about 53% accuracy; still well above chance (remember that it’s a 23-class problem), but much less than the 93% we had gotten previously.It’s worth noting that randomization tests with the flawed code showed the expected null distribution; the source of the information being used by the classifier is a bit of a mystery, but likely reflects the fact that the distance of the voxels in the matrix is related to their distance in space in the brain, and the labels were grouped together sequentially in the label file, such that they were correlated with physical distance in the brain and thus provided information that could drive the classification.This is clearly a worst-case scenario for anyone who codes up their own analyses; the paper has already been submitted and you find an error that greatly changes the results. Fortunately, the exact level of classification accuracy is not central to the paper in question, but it’s worrisome nonetheless.What are the lessons to be learned here? Most concretely, it’s important to check the size of data structures whenever you are slicing arrays. I was lazy in my coding of the crossvalidation loop, and I should have checked that the size of the dataset being fed into the classifier was what I expected it to be (the difference between 400 and 182609 would be pretty obvious). It might have added an extra 30 seconds to my initial coding time but would have saved me from a huge headache and hours of time needed to rerun all of the analyses.Second, sharing code is necessary but not sufficient for finding problems. Someone could have grabbed my code and gotten exactly the same results that I got; only if they looked at the shape of the sliced arrays would they have noticed a problem. I am becoming increasingly convinced that if you really want to believe a computational result, the strongest way to do that is to have an independent person try to replicate it without using your shared code. Failing that, one really wants to have a validation dataset that one can feed into the program where you know exactly what the output should be; randomization of labels is one way of doing this (i.e., where the outcome should be chance) but you also want to do this with real signal as well. Unfortunately this is not trivial for the kinds of analyses that we do, but perhaps some better data simulators would help make it easier.Finally, there is a meta-point about talking openly about these kinds of errors. We know that they happen all the time, yet few people ever talk openly about their errors. I hope that others will take my lead in talking openly about errors they have made so that people can learn from them and be more motivated to spend the extra time to write robust code."
  },
  {
    "objectID": "posts/anatomy-of-coding-error/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/anatomy-of-coding-error/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Anatomy of a coding error",
    "section": "13 comments captured from original post on Blogger",
    "text": "13 comments captured from original post on Blogger\npractiCal fMRI said on 2013-02-20\nI have a draft blog post gathering e-dust that bears the working title, \"Whose job is method validation, anyway?\" It was motivated by the acquisition end of the fMRI pipeline - my concern - but the issues seem to pervade the entire operation.One of my points is (er, will be) that the party responsible for creating a widget isn’t necessarily the one who should be responsible for validating it. Indeed, one can make a strong case for separating the validation from the production because of conflicts of interest, over familiarity, etc. In my post I’ll be pointing a lot of fingers. My contention is that we all bear a part of the responsibility, no-one less than the person who takes the widget you’ve made and uses it without first determining what (independent) validation has been performed.So, I congratulate you on your mea culpa and I hope that it motivates serious (re)consideration of how we use the tools we have at our disposal in neuroimaging. Because no lives are at stake we can be rather lax when it comes to insisting on, or checking out, the validation of our methods. Bad, bad, bad!\nRuss Poldrack said on 2013-02-20\nThanks Ben - I’ll look forward to your post!\nStokesBlog said on 2013-02-21\nHi Russ,Thanks for the important post - I sometimes feel like errors like this might be a pretty serious source of false positives in the literature. I too have a draft blog post gathering dust (titled: \"Biased Debugging\"), which considers how the debugging process is non-random, biased in favour of positive and sensible-looking results. If an error throws up a crazy (or non-significant result), we are more likely to track it down than if it fits nicely with our hopes and expectations. Given the increasing complexity data analyses, it is naive to think that the published data are free of coding errors. If they are random, then the additional noise is probably harmless enough. But if they are not random, and biased in favour of positive findings, then perhaps biased debugging is a serious contributor to the statistical bias that deserves more attention. As you say, sharing code is not always sufficient (though certainly better for eventual detection than not sharing). Only independent verification using fresh code is 100% safe. Incidentally, just yesterday I asked my graduate student to code up his own version of mahalanobis distance for MEG rather than use mine, just to make sure that our results converged… Your other point is also crucial - we need to be prepared to admit our mistakes. This is perhaps especially important for graduate students. If students do not feel that they are able to admit their mistakes to their supervisors, then there is little chance for redress. Errors like these should not be seen as a sign of failure, in fact more likely the opposite. We should be encouraged to try new things, to write custom scripts, implement new analytic procedures, and innovation is always going to be error prone. Perfect coding is as unrealistic as perfect prose - just think of the grammatical errors that sneak through endless drafts seen by all co-authors. Then think, whoever goes through thousands of lines of matlab code as carefully as the manuscript being prepared for submission!? \nRuss Poldrack said on 2013-02-21\nMark - very good point regarding the biases that can emerge from debugging. Hope to see your full post soon!\nMichael Frank said on 2013-02-22\nHi Russ, This is great! I completely agree about code replication. It’s amazing how easy it is to catch errors that go in the wrong direction (that decrease your results) and how hard it is to catch those that go in the right direction. Double-entry (recoding from scratch) really seems like the gold standard. Do you have thoughts about practices that help you avoid errors? It’s really important to produce reliable code but at the same time it seems cripplingly slow to implement unit tests for every analysis you do. So I have struggled with what concrete recommendations to make to students and how to implement error checking in my own coding practice… best, Mike Frank\nRuss Poldrack said on 2013-02-22\nHey Mike - thanks for your comments. I agree that it’s really hard to come up with general recommendations without going down the slippery slope towards full-blown unit testing. I’ll think a bit more and maybe put together another post with some of the major places where I think problems are most likely.\nRuss Poldrack said on 2013-02-22\nhi Michael - I think you are right that refactoring into a more general library would be one useful way to address this issue. I am getting better at that, but old habits still die hard.\nUnknown said on 2013-02-22\nBy the way, it just occurred to me to point out that the Github Pull Request interface is probably the best solution for collaborative code review. Here’s an example from the scikit-learn repo: https://github.com/scikit-learn/scikit-learn/pull/1628 that I think exemplifies pull requests at their best. You can comment at varying levels of granularity down to specific lines, code is updated live as changes are made, and it provides a public record that’s wrapped up together with the code itself. mw\nsatra said on 2013-02-23\nthanks for sharing this. validation and testing are two of the least thought about areas in the brain imaging community and that needs to change. michael hanke is leading a project called testkraut, an effort to address testing concerns for workflows and software. also, our current publication life-cycle/peer review/culture needs to address reproducibility as a core principle. i would rather see half the number of publications from a lab as long as each publication was independently verified on a different dataset. this may not only addresses the possibility of code error but would also provide an additional stamp on the veracity of the result.\nVadim said on 2013-02-24\nThe smarter the programming language the more cautious one should be. All those implicit conversions can be extremely dangerous (also in C++). Matlab I think is better in this extent. The striking thing which I think is missing completely in academic coding comparing to software industry is a code peer-review. In many industry companies you just cannot commit a code to source-control system without additional pair of eyes look at your code. It is amazing how many bugs one can catch when he just explains someone what he just programmed. So, basically, in many cases it can be even a doll and not a peer :)\nRuss Poldrack said on 2013-02-24\nThat’s a great point about code review - we have started doing this in my lab (we have the luxury of a full time software developer on staff). Perhaps I should go out and buy a bunch of dolls for the lab… :-)\nStokesBlog said on 2013-02-24\nAs promised, I managed to dust off the old draft at the Brain Box: http://the-brain-box.blogspot.co.uk/2013/02/biased-debugging.html\nRuss Poldrack said on 2013-02-25\nHere is another recent example where what looks like a very small error (using int instead of float type for a specific variable) resulted in unreasonably high accuracy on the Netflix prediction problem - http://nuit-blanche.blogspot.fr/2013/02/this-weeks-guardians-of-science-zeno.html"
  },
  {
    "objectID": "posts/having-my-cake-and-eating-it-too/index.html",
    "href": "posts/having-my-cake-and-eating-it-too/index.html",
    "title": "Having my cake and eating it too?",
    "section": "",
    "text": "This was originally posted on blogger here.\nSeveral years agoI blogged about some of the challenges around doing science in a field with emerging methodological standards. Today, a person going by the handle “Student” posted a set of pointed questions to this post, which I am choosing to respond to here as a new post rather than burying them in the comments on the previous post. Here are the comments:Dr. Poldrack has been at the forefront of advocating for increased rigor and reproducibility in neuroimaging and cognitive neuroscience. This paper provides many useful pieces of advice concerning the reporting of fMRI studies, and my comments are related to this paper and to other papers published by Dr. Poldrack. One of the sections in this paper deals specifically with the reporting of methods and associated parameters related to the control of type I error across multiple tests. In this section, Dr. Poldrack and colleagues write that “When cluster-based inference is used, this should be clearly noted and both the threshold used to create the clusters and the threshold for cluster size should be reported”. I strongly agree with this sentiment, but find it frustrating that in later papers, Dr. Poldrack seemingly disregards his own advice with regard to the reporting of extent thresholds, opting to report only that data were cluster-corrected at P&lt;0.05 (e.g. http://cercor.oxfordjournals.org/content/20/3/524.long, http://cercor.oxfordjournals.org/cgi/content/abstract/18/8/1923, http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2876211/). In another paper (http://www.ncbi.nlm.nih.gov/pmc/articles/pmid/19915091/), the methods report that “Z (Gaussianised T ) statistic images were thresholded using cluster-corrected statistics with a height threshold of Z &gt; 2.3 (unless otherwise noted) and a cluster probability threshold of P &lt; 0.05, whole- brain corrected using the theory of Gaussian random fields”, although every figure presented in the paper notes that the statistical maps shown were thresholded at Z&gt;1.96, P&lt;0.05, corrected. This last instance is particularly confusing, and borders on being misleading. While these are arguably minor omissions, I find it odd that I am thus far unable to find a paper where Dr. Poldrack actually follows his own advice here. In another opinion paper regarding fMRI analyses and reporting (http://www.ncbi.nlm.nih.gov/pubmed/21856431), Dr. Poldrack states “Some simple methodological improvements could make a big difference. First, the field needs to agree that inference based on uncorrected statistical results is not acceptable (cf. Bennett et al., 2009). Many researchers have digested this important fact, but it is still common to see results presented at thresholds such as uncorrected p&lt;.005. Because such uncorrected thresholds do not adapt to the data (e.g., the number of voxels tests or their spatial smoothness), they are certain to be invalid in almost every situation (potentially being either overly liberal or overly conservative).” This is a good point, but given the fact that Dr. Poldrack has published papers in high impact journals that rely heavily on inferences from data using uncorrected thresholds (e.g. http://www.ncbi.nlm.nih.gov/pubmed/16157284), and does not appear to have issued any statements to the journals regarding their validity, one wonders whether Dr. Poldrack wants to have his cake and eat it too, so to say. A similar point can be made regarding Dr. Poldrack’s attitude regarding the use of small volume correction. In this paper, he states “Second, I have become increasingly concerned about the use of “small volume corrections” to address the multiple testing problem. The use of a priori masks to constrain statistical testing is perfectly legitimate, but one often gets the feeling that the masks used for small volume correction were chosen after seeing the initial results (perhaps after a whole-brain corrected analysis was not significant). In such a case, any inferences based on these corrections are circular and the statistics are useless”. While this is also true, one wonders whether Dr. Poldrack only trusts his group to use this tool correctly, since it is frequently employed in his papers. In a third opinion paper (http://www.ncbi.nlm.nih.gov/pubmed/20571517), Dr. Poldrack discusses the problem of circularity in fMRI analyses. While this is also an important topic, Dr. Poldrack’s group has also published papers using circular analyses (e.g. http://www.jneurosci.org/content/27/14/3743.full.pdf, http://www.jneurosci.org/content/26/9/2424, http://www.ncbi.nlm.nih.gov/pubmed/17255512). I would like to note that the reason for this comment is not to malign Dr. Poldrack or his research, but rather to attempt to clarify Dr. Poldrack’s opinion of how others should view his previous research when it fails to meet the rigorous standards that he persistently endorses. I am very much in agreement with Dr. Poldrack that rigorous methodology and transparency are important foundations for building a strong science. As a graduate student, it is frustrating to see high-profile scientists such as Dr. Poldrack call for increased methodological rigor by new researchers (typically while, rightfully, labeling work that does not meet methodological standards as being unreliable) when they (1) have benefited (and arguably continue to benefit) from the relatively lower barriers to entry that come from having entered a research field before the emergence of a rigid methodological framework (i.e. in having Neuron/PNAS/Science papers on their CV that would not be published in a low-tier journal today due to their methodological problems) , and (2) not applying the same level of criticism or skepticism to their own previous work as they do to emerging work when it does not meet current standards of rigor or transparency. I would like to know what Dr. Poldrack’s opinions are on these issues. I greatly appreciate any time and/or effort spent reading and/or replying to this comment. I appreciate these comments, and in fact I have been struggling with exactly these same issues myself, and my realizations about the shortcomings of our past approaches to fMRI analysis have shaken me deeply. Student is exactly right that I have been a coauthor on papers using methods or reporting standards that I now publicly claim to be inappropriate. S/he is also right that my career has benefited substantially from papers published in high profile journals prior using these methods that I now claim to inappropriate. I’m not going to either defend or denounce the specific papers that the commentator mentions. I am in agreement that some of my papers in the past used methods or standards that we would now find problematic, but I am actually heartened by that: If we were still satisfied with the same methods that we had been using 15 years ago, then that would suggest that our science had not progressed very far. Some of those results have been replicated (at least conceptually), which is also heartening, but that’s not really a defense.I also appreciate Student’s frustration with the fact that someone like myself can become prominent doing studies that are seemingly lacking according to today’s standards, but then criticize the field for doing the same thing. But at the same time I would ask: Is there a better alternative? Would you rather that I defended those older techniques just because they were the basis for my career? Should I lose my position in the field because I followed what we thought were best practices at the time but which turned out to be flawed? Alternatively, should I spend my entire career re-analyzing my old datasets to make sure that my previous claims withstand every new methodological development? My answer to these questions has been to try to use the best methods I can, and to to be as open and transparent as possible. Here I’d like to outline a few of the ways in which we have tried to do better.First, I would note that if someone wishes to look back at the data from our previous studies and reanalyze them, almost all of them are available openly through openfmri.org, and in fact some of them have been the basis for previous analyses of reproducibility. I and my lab have also spend a good deal of time and effort advocating for and supporting data sharing by other labs, because we think that ultimately this is one of the best ways to address questions about reproducibility (as I discussed in the recent piece by Greg Miller in Science).Second, we have done our best to weed out questionable research practices and p-hacking. I have become increasingly convinced regarding the utility of pre-registration, and I am now committed to pre-registering every new study that our lab does (starting with our first registration committed this week). We are also moving towards the standard use of discovery and validation samples for all of our future studies, to ensure that any results we report are replicable. This is challenging due to the cost of fMRI studies, and it means that we will probably do less science, but that’s part of the bargain.Third, we have done our best to share everything. For example, in the MyConnectome study, we shared the entire raw dataset, as well as putting an immense amount of working into sharing a reproducible analysis workflow. Similarly, we now put all of our analysis code online upon publication, if not earlier. None of this is a guarantee, and I’m almost certain that in 20 years, either a very gray (and probably much more crotchety) version of myself or someone else will come along and tell us why the analyses were we doing in 2016 were wrong in some way that seems completely obvious in hindsight. That’s not something that I will get defensive about because it means that we are progressing as a science. But it also doesn’t mean that we weren’t justified to do what we are doing now, trying to follow the best practices that we know how."
  },
  {
    "objectID": "posts/having-my-cake-and-eating-it-too/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/having-my-cake-and-eating-it-too/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Having my cake and eating it too?",
    "section": "13 comments captured from original post on Blogger",
    "text": "13 comments captured from original post on Blogger\nBrad Wyble said on 2016-07-22\nI think the student has some excellent observations, particularly with regard to the fact that science has gotten harder (which is great it should be hard), but the goalposts haven’t moved in terms of getting a job (which is both terrible and hypocritical). Those of us on search committees need to put the new normal into practice by lowering our expectation of the number of publications, and also increasing our level of scrutiny of papers on the shortlist so that papers with less rigorous methods and unreplicated results are downweighted, regardless of impact factor. The students need to feel confident that we have their back. If they are willing to take the high road and come out of a PhD with only 2-3 pubs that are solid exemplars of how science should be done, they need to know that we’ll consider that as a highly competitive job application relative to someone with 8 pubs that are a bit sloppier. We also need to remind ourselves that there is a cost to doing science better. Doing science correctly IS harder, which means more hours in the lab and/or less publications. Transparent practices can help offset the cost of better science by improving research efficiency, and we should embrace those improvements. However they won’t completely offset those costs. Let’s all be realistic about what we’re asking for and expecting.\ndaniele said on 2016-07-22\nI think that there’s still too much divide between methods/opinion papers, and papers based on data collection and research. In the latter case there’s still a lot of good hypotheses and nice data, but low standard methods. These studies, by leading researchers and institutions, still contribute to build a suboptimal status quo and the wrong example. I saw a lot of that at the latest ohbm. Daniele Marinazzo\nPeterK said on 2016-07-22\nI think the Student’s letter bring good points. In many ways it teaches that \"dogmas\" in science can be just as bad as dogmas in religion. In other words, the bad vs. good science is decided not upon the dogmatism of statistic approach but rather on what stood the test of time and what failed. I am not at all convinced that the \"dogmas of good science\" will improve the science. Instead, the evolutionary approach should decided on what withstood the tests of time and what failed. In many way, what this field needs is more attention given to negative findings reports where sufficiently powered samples were used to show that one or many previous findings were not replicable, thus setting the precedent for striking them out.\nStudent said on 2016-07-22\nDr. Poldrack, thank you for taking the time to address my letter. I understand that science is in a constant state of progress, and so it is not reasonable to expect that any study will be \"perfect\" or 100% compliant with contemporary standards in terms of methodology/design/analyses/etc. in part because standards change as the field progresses and in part because doing science is an imperfect art. I think that even poorly done/methodologically deficient studies can provide important insights and sometimes uncover robust and important phenomena, but also agree that they are more prone to biases and artifacts that may lead to incorrect conclusions. I am at the early stages of my career, and I am sure that I will also one day look back on studies that I have done and wish I would have done things differently or \"more optimally\" (to be honest, I feel this way already sometimes). I understand that we (as scientists) are all playing a game where not only do the methodological/analytical rules change over time, but the evaluative rules and scoring work against what, in my opinion, should be our ultimate goal, which is to develop robust theories that robustly explain and predict natural (in our case, neural and by extension behavioral) phenomena. So, I understand that there is no clear answer regarding \"what to do\" about older studies that may not be as reliable as they were thought to be when they were published. However, I know that this is something that many people wonder about in lab meetings and private conversations, and think that public discussion about the difficulties of dealing with such things is helpful to moving the field forward. Regarding some of the comments, I do think that one major obstacle facing early career researchers is navigating the perception that many things are \"facts\" and not worth \"re-studying\" because the results (or just a specific interpretation of the results) of a single small study have been cited so frequently throughout the literature that they are not questioned (as a note, there are several such \"citation trails\" that I have personally followed only to discover that no one ever actually found the the thing that the citations are provided to support). I think that this, in concert with (1) the moving of career/grant goalposts in a way that ultimately makes reaching them more difficult without cutting corners, overselling one’s results, or focusing on being the first to do something using a \"hot\" new technique, (2) a focus on developing a literature of novel findings and compelling narratives, and (3) a reluctance to dedicate resources to scrutinizing/verifying long-held dogmas are at least as much of a threat to developing a deep understanding of neurobiology and behavior as deficient methods. I am grateful to Dr. Poldrack and other commenters for entertaining my ideas and observations and responding to them in an understanding and respectful way. I was worried after I wrote my initial letter that it would be perceived as disrespectful or accusatory, and I am happy that it does not seem to have been the case.\nRuss Poldrack said on 2016-07-22\nAll very good points. I think it’s particularly good to point out that even a study with horrible methods is not necessarily false. Our 1999 Neuroimage paper on semantic and phonological processing in the inferior frontal cortex (also my first fmri study) used methods that we would now find abysmal (eight subjects, fixed effects analysis, weak ad hoc correction for multiple comparisons, manual registration of slices to the Talairach atlas) but the results inspired a meta-analysis that confirmed the result, and subsequent studies have also confirmed it. Then again, a broken clock is also right once a day, and it’s impossible to know whether we just got lucky on this one - mostly this just highlights that we really should not be concluding much at all on the basis of any single paper. Unfortunately, there is not enough replication in our field for us to really know how to judge many findings are replicable, so the main way we can achieve cumulative knowledge is through tools like Neurosynth and Neurovault.\nStudent said on 2016-07-22\nAlso, I would like to thank Dr. Poldrack for his efforts to increase transparency and data-sharing regarding his own research. The dedication of time and resources to things such as these, despite their lack of what might be called career-boosting potential, is something I greatly respect and appreciate.\nRuss Poldrack said on 2016-07-22\nmany thanks! I’m glad we have been able to have such a productive discussion over this topic.\nStudent said on 2016-07-22\nThis is very true. I don’t think that a study with horrible methods should be the basis for strong conclusions (and I think everyone agrees with that), but I am aware of many such studies that nonetheless uncovered reliable and important effects. I would also note that the attention payed to describing the how and why of the relevant psychological/behavioral manipulations in papers such as that one is noticeably greater than many similarly oriented contemporary studies.\nDick_Retired said on 2016-07-22\nI am also guilty of having published many papers in the past which were not up to the standards we now expect in terms of the number of subjects, anatomical methods and statistical rigour. But that doesn’t mean that the results were false, only that they need replication. And indeed we repeated some of the important studies to check if they could be replicated. For example, we did a motor sequence learning study using PET in 1994, repeated it with PET a few years later, then repeated it again with fMRI. The last study used 3 subjects (!!!) because at the time our computers could only handle continuous acquisition for 45 minutes with 3 subjects. We then did a related study using PET, but with rhythm learning. Then Floyer-Lea and Matthews did a very similar motor sequence study (sequence of forces) and lo and behold much the same results. This is not an argument for inadequate studies, only an argument that if you believe in your results (on inadequate data) it is in your own hands to check them yourself with repeated studies\nColleen said on 2016-07-22\nI got into neuroimaging around the same time many of the publications linked in Student’s comments came out, and I also look back on the \"state of the art\" at that time with a mix of bemusement and horror. I expect to do the same in another 10 years with the methods I use now. In an odd way this is a sign of immense progress. When you compare the statistical standards of fields like genomics to neuroimaging, we have a long way to go - but we have also made incredible iterative progress since the early 90s. I think this is true of nearly any \"new\" (&lt;30 year old) field. I think the biggest adjustment that remains is, as others have pointed out, for search committees to appropriately weight quality vs. quantity in publications for neuroimagers. That might include considering \"rigor and reproducibility\" (as NIH has recently begun to emphasize) such as work that may take longer and produce fewer papers in order to include collection and analysis of replication samples.\nDick_Retired said on 2016-07-22\nI am all for advocating rigour in science. But don’t let’s forget that in the end what matters is how important the result is. We should be focussing on creativity at the same time as experimental care, and creativity involves exploration. So don’t let be so hung up on rigour that we cease to explore.\nRuss Poldrack said on 2016-07-23\nI think that’s a very important point, Dick. John Ioannidis made the point strikingly in http://onlinelibrary.wiley.com/doi/10.1111/add.12720/full: \"I would loathe seeing a ‘perfect’ scientific literature where everything is pre-registered, all checklist items are checked and papers are written by robotic automata before the research is conducted, but no real progress is made. We need to find ways to improve science without destroying it.\" One problem is that the current publication system forces people to present their results as if they were hypothesis-driven when really they were exploratory (i.e HARKing).\nflyingfox said on 2016-08-01\nIndeed. It is very easy to get caught up in \"the game\" that we forget our purpose. My colleague and I have been rather fortunate to made some striking observations while analyzing standard rsfmri data because we followed our noses so to speak. The fact that you can discover something new in an experiment conducted numerous times, if you look, does bring a smile to my face…..which disappears quickly enough when thinking about the publication process …"
  },
  {
    "objectID": "posts/why-preregistration-no-longer-makes-me/index.html",
    "href": "posts/why-preregistration-no-longer-makes-me/index.html",
    "title": "Why preregistration no longer makes me nervous",
    "section": "",
    "text": "This was originally posted on blogger here.\nIn a recent presidential column in the APS Observer, Susan Goldin-Meadow lays out her concerns about preregistration. She has two main concerns:The first is the fear that preregistration will stifle discovery. Science isn’t just about testing hypotheses — it’s also about discovering hypotheses grounded in phenomena that are worthy of study. Aren’t we supposed to let the data guide us in our exploration? How can we make new discoveries if our studies need to be catalogued before they are run? The second concern is that preregistration seems like it applies only to certain types of studies — experimental studies done in the lab under controlled conditions. What about observational research, field research, and research with uncommon participants, to name just a few that might not fit neatly into the preregistration script?She makes the argument that there are two stages of scientific practice, and that pre-registration is only appropriate for one of them:The first stage is devoted to discovering phenomena, describing them appropriately (i.e., figuring out which aspects of the phenomenon define it and are essential to it), and exploring the robustness and generality of the phenomenon. Only after this step has been taken (and it is not a trivial one) should we move on to exploring causal factors — mechanisms that precede the phenomenon and are involved in bringing it about, and functions that follow the phenomenon and lead to its recurrence….Preregistration is appropriate for Stage 2 hypothesis-testing studies, but it is hard to reconcile with Stage 1 discovery studies.I must admit that I started out with exactly the same concerns about pre-registration. I was worried that it would stifle discovery, and lead to turnkey science that would never tell us anything new. However, I no longer believe that. It’s become clear to me that pre-registration is just as useful at the discovery phase as at the hypothesis-testing phase, because it helps keep us from fooling ourselves. For discovery studies, we have adopted a strategy of pre-registering whatever details we can; in some cases this might just be the sample size, sampling strategy, and the main outcome of interest. In these cases we will almost certainly do analyses beyond these, but having pre-registered these details gives us and others more faith in the results from the planned analyses; it also helps us more clearly distinguish between a priori and ad hoc analysis decisions (i.e., we can’t tell ourselves “we would have planned to do that analysis”); if it’s not pre-registered, then it’s treated through the lens of discovery, and thus not really believed until it’s replicated or otherwise validated. In the future, in our publications we will be very clear about which results arose from pre-registered analyses and which were unplanned discovery analyses; I am hopeful that by helping more clearly distinguish between these two kinds of analyses, the move to pre-registration will make all of our science better.I would also argue that the phase of “exploring the robustness and generality of the phenomenon”, which Goldin-Meadow assigns to the unregistered discovery phase, is exactly the phase in which pre-registration is most important. Imagine how many hours of graduate student time and gallons of tears could have been saved if this strategy had been used in the initial studies of ego depletion or facial feedback. In our lab, it is now standard to perform a pre-registered replication before we believe any new behavioral phenomenon; it’s been interesting to see how many of them fall by the wayside. In some cases we simply can’t do a replication due to the size or nature of the study; in these cases, we register whatever we can up front, and we try to reserve a separate validation dataset for testing of whatever results come from our initial discovery set. You can see an example of this in our recent online study of self-regulation.I’m glad that this discussion is going on in the open, because I think a lot of my colleagues in the field share concerns similar to those expressed by Goldin-Meadow. I hope that the examples of many successful labs now using pre-registration will help convince them that it really is a road to better science."
  },
  {
    "objectID": "posts/why-preregistration-no-longer-makes-me/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/why-preregistration-no-longer-makes-me/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Why preregistration no longer makes me nervous",
    "section": "6 comments captured from original post on Blogger",
    "text": "6 comments captured from original post on Blogger\nUnknown said on 2016-09-01\nWell said. This expresses my thoughts on this pretty much exactly.\nAnon_Postodoc said on 2016-09-01\nSo if I’m working from an extant dataset, what are my pre-registration options? It would seem the \"best\" pre-registration option is to do a Registered Report and have everything peer-reviewed prior to actually running the planned analyses. But many of the journals in the cognitive neuroscience field simply do not allow registered reports for extant databases. What am I to do if I’m working from a publicly available or other existing dataset, and I want to pre-register my analyses?\nRuss Poldrack said on 2016-09-01\nyou can still pre-register your analyses that you plan to do on the public dataset. you won’t be able to do a registered report, but you will still have the ability to distinguish between planned and discovery analyses. here is an example: https://osf.io/wsnjb/\nAnon_Postodoc said on 2016-09-01\nThanks for the example! So then do you reference the registration document in the (eventual) final paper? Or is it just to have a time-stamped record somewhere on the internet for people to see? I understand the broader point, just not clear if putting something up on OSF or other forum is to have a public document out there somewhere to keep the research team honest, or if there’s some additional benefits at peer review, etc. Sorry if this comes off as a bit dense, just trying to get a sense of the practical mechanics/benefits of pre-registration for these situations.\nRuss Poldrack said on 2016-09-01\nWe will certainly point to the registration when we write up the work. you are right that there is nothing that keeps someone from doing some analyses and then dishonestly \"pre\"-registering them, but I think we have to work on the assumption that our colleagues are fundamentally honest, otherwise all bets are off. It’s also not clear to us yet whether there will be actual benefits (or costs, for that matter) in the review process - we have not yet submitted anything that was tied to a registration. So we are just as in the dark as you are!\nDr_Rick said on 2016-09-02\nIsn’t pre registration, then, exactly what we teach our PhD candidates by having them write up a dissertation proposal before actually doing the dissertation research?"
  },
  {
    "objectID": "posts/moving-from-mac-to-linux-finally/index.html",
    "href": "posts/moving-from-mac-to-linux-finally/index.html",
    "title": "Making the move from Mac to Linux on the desktop - finally!",
    "section": "",
    "text": "I have long toyed with the idea of moving to Linux as my desktop environment. I’ve used Linux on servers for several decades, but I have never been able to make the move from Mac to Linux for my main desktop system; I’ve tried several times, but each time there were just too many pain points for it to stick. Perhaps the main pain point has been that all of my talks have been developed using Keynote on the Mac, and committing to move all of them to a more Linux-friendly platform just seemed like a bridge too far.\nI apologize in advance for the length of this post - the TL;DR is that it’s been a remarkably smooth move so far.\n\nWhy leave the Mac?\nWhen OS X came out I was ecstatic that I could finally have a UNIX back end with a usable GUI. However, several changes over the last few years have left me increasingly annoyed with how Apple treats its power users.\n\nThe user interface changes regularly, in ways that seem to be trying to look more like iOS but ultimately just degreade the user experience. I am not the first person to point this out.. For example, there are two different file saving workflows across different applications, such that in some apps the “Save As” button is not available (at least not obviously).\n\nThe security model causes friction by blocking attempts to install any non-reviewed software, and with excessive requirements to approve many different services required by software, requiring many trips to the Security preferences panel.\nSome things have just stopped working on the M1 platform, in particular all of the virtual machine apps for OSX on M1 seem to be broken to some degree or another.\n\nAnother thing that has changed since the last time I tried to make the move is that nearly all of the major pieces of software that I rely upon for daily producitivity are now either available natively on Linux (e.g. 1Password, Slack, Zoom) or available as web services. While I still don’t want to move all of my work to cloud services, the availability of Keynote via iCloud and Word via the Office365 web portal was enough to ensure that I wouldn’t be completely lost if I needed to get back to them.\nA third and very important change that emboldened me to move away from the Mac was my recent move to a Markdown-based workflow for presentations, as I described in more detail here, which freed me from the Keynote tether.\nFinally, and foremost, as an advocate of open science and open source I felt like I finally needed to walk the walk.\n\n\nGetting started\nGiven all this, I decided to take the plunge and buy a Linux laptop that could serve as my main productivity system. I usually carry a laptop back and forth between my home office and main office; In each office I have a USB-C monitor, bluetooth keyboard and mouse, and a high-quality microphone for Zoom calls and recordings. Because I carry the laptop everywhere I wanted something relatively light, with a 14” monitor and lots of USB-C ports. I also wanted to get a system with Linux pre-installed, since I figured that would reduce the number of hardware-related hiccups. After some searching, I decided to go for the Lenovo ThinkPad X1 with Ubuntu Linux pre-installed.\nWhen the system arrived, the first thing I did was to hand it off to the Stanford IT team, who set up encryption for me (as required by Stanford). I generally like doing my own system administration, but setting up encryption on Linux is an ugly process that I didn’t want to mess with, so I was happy that they agreed to do it for me. A few days later the system was ready for me to start using it.\nI went for the latest Ubuntu LTS version (22.04) but after hitting an early hitch with sound on my external monitor I decided to upgrade the kernel to the latest release of Version 5. This was super easy using the mainline installer, and it seems to have served me well (so far).\n\n\nPlug and pray\nWhen I had previously toyed with Linux on the desktop I found that many of my devices just didn’t work. The experience this time around has been dramatically different: Pretty much everything has just worked! This includes the following devices that worked with no added drivers:\n\nDell USB-C monitor and soundbar\nAudio-Technica ATR-2100X and Shure MV-7 USB-C microphones\nPanasonic Lumix G7 camera (used as a webcam via the Elegato CamLink 4K)\n\nSome additional items worked but required additional drivers or tweaks:\n\nLG USB-C monitor/camera/speaker (required kernel upgrade for sound to work properly)\nEpson FastFoto scanner (required Epson drivers and tweaking of settings file to insert the Wifi IP address for the scanner)\nLogitech Bluetooth keyboard and mouse (required some manual tweaking of bluetooth and USB settings to prevent lagging)\n\nWithin a couple of days I had a fully working setup that I was using in place of my Mac.\n\n\nProductivity stack\nA number of my producitivity apps (Slack, 1Password, Notion, Todoist, Joplin, VSCode, RStudio) have native versions that worked straight out of the box, but in some cases I had to make tweaks for existing parts of my productivity stack to work, or find replacements. Since much of my stack relies upon web apps (Gmail, Google Docs, etc), I will focus on those parts that used apps on the Mac for which I needed an alternative, or required tweaks to work.\nZoom. Zoom worked pretty much straight out of the box, but the screen sharing options are limited when using the default Wayland window server, so I changed my window server from Wayland to XOrg. Since then I’ve had numerous successful Zoom calls with no problems yet (fingers crossed…)\nPresentations. As I noted above, I recently made the move to Markdown-based presentations. To the degree that I still need to access my older Keynote presentations (which I’m sure I will for a while), it turns out that they can be opened using LibreOffice Impress (with some disruption of the slide layout, on par with converting from Keynote to Powerpoint). Discovering this was a total game-changer, as it means I don’t have to move all of my keynote files into iCloud to use the web version of Keynote.\nWord. It’s impossible to get away from Microsoft Word files. I have installed LibreOffice though I don’t yet know how good this will be as a Word replacement. As a backup I have the Office365.com web-based version of Word through my university. It will remain to be seen which of those turns out to be more useful.\nLetter-writing. One of my main uses for Word in the past was to write letters of recommendation/support on my Stanford letterhead. I decided to try out a Markdown-based approach for this as well, adapting an approach that uses a LaTeX template with Pandoc to render a letter created in Markdown. This seems to work well so far; once I clean up my code I will post it to my Github account, as others might find it useful.\nSigning PDFs. I generally used Preview to work witih PDFs on the Mac, and a common use case was adding an image with my signature. LibreOffice Draw seems to work OK for this though I’ve already encountered one PDF that it garbled; fortunately in that case I was able to use the Acrobat Web Client to add the signature (through my Creative Cloud subscription).\nScreenshots. I take a lot of screenshots, and the MacOS Shift-Command-4 hotkey was very useful to be able to take a shot of a particular part of the screen. However, there is a really nice command-line replacement in Linux, which is the import command from the ImageMagick toolbox. This command saves a screenshot of a particular portion of the screen directly to a named file, which actually streamlines my workflow compared to the Mac (where I had to separately save a file to the intended location).\nAlfred. One of the most important utilities for me on the Mac was Alfred, which I used both as an app hotkey and as a way to store snippets of text that I reuse (e.g. canned email responses). Fortunately I was able to find a replacement called Albert, which does a good job of storing snippets, as well as having many other features that I’ve yet to explore.\n\n\nThings I miss so far\nMac hardware. A major reason that I have always stuck with Mac is the hardware user experience. Even when the hardware has had issues, their laptops have always seemed to have the best usabilty of any I have seen. I’ve used a couple of Dell laptops at various points, and their keyboard/touchpad experience was absolutely horrible in comparison to the Mac. The ThinkPad is defintely better, but still nowhere as nice as my M1 Macbook Pro. If it was possible to run Linux effectively on the M1 then I would have definitely gone that way, but it seems that it’s just too much of an experimental project right now.\nMessages. I hate typing messages on my phone, so I usually use the Messages app on OS X to send iMessages. There doesn’t seem to be a web interface to iMessages or a replacement app for Linux, so I’m out of luck there. I’m probably going to use WhatsApp more frequently since they have a web interface.\nQuickLook. When I am attaching a file to an email, I want to be really sure that it’s the right one before I send it. One great feature of OSX is the ability to hit the space bar and see a preview of a file’s contents within the file chooser. The Ubuntu Files app has this feature when looking at a directory, but the feature does’t seem to be present in the file choosers that one uses to select a file, at least within Firefox. I’ll keep my eyes out for a tool that enables this.\nGoogle Drive access in the terminal. I like to work at the command line when I am moving files around, and often do this in my Google Drive space. On OSX the terminal automatically exposes the Google Drive space as a regular file system. On Linux, Google Drive shows up as a filesystem in the Files browser, but in the terminal it shows up an as object store, and one requires additional tools to work with the objects as files. I will probably keep a Mac around for the few times when I need to do something like this.\nI’ll keep you all posted as the journey continues!"
  },
  {
    "objectID": "posts/the-nih-should-stop-penalizing/index.html",
    "href": "posts/the-nih-should-stop-penalizing/index.html",
    "title": "The NIH should stop penalizing collaborative research",
    "section": "",
    "text": "This was originally posted on blogger here.\nThe National Institutes of Health (NIH) just put out its most recent strategic plan for research in behavioral and social sciences, which outlines four directions for behavioral/social research in the future (integrating neuroscience, better measurement, digital interventions, and large-scale data-intensive science). All of these require collaboration between researchers across multiple domains, and indeed Collins and Riley point out the need for more “transdisciplinary” research in the behavioral and social sciences. Given the strong trend towards transdisciplinary work over the last couple of decades, one would think that the NIH would do whatever it can to help remove barriers to the kinds of collaborations that are often necessary to make transdisciplinary science work. Instead, collaborative work across institutions is actively penalized by the way that grants are awarded and administered. A simple change to this could greatly smooth the ability for researchers across different institutions to collaborate, which is often necessary in order to bring together the best researchers across different scientific disciplines.To explain the situation, first let’s think about how one would administer a collaborative grant in the ideal world. Let’s say Professor Smith is a biologist at University X studying cancer, and Professor Jones is a computer scientist at University Y who has a new method for statistical analysis of cancer cells. They decide to write a grant proposal together, and each of them develops a budget to pay for the people or materials necessary to do the research (let’s say $150,000/year for Smith and $100,000/year for Jones). The grant gets a very good priority score from the reviewers, and the agency decides to fund it. In an ideal world, the agency would then send $150,000 to University X and $100,000 to University Y, and each would be treated as separate accounts from the standpoint of financial administration, even if their scientific progress would be judged as a whole.At some agencies (for example, for collaborative grants from the National Science Foundation), this is how it works. However, for nearly all regular grants at the NIH, the entire grant gets awarded to the lead institution, and then this institution must dole out the money to the collaborators via subawards. This might sound like no big deal, but it causes significant problems in two different ways:The first problem has to do with “indirect costs” (also known as “overhead”), which are the funds that universities receive for hosting the grant; they are meant to pay for all of the administrative and physical overhead related to a research project. The overhead rates for federal grants are negotiated between each institution and the federal government; for example, at Stanford the negotiated rate is 57%. This means that if the grant was awarded by NIH to Dr. Smith at a university where the rate was 50%, then NIH would send the entire $250,000 in “direct costs” plus $125,000 in “indirect costs” to University X. In the situation above, University X would then create a subaward to University Y, and send them the $100,000 for Dr. Jones’s part of the research. But what about the indirect costs? In the best-of-all-worlds model, each institution would take its proportion of the indirect costs directly. In the NIH model, what happens is that the subaward must include both the direct and indirect costs for University Y, which both must come out of the direct costs given to University X; that is, the subaward amount would be $150,000 ($100,000 in direct costs plus $50,000 in indirect costs). This penalizes researchers because it means that they will generally get about 1/3 less direct funds for work to be done on a subaward than work done directly from the primary grant, since the indirect costs (usually around 50%) for the subrecipient have to come out of the direct costs of the main grant. If grant funds were unlimited then this wouldn’t be a problem, but many grant mechanisms have explicit caps on the amount that can be requested. In addition to the reduced budget due to treating subaward indirect costs as as direct costs in the main budget, there is also an added extra expense due to “double dipping” of indirect costs. When the primary institution computes its indirect costs, it is allowed to charge indirect costs on the first $25K of the subaward; this means that NIH ends up spending an extra ~$12.5K in indirect costs on each subaward. This is presumably meant to cover the administrative budget of managing the subcontract, but it is another extra cost that arises for collaborative grants due to the NIH system.There is a second way that the NIH model makes collaboration harder, which is the greatly increased administrative burden for subaward management for grants lasting more than a year (as they almost always do). When an investigator receives an NIH grant directly, the university treats the grant as lasting the entire period; that is, the researcher can spend the money continuously over the grant period. If they don’t spend the entire budget they can automatically carry over the leftover funds to the next year (as long as this amount isn’t too much), and the university will also usually allow them to spend a bit of the next year’s money before it arrives, since it’s guaranteed to show up. For subawards, the accounting works differently. Every year the primary recipient generates a new subaward, which can’t happen until after the primary award for that year has been received and processed. Then this new subaward has to be processed and given a new account number by the recipient’s university. In addition, it is common for the lead school to not allow automatic carry-forward of unspent funds between years, and sometimes they requite any unused funds to be relinquished, and then be rewarded back in the new year’s fund. All of these processes take time, which means that the subaward recipient is often left hanging without funding for periods of time, particularly at the end of the yearly grant period. This is a pretty minimal cost compared to the actual cost described above, but it ends up taking a substantial amount of time away from doing research.Why can’t the NIH adopt a process like the one used for collaborative grants at NSF, in which the money goes directly to each institution separately and indirect costs are split proportionately? This would be a way in which NIH could really put its money where its mouth is regarding collaborative transdisciplinary research. UPDATE: Vince Calhoun pointed out to me that the indirect costs in the subcontract do not actually count against the modular budget cap. According to the NIH Guide on budget development: “Consortium F&A costs are NOT included as part of the direct cost base when determining whether the application can use the modular format (direct costs &lt; $250,000 per year), or determining whether prior approval is needed to submit an application (direct costs $500,000 or more for any year)…NOTE: This policy does not apply to applications submitted in response to RFAs or in response to other funding opportunity announcements including specific budgetary limits.” Thus, while this addresses the specific issue of modular budgets, it doesn’t really help with the many funding opportunities that include specific budget caps, which covers nearly all of the grants that my lab applies for."
  },
  {
    "objectID": "posts/how-well-can-we-predict-future-criminal/index.html",
    "href": "posts/how-well-can-we-predict-future-criminal/index.html",
    "title": "How well can we predict future criminal acts from fMRI data?",
    "section": "",
    "text": "This was originally posted on blogger here.\nA paper recently published in PNAS by Aharoni et al. entitled “Neuroprediction of future arrest” has claimed to demonstrate that future criminal acts can be predicted using fMRI data. In the study, the group performed fMRI on 96 individuals who had previously been incarcerated, using a go/no-go task. They then followed up the individuals (up to four years after release) and recorded whether they had been rearrested. A survival model was used to model the likelihood of being re-arrested, which showed that activation in the dorsal anterior cingulate cortex (dACC) during the go/no-go task was associated with rearrest, such that individuals with higher levels of dACC activity during the task were less likely to be rearrested. This fits with the idea that the dACC is involved in cognitive control, and that cognitive control is important for controlling impulses that might land one back in jail. For example, using a median split of dACC activity, they found that the upper half had a rearrest rate of 46% while the lower half had a rearrest rate of 60%. Survival models also showed that dACC was the only variable amongst a number tested that had a significant relation to rearrest.This is a very impressive study, made even more so by the fact that the authors released the data for the tested variables (in spreadsheet form) with the paper. However, there is one critical shortcoming to the analyses reported in the paper, which is that they do not examine out-of-sample predictive accuracy. As I have pointed out recently, statistical relationships within a sample generally provide an overly optimistic estimate of the ability to generalize to new samples. In order to be able to claim that one can “predict” in a real-world sense, one has to validate the predictive accuracy of the technique on out-of-sample data.With the help of Jeanette Mumford (my local statistical guru), I took the data from the Aharoni paper and examined the ability to predict rearrest on out-of-sample data using crossvalidation; the code and data for this analysis are available at https://github.com/poldrack/criminalprediction. The proper way to model the data is using a survival model that can deal with censored observations (since subjects differed in how long they were followed). We did this in R using the Cox regression model from the R rms library. We replicated the reported finding of a significant effect of dACC activation on rearrest in the Cox model, with parameter estimates matching those reported in the paper, suggesting to me that we had correctly replicated their analysis. We examined predictive accuracy using the pec library for R, which generates out-of-sample prediction error curves for survival models. We used 10-fold crossvalidation to estimate the prediction error, and ran this 100 times to assess the variability of the prediction error estimates. The figure below shows the prediction error as a function of time for the reference model (which simply estimates a single survival curve for the whole group) in black, and the model including dACC activation as a predictor in green; the thick lines represent the mean prediction error across the 100 crossvalidation runs, and the light lines represent the curve for each individual run. This analysis shows that there is a slight benefit to out-of-sample prediction of future rearrest using dACC activation, particularly in the period from 20 to 48 months after release. However, this added prediction ability is exceedingly small; if we take the integrated Brier score across the period of 0-48 months, which is a metric for assessment of probabilistic predictions (taking the value of 0 for perfect predictions and 1 for completely inaccurate predictions), we see that the score for the reference model is 0.214 and the score for the model with dACC as a predictor is 0.207. We found slightly improved prediction (integrated Brier score of 0.203) if we also added Age alongside dACC as a predictor. The take-away message from this analysis is that fMRI can indeed provide information relevant to whether an individual will be rearrested for a crime. However, this added predictability is exceedingly small, and we don’t know whether there are other (unmeasured) demographic or behavioral measures that might provide similar predictive power. In addition, these analyses highlight the importance of using out-of-sample prediction analyses whenever one makes a claim about the predictive ability of neuroimaging data for any outcome. We are currently preparing a manuscript that will address the issue of “neuroprediction” in greater detail."
  },
  {
    "objectID": "posts/how-well-can-we-predict-future-criminal/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/how-well-can-we-predict-future-criminal/index.html#comments-captured-from-original-post-on-blogger",
    "title": "How well can we predict future criminal acts from fMRI data?",
    "section": "4 comments captured from original post on Blogger",
    "text": "4 comments captured from original post on Blogger\nUnknown said on 2013-04-18\nThis is a nice first step in improving prediction models using neuroimaging. What did you use for your out-of-sample data? I’m working on a manuscript using imaging data to predict relapse to substance use. It would be nice to test my model on out-of-sample data and include that information in the manuscript.\nRoss said on 2013-05-10\nAs I understand there’s no out of sample data. They do cross-validation, so they run their model on 90% of the data, and test it on the 10% that was left out. The 10-fold means they do this ten times with leaving out each of 10 10% pieces. Then they do some randomness, in generating randomly the different breakdowns of training versus validation sets.\nhcp4715 said on 2016-08-13\nHi, Prof. Poldrack, This is a very nice post!I noticed that the authors published a new paper about this problem, and they concluded that \"Modest to strong discrimination and calibration accuracy were found, providing additional support for the utility of neurobiological measures in predicting rearrest.\" I am not expert on the methods, but interested whether neuroimaging data could improve the predictive accuracy. So I wish you could give some comments on this new paper: http://www.tandfonline.com/doi/abs/10.1080/17470919.2014.907201\nRuss Poldrack said on 2016-08-13\nThanks for your comment! I don’t think that paper fully addresses the problem, because it doesn’t examine out-of-sample prediction as we did in our analyses."
  },
  {
    "objectID": "posts/becoming-academic-road-warrior/index.html",
    "href": "posts/becoming-academic-road-warrior/index.html",
    "title": "Becoming an academic road warrior",
    "section": "",
    "text": "This was originally posted on blogger here.\nIt’s a reality that if you want to make it in academic science, you have to be ready and willing to travel a good bit. Between conferences, talks at other universities, grant review panels, and other various events in the US and abroad, I generally end up flying between 50,000 and 100,000 miles a year. Students and postdocs often ask me how I can stand to travel so much, and this post will lay out some of the strategies that I use keep myself sane while on the road. I should note up front that many of these strategies are not exactly “budget travel” ; my feeling is that I travel enough that it’s worth investing in some things that make it pleasant, but I realize that this may conflict with the realities of student or postdoc salaries.Pick an airline and stick with itI think that airline loyalty is probably the most important thing one can do to make traveling more pleasant. Most airlines have loyalty programs for people who travel at least 25,000 miles (that’s about 5 rounds trips from LA to DC, for example), and achieving this level of frequent flyer “status” comes with some very nice benefits. Foremost is access to priority check-in and priority security lines. The benefit of priority security varies between airports, but in many places (like the United terminals at LAX or O’Hare) it can save a very long wait to get through security during busy times. As your status increases, so do the benefits; in particular, once you reach “gold” status (50,000 miles on most airlines) then you become much more likely to get upgraded to first class, which is always a nice perk. Some airlines (like United) give preferred seating (with extra legroom) to frequent flyers, which makes using your laptop much more bearable. Another benefit is that you have priority for rebooking when problems occur. Finally, another benefit of loyalty is that it gives you a chance to learn the airline’s terminals and systems, which can be really useful when you are running late or things go wrong.I was a frequent flyer with United for many years, but recently switched to Continental after moving to Austin – I’ve been quite happy with both overall, and fortunately with their merger these are becoming one airline. When another institution invites you to come and give a talk, it’s perfectly legitimate to tell them what airline you want to fly on (and even what flights you want to take), and give them your frequent flyer number. This has the nice side benefit that your itinerary will show up on the airline’s online system, so you can see schedule changes and check in online.Be sure to sign up for text alerts from your airline, which will let you know about delays or upgrades. Also, when problems arise, it’s often better to call the airline directly rather than waiting in line to speak to a customer service representative when the lines are long.Join the clubIf you’ve spent any time in airports you have probably wondered what goes on inside the “Red Carpet Club” or other airline lounges. The short story is that it’s a place where you can pay to get away from the craziness of the terminal, and if you are going to fly regularly it is money well spent. It’s not cheap (generally about $400/year) but if you are flying 20 round trips in a year then that’s just $10 for each one-way trip – well worth the price IMHO. Perks include free wi-fi, snacks, and drinks (usually just beer) – unless you are in a European airport, where it generally includes a full spread of food and a full bar (though not always free wi-fi). But perhaps the most important benefit of the club is that there are customer service representatives that can help with flight problems – this is especially useful when large disruptions occur due to weather, resulting in long lines at the customer service desks out in the terminal. Keep calm and carry onNEVER check a bag unless you absolutely have to. It is certainly possible to travel for weeks with just a carry-on bag, if you pack carefully and take advantage of hotel laundry services while on the road.I have two bags that I use for most of my trips. My primary bag for trips longer than 2 days is a21” expanadable carry on. I find that I can generally make it up to 4-5 days with this bag in its carry-on (un-expanded) configuration, depending on the season and what kinds of clothing I have to carry. For 1-2 day trips I use a small carry-all. Learning to pack can help you really maximize your carry-on space - for example, see (Packing Tips from Professional Travelers) - I particularly find that rolling my clothes helps to maximize packing density.Once you can afford it, buy some good luggage. I use Briggs & Riley, which is not cheap but has a lifetime warranty and has very good usability in general. Nothing sucks more than luggage failure in the middle of a trip.I also carry a TSA-approved laptop bag; having to remove your laptop is a pretty small thing, but every little bit counts when it comes to getting through security without stress.Stay preparedI don’t want to have to scramble the morning of my flight to find all of the toiletries that you need to take with me, so I have a separate set of toiletries that I keep ready just for travel. I use either 2 ounce Nalgene bottles or recycled pill bottles to hold most of my toiletries, so that I can fit everything into the TSA-approved quart bag. Some things I always bring:extra ziploc bagsearplugs (for the plane and/or the hotel)noise-cancelling headphones (I use the Sennheiser travel headphones which are are small and light)an energy bar (in case I miss the chance to eat between long flights)my yoga mat (to keep me flexible after too much sitting - I have a very thin and light mat that’s easy to pack)for international travel: Ambien (for sleeping on the plane and overcoming jet lag)"
  },
  {
    "objectID": "posts/becoming-academic-road-warrior/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/becoming-academic-road-warrior/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Becoming an academic road warrior",
    "section": "1 comments captured from original post on Blogger",
    "text": "1 comments captured from original post on Blogger\nStephan said on 2010-11-14\nRuss, This is awesome advice!"
  },
  {
    "objectID": "posts/nyt-editorial-fmri-complete-crap/index.html",
    "href": "posts/nyt-editorial-fmri-complete-crap/index.html",
    "title": "NYT Op-Ed + fMRI = complete crap",
    "section": "",
    "text": "This was originally posted on blogger here.\nMany of you may remember the controversy that arose a few years back when the NY Times published an Op-Ed titled “This is your brain on politics” by Marco Iacoboni and colleagues. This steaming pile of shoddy reverse inferences inspired a group of us to write a letter to the editor, published online. Well, the NYT editorial page is at it again, this time with a piece by self-proclaimed neuromarketer Martin Lindstrom, titled “You love your iPhone, literally” (h/t Raj Raizada for pointing me to it). The argument of the article is that rather than our feelings about iphones reflecting something like an addiction driven by dopamine (which I have argued for in the past), our feelings about our digital devices instead reflect true love, based on fMRI:But most striking of all was the flurry of activation in the insular cortex of the brain, which is associated with feelings of love and compassion. The subjects’ brains responded to the sound of their phones as they would respond to the presence or proximity of a girlfriend, boyfriend or family member.In short, the subjects didn’t demonstrate the classic brain-based signs of addiction. Instead, they loved their iPhones.Insular cortex may well be associated with feelings of love and compassion, but this hardly proves that we are in love with our iPhones. In Tal Yarkoni’s recent paper in Nature Methods, we found that the anterior insula was one of the most highly activated part of the brain, showing activation in nearly 1/3 of all imaging studies! Further, the well-known studies of love by Helen Fisher and colleagues don’t even show activation in the insula related to love, but instead in classic reward system areas. So far as I can tell, this particular reverse inference was simply fabricated from whole cloth. I would have hoped that the NY Times would have learned its lesson from the last episode."
  },
  {
    "objectID": "posts/nyt-editorial-fmri-complete-crap/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/nyt-editorial-fmri-complete-crap/index.html#comments-captured-from-original-post-on-blogger",
    "title": "NYT Op-Ed + fMRI = complete crap",
    "section": "6 comments captured from original post on Blogger",
    "text": "6 comments captured from original post on Blogger\nbiobabbler said on 2011-10-02\nRe: your post title: Tell us how you really feel.=) I really appreciate direct people.=) And am now following.Oh, and if I used my cell phone (or iPhone, if I had one), and programmed it to have a special ring when my husband called (really the only person I really ever want to talk to, as am NOT a phone person), you can bet my brain would then be sending hearts out (like Sally from Peanuts, when she is near Linus) ’cause I am in love with my husband. MOST DECIDEDLY not the phone. The phone is a means to an end.So, if I am writing a note to my husband, whom I love, am I also then, based on my brain activity or blood chemistry (if they actually accurately pinpointed indicators of love), in love with the paper and pen? I think not.\nCatherine Kerr said on 2011-10-02\nI agree with you about the lack of specificity in the insula… However, Aron Fisher et al’s J Neurophys (2005) study did report some interesting insula findings \"length of time in love is a major factor for neural activity in the insula and cingulate/retrosplenial cortex when looking at an image of a romantic partner.\"Of course that group’s romantic rejection paper had I think a more robust insula finding….speaking to your larger point.\nAce said on 2011-10-02\nI agree but your post has been sent around with the assertion that fmri is pseudoscience, with which I am pretty sure you’d disagree. I don’t blog much but I had to comment on this: http://www.sayginlab.org/blog/?p=135Would love to hear your thoughts.\nRuss Poldrack said on 2011-10-02\n@Ace: thanks for sending the link to your post, which was right on.\nUnknown said on 2011-10-05\nI wouldn’t trust anything coming from a guy who writes a book called, \"Brandwashed: Tricks Companies Use to Manipulate Our Minds and Persuade Us to Buy.\" He should have titled his article, \"How I manipulate the facts to support my baseless argument.\" Clearly he’s deducing from his anti-Capitalist premise that companies are only out to manipulate you to make a profit. And he’s using fMRI data to erroneously support his erroneous conclusion. He thinks that if equating \"love\" for one’s iPhone with \"love\" for one’s significant other that this will demonstrate that companies are trying to supplant your higher values such as one’s relationship with a husband or wife, friends, and family, with inanimate objects (although important, still of lesser value than the people in one’s life). He’s engaging in this mind-body dichotomy by assuming that our material values are at war with our spiritual values. Our iPhones are eroding our personal relationships he claims. For rational people who know how to prioritize, an iPhone is not destroying our relationships. If anything it’s enhancing them by providing more efficient means of facilitating contact with those who are important to us.\nZR said on 2011-10-06\nOn the subject of using abstract words to describe CONCRETE science:please stop. you are making science look inaccurate.\"love\" \"addiction\" all of those things are income-generating, politic-/law-making words… science is neutral.dopamine. that’s it. that’s all we are talking about here.dopamine.there isn’t more than one dopamine.just one. no reason to have like 50 words for it."
  },
  {
    "objectID": "posts/teaching-statistics-online-for-first/index.html",
    "href": "posts/teaching-statistics-online-for-first/index.html",
    "title": "Teaching statistics online (for the first time)- Lessons learned",
    "section": "",
    "text": "This was originally posted on blogger here.\nWe just completed the 2021 Winter Quarter at Stanford, and it was my first time to teach my introductory statistics course fully online. The goal of this post is to talk through how it went and the lessons learned. I apologize in advance for the length of this!Course structureThis course (Stats 60/Psych 10) is the pre-calculus version of intro statistics that is meant to serve a broad range of students across the university (including, but certainly not limited to Psychology students), with a roughly even mix of students from each class (freshman through senior). The enrollment this quarter was well over 100 students. This is the fourth time that I have taught the class, but the first time to teach it fully online. We made a number of major changes to accommodate the online format:Modular course structure: We reorganized the course around Canvas modules, each of which was focused on a particular topic and lasted one week. Each module included:Pre-recorded lecture videos (usually 2-3 videos, 10-15 minutes each), with quiz questions embedded (using Panopto) to ensure engagementReadings from my open-source textbookA quiz (which had to be completed with 100% correct for credit, and could be retaken as many times as necessary to achieve that score)Every other module included a problem setSome modules included milestones related to the final project, which was an independent data analysis project using openly available data completed in groups of 3-4 students.At any point in the quarter, the students could access the modules for the current week and the following two weeks, which gave them an opportunity to get ahead when needed but also ensured some degree of spaced learning.Fully flipped class: There were no standard lectures during the synchronous class sessions (which met 3 times weekly, 50 minutes per session); pre-recorded lecture videos were provided for each module, and students were required to watch them and complete questions embedded in the videos (using Panopto on Canvas). Students were required to attend at least one of these synchronous sessions, or alternatively to complete a written makeup assignment. The synchronous sessions followed roughly the following organization:Monday: review of core concepts (usually with me drawing on the Zoom whiteboard), and group activitiesWednesday: Live coding, including problem set review in weeks following a problem set deadlineFriday: Answering questions (which students could post to a Google Doc each week) and breakout room activitiesSchedule-driven grading: Inspired by Patrick Watson’s outstanding post, I decided to move to a schedule-driven grading system, in which students start the quarter with 105 points, and lose 2.5 points for every assignment that they don’t complete (including attendance to at least one course session or completion of a makeup exercise, and attendance at discussion sections). Thus, they always know exactly what their grade is (assuming they do everything else in the quarter). Most grading was for completion; for the problem sets, we ran the submissions through an automated testing system, and students with multiple errors were given a chance to revise their submission. For lecture attendance, students self-reported their attendance; this was doublechecked on occasion against the Zoom logs, with no major discrepancies. The goal in general was to ensure not just completion but mastery of the material.Purpose-built R tutorials: An essential part of the class is learning to perform statistical analyses using R. This is challenging because many of the students in the course have never coded before, and 10 weeks is a very short time to teach this! In past years we have used Datacamp tutorials, but found that they were not well aligned with the specific topics that we were teaching. For this year, I developed a set of interactive R tutorials (using the awesome learnr package) which were specifically built to emphasize the skills that we wanted them to have, with minimal distraction. This also allowed some changes in how we teach R. For example, in recent years we have introduced pipes from the very beginning of teaching the tidyverse, but found that they were very difficult for many students to conceptualize. This year we started with the tidyverse without using pipes (i.e. using strings of individual commands), and only introduced pipes in the last few weeks of the course. Again it’s hard to say since so many things changed, but this change definitely seemed to reduce confusion in the early stages of R learning.Google Colab: In the past we have tried having students install RStudio on their own computers, or using rstudio.cloud for cloud access. However, the former is problematic since many students have Chromebooks that can’t run RStudio, and the latter now charges a substantial amount. We made the choice to try Google Colabas our coding platform; the ability to run R notebooks is somewhat obscured (they have to be created using a special link) but once created they work well.Problem sets with embedded tests: In the past we have given students a “skeleton” code file that provided them with some scaffolding for their problem sets and ensured that variables have the correct names (so that our automated testing system is less likely to fail). This year, we decided to embed a test into each code cell in the skeleton, so that students could immediately see for each cell whether they had correctly completed the problem (primarily by testing to see that their variables had the correct sizes, types, and values), giving them a “good job!” message when they did so. Sections: In the past, we have used discussion sections for concept review in addition to working on the group projects. This year, we decided to dedicate section time solely to working on group projects, given that students often need a lot of time to work on these.Lessons learned:Overall I thought the new course structure worked incredibly well, and the students seemed to agree. On the question of “How much did you learn from this course”, more than 2/3 of the students said “A lot” or “A great deal”. We didn’t obtain these overall ratings last year due to the COVID onset, but in 2019 only 45% rated the course at this level, suggesting that we have significantly improved the student experience (that difference is significant at p&lt;.001 if you need statistical evidence :-). Clearly one needs to take any such comparison with a grain of salt since many things have changed, but the qualitative comments from the students were also markedly more positive this year. In what follows I will paraphrase some of the comments from the student evaluations. Modular structure: Students appreciated the organization of the modules; the course received very high ratings on the question “How organized was this course?”, with 95% saying “Extremely organized” or “Very organized”; only 59% of students rated it that well in the 2019. Flipped class structure: I felt that the ability to talk with students and walk through problems on the digital whiteboard was really effective at helping me understand which concepts they were struggling with. In addition, I was able to address the questions that they had posted to the weekly Google Doc. This allowed me to spend much more time focusing on concepts that needed additional attention. In addition, the chat function in Zoom seemed to help encourage questions from students who might have been reticent to speak up in class before.Schedule-driven grading: Perhaps not surprisingly, the students loved the grading system; on the course evaluation question of “How did you feel about the schedule-driven grading system?”, 90% of students were strongly positive and 7% were weakly positive. A number of students mentioned in their comments that the grading system allowed them “focus on learning” rather than worrying about their grade. This was particularly noted by students with no coding experience, for whom the course can be quite daunting. The only major complaints regarding the grading system were from students who thought that they would have been more engaged in the course if they had been graded for accuracy rather than completion.Purpose-built tutorials: These were a big hit, in particular because they directly matched the problem sets; whereas in previous years students might have to search online resources to find how to solve a particular problem, this year every coding concept that they needed for the problem sets had been covered in the tutorials. And they didn’t have to learn a lot of R concepts that would never show up in class.Colab: In comparison with previous years, there was very little friction around the coding platform; in general Colab worked very well. In particular, it made it very easy to share my notebooks from class, so that students could view them and create a copy that they could edit themselves if they wanted. 77% of students rated Colab positively, and only 5% rated it negatively. The main complaint was that it doesn’t allow simultaneous editing of a notebook by multiple people (ala Google Docs), which occasionally led to collisions if students were simultaneously editing a shared notebook. I will definitely choose Colab again for next year’s class!I would say that the one thing that didn’t work well was breakout rooms during the synchronous class sessions; the majority of students rated them as Moderately useful (33%), Slightly useful (17%), or Not useful at all (18%). One major issue is that many students would apparently join the breakout room but then keep their cameras turned off and not participate in the discussion; occasionally this would lead a student to be the only responsive individual out of 5 or 6 people in a breakout room. Let’s hope that when I teach it again in Winter 2022 that we will be in person and not over Zoom…That said, one thing that I think actually worked better over Zoom than in person was live coding. It was certainly better for me as the instructor, because I was able to use my large monitor and have more information at my fingertips (sharing only my notebook screen) than I can using my laptop in a lecture hall where my entire monitor is in view. In addition, it’s much easier for students to type coding answers into the chat window than it is to say them out loud. I am definitely going to consider a hybrid going forward where the live coding sessions are held remotely even if the remainder of the class is held in person.In closing, I have to give a shout out to my awesome teaching team, who helped make the experience of teaching this class so seamless and enjoyable, as well as to my students who remained engaged despite the challenges of online learning. I’m looking forward to teaching the course in person next year, but I think that the experience of taking it fully online will definitely improve the course even when it’s back in a physical classroom."
  },
  {
    "objectID": "posts/teaching-statistics-online-for-first/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/teaching-statistics-online-for-first/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Teaching statistics online (for the first time)- Lessons learned",
    "section": "1 comments captured from original post on Blogger",
    "text": "1 comments captured from original post on Blogger\nJohnKubie said on 2021-04-03\nSounds terrific. Trying to see how our my institution (medical school) can apply some of these ideas. In particular, which aspects of remote instruction are better than previous approaches."
  },
  {
    "objectID": "posts/automated-web-site-generation-using/index.html",
    "href": "posts/automated-web-site-generation-using/index.html",
    "title": "Automated web site generation using Bookdown, CircleCI, and Github",
    "section": "",
    "text": "This was originally posted on blogger here.\nFor my new open statistics book (Statistical Thinking for the 21st Century), I used Bookdown which is a great tool for writing a book using RMarkdown. However, as the book came together, the time to build the book grew to more than 10 mins due to the many simulations and Bayesian model estimation. And since each output type (of which there are currently three: Gitbook, PDF, and EPUB) requires a separate build run, rebuilding the full book distribution became quite an undertaking. For this reason, I decided to implement an automated solution using the CircleCIcontinuous integration service. We already use this service for many of the software development projects in our lab (such as fMRIPrep and MRIQC), so it was a natural choice for this project as well. The use of CircleCI for this project is made particularly easy by the fact that both the book source and the web site for the book are hosted on Github — the ability to set up hooks between Github and CircleCI allows two important features. First, it allows us to automatically trigger a rebuild of the site whenever there is a new push to the source repo. Second, it allows CircleCI to push a new copy of the book files to the separate repo that the site is served from. Here are the steps to setting this up - see the Makefile and CircleCI config.yml file in the repo for questions. And if you come across anything that I missed please leave a comment below! Create a CircleCI account linked to the relevant GitHub account.Add the source repo to CircleCI.Create the CircleCI config.yml file. Here is the content of my config file, with comments added to explain each step: version: 2jobs: build: docker:# this is my custom Docker image - image: poldrack/statsthinking21 CircleCI spins up a VM specified by a Docker image, to which we can then add any necessary additional software pieces. I initially started with an image with R and the tidyverse preinstalled (https://hub.docker.com/r/rocker/tidyverse/) but installing all of the R packages as well as the TeX distribution needed to compile the PDF took a very long time, quickly using up the 1,000 build minutes per month that come with the CircleCI free plan. In order to save this time I build a custom Docker container (Dockerfile) that incorporates all of the dependencies needed to build the book; this way, CircleCI can simply pull the container from my DockerHub repoand run it straight away rather than having to build a bunch of R packages. steps: - add_ssh_keys: fingerprints: - “73:90:5e:75:b6:2c:3c:a3:46:51:4a:09:ac:d9:84:0f” In order to be able to push to a github repo, CircleCI needs a way to authenticate itself. A relatively easy way to do this is to generate an SSH key and install the public key portion as a “deploy key” on the Github repo, then install the private key as an SSH key on CircleCI. I had problems with this until I realized that it requires a very specific type of SSH key (a PEM key using RSA encryption), which I generated on my Mac using the following command: ssh-keygen -m PEM -t rsa -C “poldrack@gmail.com” # check out the repo to the VM - it also becomes the working directory - checkout# I forgot to install ssh in the docker image, so install it here as we will need it for the github push below - run: apt-get install -y ssh# now run all of the rendering commands - run: name: rendering pdf command: | make render-pdf - run: name: rendering epub command: | make render-epub - run: name: rendering gitbook command: | make render-gitbook The Makefile in the source repo contains the commands to render the book in each of the three formats that we distribute: Gitbook, PDF, and EPUB. Here we build each of those. # push the rendered site files to its repo on github - run: name: check out site repo command: | cd /tmp ssh-keyscan github.com &gt;&gt; ~/.ssh/known_hostsThe ssh-keyscan command is necessary in order to allow headless operation of the ssh command necessary to access github below. Otherwise the git clone command will sit and wait at the host authentication prompt for a keypress that will never come.# clone the site repo into a separate directory git clone git@github.com:psych10/thinkstats.git cd thinkstats# copy all of the site files into the site repo directory cp -r ~/project/_book/* . git add .# necessary config to push git config –global user.email poldrack@gmail.com git config –global user.name “Russ Poldrack” git commit -m”automated update” git push origin master That’s it! CircleCI should now build and deploy the book any time there is a new push to the repo. Don’t forget to add a CircleCI badge to the README to show off your work!"
  },
  {
    "objectID": "posts/my-adventures-in-self-quantification/index.html",
    "href": "posts/my-adventures-in-self-quantification/index.html",
    "title": "My adventures in self-quantification",
    "section": "",
    "text": "This was originally posted on blogger here.\nIn September 2012 I began a project to characterize how my own brain function and metabolism fluctuate over the course of an entire year. This has involved MRI scans three times a week along with blood draws once a week and daily tracking of a large set of potentially interesting variables. This post is the first installment in my story about the experience.First, the motivation. In the last couple of years I have become very interested in understanding the dynamics of brain function over a days-months timescale and how they relate to cognitive function and bodily metabolism. This interest has been spurred by a number of influences, such as my growing interest in nutrition and its relation to brain function as well as my ongoing interest in better understanding psychiatric disorders. Once I started thinking about the issue, it became very clear that there were basically no data in existence that provide any insight into how the function of an individual’s brain fluctuates over such a relatively long time course. This is probably not surprising, because doing studies with volunteers that require repeated testing over a long period of time is very challenging. At some point in 2011 it dawned upon me that I should try to bootstrap such a study by collecting data from myself. There were several inspirations for this idea. First was Michael Snyder’s “integrated personal omics” study, published in Cell in 2011, in which he repeatedly collected blood from himself and performed a broad set of”omics” measures on his samples, which provided some interesting insights into the temporal dynamics of metabolic function. Second was my interaction with Laurie Frick, who is the artist-in-residence at the UT Imaging Research Center. Laurie’s work is based on patterns that she finds in data obtained by self-tracking, and she is deeply enmeshed in the Quantified Self movement (see her excellent TEDx talk). Talking to her got me increasingly interested in tracking a broader set of data about myself. The very fun book Smoking Ears and Screaming Teeth also convinced me that self-experimentation is not (completely) crazy, and actually has long been an important tool for scientific discovery.In early 2012 I began hatching a plan to collect a broad set of data on myself. It was essential that the all aspects of data collection were as consistent as possible in order to minimize extraneous variability in the data (such as time of day effects). I ended up settling on a schedule of three MRI scanning sessions a week, at consistent times of day and days of the week (one afternoon and two mornings every week). Each of the MRI scanning sessions includes a resting state fMRI scan, which will allow us to assess how functional connectivity between brain regions fluctuates over time. In addition, once a week I perform other scans, including structural MRI (T1- and T2-weighted), diffusion tensor MRI (to assess white matter connectivity), and task fMRI (using a working memory task with faces, scenes, and chinese characters). I also wanted to collect biological samples in order to measure the relation between bodily metabolism and brain function. Working with some molecular biologists here at UT (along with helpful input from the Snyder lab at Stanford), we developed a protocol in which I have 20 ml of blood drawn once a week (while fasting, immediately after one of the morning MRI scans). This sample is then processed to extract RNA, white blood cells, and plasma, all of which are frozen for later analysis. This will let us examine many different aspects of metabolism, including gene expression (via RNA sequencing), metabolomic and proteomic analyses, and other potential analyses to relate metabolism to brain function.Finally, I realized that the dataset would be most useful if I also collected as much data as possible about my daily life activities. Working with Zack Simpson, we developed a self-tracking app using the Appsoma framework, which allows me to easily complete surveys every morning and evening and after every MRI scan. These data are automatically fed into a web database which is the central repository for all of the self-tracking data in the study other than MRI and biological analyses. Some of the things that I track daily include:- blood pressure and weight (using a FitBit Aria wireless scale)- foods eaten, alcohol intake, and supplements/medicines taken- exercise, time spent outdoors, and physical soreness- a free-text log of daily events- sleep quality (assessed both by subjective report and using a ZEO sleep monitor)After every scan I also complete a mood questionnaire and also provide a structured report of what I was thinking about during the resting state fMRI scan. I should note that other than the addition of all of these tracking activities, I have done my best to keep my life as consistent as possible and have avoided any other major lifestyle changes.With this plan in place, we began data collection on September 25, 2012. We treated the first month as a pilot period, and made some changes to the imaging protocol to optimize data collection, beginning the production period on October 22, 2012. In total so far we have collected 20 blood samples and 55 MRI scanning sessions. Members of the research team have started analyzing the data, though I have made every effort not to expose myself to the results of any analyses that examine changes over time, because I don’t want the results to feed back and change my behavior. When I describe this study, many people ask if I am worried about being exposed to MRI scanning so often. My answer has been “no”, at least not with regard to the magnetic fields involved in MRI; there is no evidence of lasting effects of MRI exposure (though of course we can’t ever prove that something is safe). However, soon after the study began it became clear that there was a side effect that I had to worry about, which is the intense noise of the MRI scanner. I have long suffered from tinnitus (which I attribute to too many loud rock shows as a youngster without ear plugs), and within the two weeks of scanning I noticed that my tinnitus was increasing. For this reason, I went to the UT Speech and Hearing Center and had my hearing tested. I had never had my hearing tested as an adult, but I was not terribly shocked to find out that I had quite significant high frequency hearing loss. Because I don’t want to damage my hearing any further, I have continued to get tested each month. The results had been fairly stable until early March, when they showed about a slight worsening at 6000 Hz (consistent with a subjective increase in tinnitus around the same time). For this reason, I am taking the month of March off of scanning, and will have my hearing re-tested at the beginning of April before resuming the scans.We will also make some changes to the MRI protocol to reduce scanner noise. We have an OptoAcoustics noise canceling headphone system in place at our imaging center that works quite well to reduce the noise of the functional MRI scans, so those can continue without much danger. However, we will likely discontinue some of the other scans (such as gradient field maps, which are useful but not necessary) and greatly reduce the frequency of others that we don’t expect to change much over time (including the anatomical and diffusion scans), because those scans are not compatible with the noise cancellation system. I am hopeful that with these changes I can continue scanning without danger of further hearing damage, while still collecting a very useful dataset. Assuming that I am able to continue scanning, I plan to collect 50 weeks worth of usable data, which should provide sufficient power for an initial set of analyses. This will likely take though the end of 2013 due to travel and other events that will interfere with data collection during some weeks. Once we have completed our initial set of analyses, nearly all of the data will be made available to other researchers, which I hope will help spur new analyses. I’ll keep you all posted as the study moves along."
  },
  {
    "objectID": "posts/my-adventures-in-self-quantification/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/my-adventures-in-self-quantification/index.html#comments-captured-from-original-post-on-blogger",
    "title": "My adventures in self-quantification",
    "section": "8 comments captured from original post on Blogger",
    "text": "8 comments captured from original post on Blogger\npractiCal fMRI said on 2013-03-13\nNice. I was thinking of doing a 24-hr study on myself last summer, with scans every 90 mins or so. But diurnal effects are well researched by others, it was really just an excuse to offer myself as a lab rat. I, too, have no fear of the exposure effects of the MRI. Indeed, I reckon a clever statistician could show that I am safer for the time spent in the MRI than a similar amount of time spent driving my car.Good luck with the tinnitus. Nasty. I assume you’re using the 12ch head coil and using the Siemens pneumatic headphones? Try dispensing with the latter entirely, and use instead those green curved foam pieces either side of your head (along with the standard ear plugs). My experience is that total hearing protection (and comfort) is improved by omitting headphones entirely. Be interesting to try that approach vs your ANR headphones. As you’re an experienced subject you probably don’t need to hear the operator clearly over the bore speaker.\nRuss Poldrack said on 2013-03-13\nI’m actually using the 32ch Siemens coil and the optoacoustics headphones over foam earplugs. interesting idea about trying the foam, will give that a shot. At this point I know exactly what is going to happen during the scan, and the only reason they talk to me is so that I can swallow before the scan starts :-).\npractiCal fMRI said on 2013-03-13\nEven better! In the 32ch coil I’ve found the green foam to be more comfortable and better sound protection than anything I can contrive in the 12ch coil.\nUnknown said on 2013-03-13\nruss, have you you considered switching to a spiral pulse sequence? the high-frequency pinging of EPI could be especially damaging. spirals make a nice low-frequency hum.\nRuss Poldrack said on 2013-03-13\nThanks Eli - interesting suggestion. I had not considered this, mostly because I don’t think there is currently a multiband spiral sequence for Siemens machines, and I would not want to give up the substantial performance benefits we get from multiband imaging (e.g., right now are getting 64 slices at 2.4 mm isotropic resolution in 1.16 seconds).\nGuillaume said on 2013-03-21\nFascinating project, I’m very curious about the outcomes!I was also wondering about the ethical implications: did you have to obtain approval from an ethical committee before starting the project, and are you potentially concerned with sharing such personal data?\nRuss Poldrack said on 2013-03-21\nThanks Guillaume - My local IRB has reviewed the project and made the determination that it does not meet the definition of \"human subjects research\" which is \"a systematic investigation designed to develop or contribute to generalizable knowledge\", because a study of a single individual is not generalizable. In addition, it does not involve any recruitment of human volunteers, just the PI (me). I am somewhat concerned about sharing identifiable personal data. I will most likely make the data available under a data sharing agreement that prevents the further sharing of the data or the disclosure of any health-related findings from the data without my permission.\nRuss Poldrack said on 2013-04-07\nthanks for the tip!"
  },
  {
    "objectID": "posts/psychology-climate-change/psychology-climate-change.html",
    "href": "posts/psychology-climate-change/psychology-climate-change.html",
    "title": "Teaching the Psychology of the Climate Crisis",
    "section": "",
    "text": "“It is now incontrovertible that human behavior has led to significant changes in the Earth’s climate, and that these changes will inflict an increasingly significant toll on human wellbeing if they are not reversed. It is also clear that both the opportunities and obstacles to the needed changes are largely psychological and behavioral.”\n\nSo began the syllabus for a course that Noah Goodman and I taught together in the Spring Quarter of 2023 at Stanford, titled “Psychology of the Climate Crisis”. Noah and I had discussed teaching such a class for several years, and finally had the chance to do it this year. The group of students in the class ended up being quite small, with four undergradates, two Master’s students, and two PhD students, hailing from six different departments across the university. Fortunately, these were amongst the most engaged and dedicated students that I have ever taught in my entire career. The small size of the class also allowed us to get to know the students very well and give them more agency in designing the class.\nThe goal of this post is to catalog our experiences in teaching this class, in hopes that they might inspire others to teach similar courses."
  },
  {
    "objectID": "posts/psychology-climate-change/psychology-climate-change.html#teaching-the-psychology-of-the-climate-crisis",
    "href": "posts/psychology-climate-change/psychology-climate-change.html#teaching-the-psychology-of-the-climate-crisis",
    "title": "Teaching the Psychology of the Climate Crisis",
    "section": "",
    "text": "“It is now incontrovertible that human behavior has led to significant changes in the Earth’s climate, and that these changes will inflict an increasingly significant toll on human wellbeing if they are not reversed. It is also clear that both the opportunities and obstacles to the needed changes are largely psychological and behavioral.”\n\nSo began the syllabus for a course that Noah Goodman and I taught together in the Spring Quarter of 2023 at Stanford, titled “Psychology of the Climate Crisis”. Noah and I had discussed teaching such a class for several years, and finally had the chance to do it this year. The group of students in the class ended up being quite small, with four undergradates, two Master’s students, and two PhD students, hailing from six different departments across the university. Fortunately, these were amongst the most engaged and dedicated students that I have ever taught in my entire career. The small size of the class also allowed us to get to know the students very well and give them more agency in designing the class.\nThe goal of this post is to catalog our experiences in teaching this class, in hopes that they might inspire others to teach similar courses."
  },
  {
    "objectID": "posts/psychology-climate-change/psychology-climate-change.html#syllabus",
    "href": "posts/psychology-climate-change/psychology-climate-change.html#syllabus",
    "title": "Teaching the Psychology of the Climate Crisis",
    "section": "Syllabus",
    "text": "Syllabus\nSince neither Noah nor myself had any expertise in climate psychology, we decided to work together with the students to put together the syllabus; in essence they got to experience the same kind of educational journey that we did. The overall structure and set of topics that we ultimately covered was decided in the first class session, based on a starting set of topics derived primarily from our reading of the 2009 APA Report on Psychology and Global Climate Change.\nThe other major event of the first week was a talk to the class by Adam Aron, who happened to be visiting campus that week. Adam has been at the forefront of advocacy around the climate crisis, and has recently written a very nice book on the topic, which includes a substantial amount of discussion around relevant psychological issues.\nThe first session of each week (Tuesday) was dedicated to student-led discussion of papers regarding the week’s subtopic. The students were also responsible for selecting the papers. Because of the small number of students in the class, we assigned the leadership of the discussion each Tuesday to one or two students. The second session each week was dedicated to working on the final project (discussed below) or to wide-ranging discussions of relevant topics.\n\n\n\nWeek\nTopic\nReadings\n\n\n\n\n1\nOverview and introductory talk by Adam Aron\n\n\n\n2\nPerceiving and understanding risk and uncertainty\nWeber (2006), Zaval et al., 2014)\n\n\n3\nIntertemporal choice and future-oriented thinking\nMaiella et al. (2020), Kim & Ahn (2019)\n\n\n4\nBeliefs about climate change: Susceptibility to misinformation and motivated reasoning\nDruckman & McGrath (2019), Hart & Nisbet (2012)\n\n\n5\nBehavior change\nBamberg et al. (2015), Goldberg et al. (2020)\n\n\n6\nPersonal and group identity\nDoell et al (2021), Grindal et. al (2023)\n\n\n7\nCooperation and social norms\nLatkin, et al. (2021) , Constantino, et al. (2022)\n\n\n8\nCollective behavior (game theory / networks)\nTavoni and Iris (2020) , Diaz et al., (2012)\n\n\n9\nPsychological interventions to address climate change\nBergquist et al. (2023), Dietz et al. (2009)\n\n\n10\nWrapup\n\n\n\n\nWith only ten weeks there were a number of topics that we were not able to cover. One important one, which is the focus of recent work by Adam Aron and colleagues, is the psychology of collective action; that is, what are the psychological factors that drive people to become involved in larger groups and ultimately to become activists. Another important topic that we didn’t touch upon is the psychological impacts of climate change, though this topic came up often in our discussions."
  },
  {
    "objectID": "posts/psychology-climate-change/psychology-climate-change.html#class-project-an-open-source-book",
    "href": "posts/psychology-climate-change/psychology-climate-change.html#class-project-an-open-source-book",
    "title": "Teaching the Psychology of the Climate Crisis",
    "section": "Class project: An open-source book",
    "text": "Class project: An open-source book\nIn planning the class I floated the somewhat off-the-wall idea of having the students work together to generate an open source e-book as their final project. I’ve done this once before and thought the students found it quite fulfilling to generate a resource that will live beyond the end of the quarter.\nIn the first few weeks we discussed the chapter topics (which aligned roughly with the syllabus topics), and the students worked together to identify potential synergies and overlaps between their intended topics. In Week 5 students submitted a one-page topic proposal with proposed references, which were peer-reviewed by other students as well as by the instructors. An initial draft of the chapter was due in Week 8, which were also peer reviewed, and the final draft was due during Finals Week (Week 11). The chapters will be edited by the instructors over the summer and will be released as an open-source book later this summer."
  },
  {
    "objectID": "posts/psychology-climate-change/psychology-climate-change.html#major-takeaways",
    "href": "posts/psychology-climate-change/psychology-climate-change.html#major-takeaways",
    "title": "Teaching the Psychology of the Climate Crisis",
    "section": "Major takeaways",
    "text": "Major takeaways\nA first takeaway for me was reframing where I think the problem really lies. I went into the course thinking about climate change as primarily a problem of behavior change, just like the many other habits that I wrote about in my book, Hard to Break. I still think that the stickiness of behavior is a central problem, but I am less convinced that an understanding of the cognitive mechanisms underlying this stickiness is actually useful for understanding how to actually address the climate crisis. Instead, I have become increasingly convinced that the most important psychological levers are social, particularly regarding social norms. The paper from our syllabus by Constantino et al (2022) lays this out nicely. This is also fairly clear from the meta-analysis of intervention effects by Bergquist et al., 2023, which showed that inerventions based on social comparison had the largest effects.\nSecond, I am less convinced after the class that psychological interventions are going to be an important part of the solution, even though psychological research is clearly important for understanding the important levers for intervention. This in part driven by the empirical evidence from the aforementoined large second-order meta-analysis by Berquist et al., (2023), which showed that the effects of psychological interventions were moderate at best, and became quite small after bias correction. More importantly, I think that making major changes in behavior is going to require large-scale social norm changes; in particular, in the context of the US, massive changes in behavior could occur relatively quickly if Republican leaders were to begin advocating for the importance of action to address the climate crisis and thus changing social norms within their community. Given the existing evidence regarding psychological interventions, it’s just not clear to me that they offer any hope on their own of addressing the climate crisis in the near term. I should note that the work on collective action that I mentioned above may provide useful interventions that could have larger impacts than those targeted at the individual.\nIf there is one important new concept that I learned in the class, it is pluralistic ignorance, which is the idea that a group of individuals can systematically misunderstand what others in their group believe. In the context of climate change, a compelling example comes from Gregg Sparkman and colleagues, who found:\n\n“80–90% of Americans underestimate the prevalence of support for major climate change mitigation policies and climate concern. While 66–80% Americans support these policies, Americans estimate the prevalence to only be between 37–43% on average. Thus, supporters of climate policies outnumber opponents two to one, while Americans falsely perceive nearly the opposite to be true.”\n\nIt’s very clear that this kind of systematic misunderstanding is a serious impediment to any kind of climate action at the national level. Unfortunately it’s less clear how we fix this problem.\nNoah raised another important takeaway: Climate change is a case study for “psychology in the wild” that helps bring together many different psychological issues that are often studied in different subfields. A few examples would include: behavioral decision making, game theory, intertemporal choice, belief updating, behavior change, and social norms. Thinking about the larger topic of climate change required bringing all of these together in a way that was quite illuminating for both of us.\nFinally, I must mention the methodological issues within the literature that we read. Our class discussions of the readings tended heavily towards deep dives into the methodology of each study (perhaps not surprisingly given the instructors), and many of the empirical studies we examined had fairly significant methodological shortcomings. In a number of cases there were methodological choices made that were non-standard and not well motivated, which for me evokes a sense of p-hacking. In fact, there is evidence for at least some degree of p-hacking in this literature from the p-curve analysis presented by Bergquist et al in the meta-analysis. Figure D from Appendix B shows the bump in p-values just below .05 that would be expected if researchers were exploiting analytic variability in order to find a signficant effect:\n\nIt was also disappointing that few of the studies shared their data and none (to my memory) of the empirical studies we read were pre-registered. There was also a particularly heavy use of mediation analysis in this literature, which is problematic for reasons outlined by Julia Rohrer and colleagues (and recently described by Richard Mcelreath as a “dumpster fire”). I hope that as methodological standards in the field increase that this area will come along, but at present the poor methodological standards in the published work make it difficult to form very strong conclusions.\nI hope this overview is helpful for those of you who are thinking of teaching on the climate crisis. Despite the challenges that I outlined above, both of us found this to be a notably fulfilling teaching experience, due in large part to the passion that our students felt around the topic. I also hope that if you do research or teach on this topic that you will consider contributing to our open source book, to help make it a more robust resource for people interested in the topic.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/things-i-like-to-do-in-beijing/index.html",
    "href": "posts/things-i-like-to-do-in-beijing/index.html",
    "title": "Things I like to do in Beijing",
    "section": "",
    "text": "This was originally posted on blogger here.\nSeveral friends have asked for suggestions about things to do while in Beijing for the OHBM meeting this June. I don’t have any particular wisdom, but having visited several times I thought that I would share a few of my favorite things (along with photos from some of our past trips). I’m not including many of the obvious attractions (Forbidden City, Summer Palace, Olympic Park) because I figure those will be in every guide book.798 Art Complex: an amazing art complex created from an old factory complex. If you like modern art you can easily spend more than 1/2 a day there. There is a great noodle shop tucked away in the middle of the complex, and also at least one really nice cafe. Hutongs: These are the old neighborhoods in the center of the city. Some of them are very touristy, but if you walk a few blocks off the main street you can find some streets that feel pretty far from touristy. I would particularly recommend the area around Heizhima Hutong, where these photos were taken. Drum Tower: This was built in 1272 and served as the official timepiece of the Chinese government until 1924. If you show up at the right time, you can see an awesome drumming show. Great Wall: We visited the Great Wall at Badaling in 2005, which is apparently the most touristy place to go but also relatively close to Beijing. Go early in the day, to avoid both crowds and heat. Perhaps the best part of visiting at Badaling is that there is a roller coaster that can take you down from the top. EatingRoasted duck hearts at QuanjudeWe have had a lot of wonderful meals in Beijing, both as fish-etarians on our first two visits and as omnivores on our most recent visit. Prepare to eat well, but also be prepared to have your sensibilities challenged. A few highlights are:Roast duck: As recently reformed vegetarians, we spent our first two visits to China without trying “Peking Duck” (or, as they call it in Beijing, “roast duck”). On our last visit we had it at Quanjude and it was pretty awesome. We had the full on roast duck experience, including “duck breast” (which is basically just fat and skin) and roasted duck hearts. A must-have. (NB: If you order the duck hearts, they come with a bowl of flaming liquid. Apparently you are not supposed to actually dip the heart into the liquid, as I did.)Spicy snails at Spicy Grandma restaurantSichuan food: Our friends in China are largely from Sichuan province, and thus we often end up eating at Sichuan restaurants. The Sichuan peppercorn has an amazing numbing quality. Also be sure to try the Sichuan hot pot, which is like a very spicy version of shabu shabu. I would suggest bringing a significant ration of Pepto Bismol, as the western gut starts to ache after a few days of this kind of spicy food. But it is so worth the burn. Yunnan food: One of the most amazing meals we had was at the Rainbow Restaurant in the Beijing Sun Palace Hotel. The greeters are dressed in traditional Yunnan dress, and the food is absolutely amazing with a heavy focus on mushrooms. A dish that contained “smelly tofu” - actually really tastyGrilled matsutake musrooms at Rainbow"
  },
  {
    "objectID": "posts/things-i-like-to-do-in-beijing/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/things-i-like-to-do-in-beijing/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Things I like to do in Beijing",
    "section": "1 comments captured from original post on Blogger",
    "text": "1 comments captured from original post on Blogger\njen o said on 2012-04-25\nI also highly recommend a visit to the Lama Temple (Yonghe Temple) to view the various halls and the 85-foot tall Maitreya buddha–at one time the largest statue of buddha on earth. If you are lucky you can be there during a tibetan prayer service featuring hundreds of monks chanting in a candle-lit and incense-infused hall. truly mesmerizing.http://en.wikipedia.org/wiki/Yonghe_Temple"
  },
  {
    "objectID": "posts/how-can-one-do-reproducible-science/index.html",
    "href": "posts/how-can-one-do-reproducible-science/index.html",
    "title": "How can one do reproducible science with limited resources?",
    "section": "",
    "text": "This was originally posted on blogger here.\nWhen I visit other universities to talk, we often end up having free-form discussions about reproducibility at some point during the visit. During a recent such discussion, one of the students raised a question that regularly comes up in various guises. Imagine you are a graduate student who desperately wants to do fMRI research, but your mentor doesn’t have a large grant to support your study. You cobble together funds to collect a dataset of 20 subjects performing your new cognitive task, and you wish to identify the whole-brain activity pattern associated with the task. Then you happen to read “Scanning the Horizon” which points out that a study with only 20 subjects is not even sufficiently powered to find the activation expected from a coarse comparison of motor activity to rest, much less to find the subtle signature of a complex cognitive process. What are you to do?In these discussions, I often make a point that is statistically correct but personally painful to our hypothetical student: The likelihood of such a study identifying a true positive result if it exists is very low, and the likelihood of any positive results being false is high (as outlined by Button et al, 2013), even if the study was fully pre-registered and there is no p-hacking. In the language of clinical trials, this study is futile, in the sense that it is highly unlikely to achieve its aims. In fact, such a study is arguably unethical, since the (however miniscule) risks of participating in the study are not offset by any potential benefit to the subject or to society. This raises a dilemma: How are students with limited access to research funding supposed to gain experience in an expensive area of research and test their ideas against nature? I have struggled with how to answer these questions over the last few years. I certainly wouldn’t want to suggest that only students from well-funded labs or institutions should be able to do the science that they want to do. But at the same time, giving students a pass on futile studies will have dangerous influence, since many of those studies will be submitted for publication and will thus increase the number of false reports (positive or negative) in the literature. As Tal Yarkoni said in his outstanding “Big Correlations in Little Studies” paper:Consistently running studies that are closer to 0% power than to 80% power is a sure way to ensure a perpetual state of mixed findings and replication failures.Thus, I don’t think that the answer is to say that it’s OK to run underpowered studies. In thinking about this issue, I’ve come up with a few possible ways to address the challenge. 1) “if you can’t answer the question you love, love the question you can”In an outstanding reflection published last year in the Journal of Neuroscience, Nancy Kanwisher said the following in the context of her early work on face perception:I had never worked on face perception because I considered it to be a special case, less important than the general case of object perception. But I needed to stop messing around and discover something, so I cultivated an interest in faces. To paraphrase Stephen Stills, if you can’t answer the question you love, love the question you can.In the case of fMRI, one way to find a question that you can answer is to look at shared datasets. There is now a huge variety of shared data available from resources including OpenfMRI/OpenNeuro, FCP/INDI, ADNI, the Human Connectome Project, and OASIS, just to name a few. If a relevant dataset is not available openly but you know of a paper where someone has reported such a dataset, you can also contact those authors and ask whether they would be willing to share their data (often with an agreement of coauthorship). An example of this from our lab is a recent paper by Mac Shine (published in Network Neuroscience), in which he contacted the authors of two separate papers with relevant datasets and asked them to share the data. Both agreed, and the results came together into a nice package. These were pharmacological fMRI studies that would not have even been possible within my lab, so the sharing of data really did open up a new horizon for us. Another alternative is to do a meta-analysis, either based on data available from sites like Neurosynth or Neurovault, or by requesting data directly from researchers. As an example, a student in one of my graduate classes did a final project in which he requested the data underlying meta-analyses published by two other groups, and then combined these to perform a composite meta-analysis, which was ultimately published. 2) Focus on cognitive psychology and/or computational models for now One of my laments regarding the training of cognitive neuroscientists in today’s climate is that their training is generally tilted much more strongly towards the neuroscience side (and particularly focused on neuroimaging methods), at the expense of training in good old fashioned cognitive psychology. As should be clear from many of my writings, I think that a solid training in cognitive psychology is essential in order to do good cognitive neuroscience; certainly just as important as knowing how to properly analyze fMRI data. Increasingly, this means thinking about computational models for cognitive processes. Spending your graduate years focusing on designing cognitive studies and building computational models of them will put you in an outstanding position to get a good postdoc in a neuroimaging lab that has the resources to support the kind of larger neuroimaging studies that are now required for reproducibility. I’ve had a couple of people from pure cognitive psychology backgrounds enter my lab as postdocs, and their NIH fellowship applications were both funded on the first try, because the case for additional training in neuroscience was so clear. Once you become skilled at cognition and (especially) computation, imaging researchers will be chomping at the bit to work with you (I know I would!). In the meantime you can also start to develop chops at neuroimaging analysis using shared data as outlined in #1 above. 3) Team up The field of genetics went through a similar reckoning with underpowered studies more than a decade ago, and the standard in that field is now for large genome-wide association studies which often include tens of thousands of subjects. They also usually include tens of authors on each paper, because amassing such large samples requires more resources than any one lab can possess. This strategy has started to appear in neuroimaging through the ENIGMA consortium, which has brought together data from many different imaging labs to do imaging genetics analyses. If there are other labs working on similar problems, see if you can team up with them to run a larger study; you will likely have to make compromises, but a reproducible study is worth it (cf. #1 above). 4) Think like a visual neuroscientist This one won’t work for every question, but in some cases it’s possible to focus your investigation on a much smaller number of individuals who are characterized much more thoroughly; instead of collecting an hour of data each on 20 people, collect 4 hours of data per person on 5 people. This is the standard approach in visual neuroscience, where studies will often have just a few subjects who have been studied in great detail, sometimes with many hours of scanning per individual (e.g. see any of the recent papers from Jack Gallant’s lab for examples of this strategy). Under this strategy you don’t use standard group statistics, but instead present the detailed results from each individual; if they are consistent enough across the individuals then this might be enough to convince reviewers, though the farther you get from basic sensory/motor systems (where the variance between individuals is expected to be relatively low) the harder it will be to convince them. It is essential to keep in mind that this kind of analysis does not allow one to generalize beyond the sample of individuals who were included in the study, so any resulting papers will be necessarily limited in the conclusions they can draw. 5) Carpe noctem At some imaging centers, the scanning rates become drastically lower during off hours, such that the funds that would buy 20 hours of scanning during prime time might stretch to buy 50 or more hours late at night. A well known case is the Midnight Scan Club at Washington University, which famously used cheap late night scan time to characterize the brains of ten individuals in detail. Of course, scanning in the middle of the night raises all sorts of potential issues about sleepiness in the scanner (as well in the control room), so it shouldn’t be undertaken without thoroughly thinking through how to address those issues, but it has been a way that some labs have been able to stretch thin resources much further. I don’t want this to be taken as a suggestion that students be forced to work both day and night; scanning into the wee hours should never be forced upon a student who doesn’t want to do it, and the rest of their work schedule should be reorganized so that they are not literally working day and night. I hope these ideas are useful - If you have other ideas, please leave them in the comments section below! (PS: Thanks to Pat Bissett and Chris Gorgolewski for helpful comments on a draft of this piece!)"
  },
  {
    "objectID": "posts/how-can-one-do-reproducible-science/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/how-can-one-do-reproducible-science/index.html#comments-captured-from-original-post-on-blogger",
    "title": "How can one do reproducible science with limited resources?",
    "section": "2 comments captured from original post on Blogger",
    "text": "2 comments captured from original post on Blogger\nUnknown said on 2018-04-18\nFor suggestion #3, StudySwap was created to provide a solution to this problem: https://osf.io/view/StudySwap/\nRuss Poldrack said on 2018-04-18\nthanks!"
  },
  {
    "objectID": "posts/home-office-setup/index.html",
    "href": "posts/home-office-setup/index.html",
    "title": "Home office setup",
    "section": "",
    "text": "This was originally posted on blogger here.\nOne of my colleagues recently asked me about my home office setup, after noting that my video and audio quality is generally quite good on our frequent Zoom calls. Whenever anyone asks me a question like this, I take it as a good excuse to write a blog post! We have all spent a lot of time in our home offices since March, and I’m lucky that we have a guest bedroom that I was able to repurpose as my home office. I’ve ended up spending a bit of money to make it nice, but I think in general the investments have been good. However, I’ve also gone cheap/DIY when I can. Here is a photo of my desk setup:Here’s a quick rundown of the various items:Camera: Logitech C930e Microphone: Audio-Technica ATR2100x-USB with Sterling Audio Sterling SM5 Shock Mount Pop Filter: Stedman Proscreen XL Mic boom: Rode PSA1 Headphones (over-ear): Audio-Technica ATH-AD700X Open-air Headphones (in-ear): Apple AirPods Lighting: Homemade diffusers with Cree 5000K LED bulbs Green screen: Homemade Chair: Steelcase LeapCamera: This webcam was scavenged from my lab at the beginning of the pandemic, back when it was impossible to find a webcam in stock for purchase. It works fine, though I wouldn’t say that the picture quality is amazing. After seeing one of my colleagues get amazing video quality by using their DSLR as a webcam, I tried it out with our relatively ancient Canon Rebel - the color was much better but its video was way too laggy, and the camera/tripod setup took up too much room on my desk, so I’ve stuck with the Logitech. I use the Webcam Settings App for Mac to zoom the image so that my head takes up most of the image without having to lean into the camera. Microphone setup: I wanted to get a boom mic rather than a stand mic, mostly because I didn’t want a stand mic taking up extra space on my desktop. I know that many people use either a lapel mic or a mic integrated into their headset, but neither of those sounded attractive to me. The microphone connects via USB to my computer, and works really well. I went for a nicer mic in part because I was planning to record an audio version of my statistics book to provide to my students, and I’ve been really happy with the sound quality. The shock mount does a good job of isolating low-frequency noise from the desk, though a tiny bit of keyboard noise is evident when I’m typing, even with the mic pointed directly away from the keyboard. The Rode mic stand can sometimes be difficult to keep in position, but works fine for my purposes. I don’t use the popscreen for Zoom calls, but it has been important for recording spoken word material, which otherwise sounds like I’m spitting on the listener. Headphones: I generally alternate between in-ear and over-ear headphones over the day. I love the AirPods, but after a while they start hurting my ears, and they don’t have enough battery life to get me through a full day of Zoom meetings. The Audio-Technica headphones were my first open-back headphones, and I am definitely a convert - they let you hear the outside world, and don’t leave you with that closed-in feel that you get from closed-back headphones. They are also super comfortable. These are standard wired headphones, which I like both because they don’t have a lag like bluetooth headphones (not so important for Zoom calls but essential when I’m playing guitar), and also because I will never be stuck with a dead battery. Lighting: Everything else involved buying some equipment, so for the lighting setup I decided to go DIY (with lots of help and encouragement from my designer/wife Jen). I wanted a simple two-point lighting setup from the two sides of my monitor, so we started with a couple of old table lamps that we had around the house. I took a couple of empty wooden picture frames and attached each one to one of the arms of the lamp using a plastic cable stay, which is not exactly bulletproof but so far as lasted several months without failing. To create a diffuser I started with some architectural tracing paper which I affixed in a sleeve around the picture frames. ultimately this wasn’t quite enough diffusion (I was still seeing strong reflections of the light in my glasses), so I also attached a piece of standard printer paper to the front with a binder clip. I still get a bit of point glare, but it’s not too bad:I’ll probably try to do some more tweaking to resolve that. We started with some warmer bulbs but I didn’t love the color, so I replaced them with Cree 5000K LED bulbs which I’m pretty happy with.Green screen: I don’t usually use a green screen, but sometimes I need it if I want to play with video editing software for lecture videos. This one is also DIY - basically a wheeled clothing rack with a green fleece blanket attached using some large binder clips. Definitely not pretty, but gets the job done. Chair: After spending the first few months of quarantine sitting in a cheapo office chair (and feeling the effects by the end of the day), I decided to splurge on a serious office chair. I already had a Steelcase Leap in my campus office, so I knew I would be happy with it. It has not disappointed - it’s definitely not cheap, but if you need a really good chair and have the budget I would definitely recommend it. Your butt will thank you! I’m interested to hear your thoughts and any tips on how to further optimize the setup."
  },
  {
    "objectID": "posts/home-office-setup/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/home-office-setup/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Home office setup",
    "section": "3 comments captured from original post on Blogger",
    "text": "3 comments captured from original post on Blogger\nRuss Poldrack said on 2020-10-13\nSome great additional tips from Fernando Pérez here: https://twitter.com/fperez_org/status/1316159055333785601?s=20\nUnknown said on 2020-10-15\nRuss, Love your setup. Being retired now, I am especially interested in your water/wine glass with what looks like a silicon protector sleeve. Where did you get that?\nUnknown said on 2021-01-16\nThey are from Lifefactory: https://www.lifefactory.com/glassware/wine-tumblers-large"
  },
  {
    "objectID": "posts/defaults-in-r-can-make-debugging/index.html",
    "href": "posts/defaults-in-r-can-make-debugging/index.html",
    "title": "Defaults in R can make debugging incredibly hard for beginners",
    "section": "",
    "text": "This was originally posted on blogger here.\nI am teaching a new undergraduate statistics class at Stanford, and an important part of the course is teaching students to run their own analyses using R/RStudio. Most of the students have never coded before, and debugging turns out to be one of the major challenges. Working with students over the last few days I have found that a couple of the default features in R can combine to make debugging very difficult on occasion. Changing these defaults could have a big impact on new users’ early learning experiences.One of the datasets that we use is the NHANES dataset via the NHANES library. Over the last few days several students have experienced very strange problems, where the NHANES data frame doesn’t contain the appropriate data, even after restarting R and reloading the NHANES library. It turns out that this is due to several “features” in R: Users are asked when exiting whether to save the workspace image, and the default is to save it.The global workspace (saved in ~/.RData) is by default automatically loaded upon starting R.When a package is loaded that contains a data object, this object is masked by any object in the global workspace with the same name. Here is an example. First I load the NHANES library, and check that the NHANES data frame contains the appropriate data. &gt; library(NHANES) &gt; head(NHANES) ID SurveyYr Gender Age AgeDecade AgeMonths Race1 Race3 Education MaritalStatus HHIncome HHIncomeMid 1 51624 2009_10 male 34 30-39 409 White  High School Married 25000-34999 30000 2 51624 2009_10 male 34 30-39 409 White  High School Married 25000-34999 30000 3 51624 2009_10 male 34 30-39 409 White  High School Married 25000-34999 30000 4 51625 2009_10 male 4 0-9 49 Other    20000-24999 22500 5 51630 2009_10 female 49 40-49 596 White  Some College LivePartner 35000-44999 40000 6 51638 2009_10 male 9 0-9 115 White    75000-99999 87500 Now let’s say that I accidentally set NHANES to some other value: NHANES=NA &gt; NHANES [1] NA Now I quit RStudio, clicking the default “Save” option to save the workspace, and then restart RStudio. I get a message telling me that the workspace was loaded, and I see that my altered version of the NHANES variable still exists. I would think that reloading the NHANES library should fix this, but this is what happens: &gt; library(NHANES) Attaching package: ‘NHANES’ The following object is masked by ‘.GlobalEnv’: NHANES &gt; NHANES [1] NA That is, objects in the global environment take precedence over newly loaded objects. If one didn’t know how to parse that warning they would have no idea that this loading operation is having no effect. The only way rid ourselves of this broken variable is either restart R after removing ~/.RData, or remove the variable from the global workspace: &gt; rm(NHANES, envir = globalenv()) &gt; library(NHANES) &gt; head(NHANES) ID SurveyYr Gender Age AgeDecade AgeMonths Race1 Race3 Education MaritalStatus HHIncome HHIncomeMid 1 51624 2009_10 male 34 30-39 409 White  High School Married 25000-34999 30000 2 51624 2009_10 male 34 30-39 409 White  High School Married 25000-34999 30000 3 51624 2009_10 male 34 30-39 409 White  High School Married 25000-34999 30000 4 51625 2009_10 male 4 0-9 49 Other    20000-24999 22500 5 51630 2009_10 female 49 40-49 596 White  Some College LivePartner 35000-44999 40000 6 51638 2009_10 male 9 0-9 115 White    75000-99999 87500 This seems like a combination of really problematic default behaviors to me: automatically saving and then loading the global workspace by default, and masking objects loaded from libraries with objects in the workspace. Together they have resulted in hours of unnecessary confusion and frustration for my students, at exactly the point in their learning curve where it is most problematic to do so.I have one simple suggestion for the R developers: Please turn off automatic loading of the workspace by default. It would be as simple as changing the default on one radio box, and it would potentially save new users lots of time and frustration. Until that happens, beginning R users should do the following: Under the Preferences panel (the General Tab in R), unselect the “Restore .RData into workspace on startup” option. I would also recommend setting the “Save workspace to .RData on exit” preference to “Never”, since I find that I generally only restart R when I want the entire workspace cleared out, so this option will never be of use to me."
  },
  {
    "objectID": "posts/defaults-in-r-can-make-debugging/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/defaults-in-r-can-make-debugging/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Defaults in R can make debugging incredibly hard for beginners",
    "section": "1 comments captured from original post on Blogger",
    "text": "1 comments captured from original post on Blogger\nmuswellbrook said on 2018-01-22\nI agree. I found exactly the same problem when I first started using R and quickly discovered the solutions you describe here. But the problem is the quirky default behavior. And I suspect most experienced R users don’t even think about it anymore (because they have changed their default settings). But it is annoying and especially so when you discover there is no appropriate command to clear the workspace and the recommended method is to restart R"
  },
  {
    "objectID": "posts/statistical-thinking-for-21st-century/index.html",
    "href": "posts/statistical-thinking-for-21st-century/index.html",
    "title": "Statistical Thinking for the 21st Century - a new intro statistics book",
    "section": "",
    "text": "This was originally posted on blogger here.\nI have published an online draft of my new introductory statistics book, titled “Statistical Thinking for the 21st Century”, at http://thinkstats.org. This book was written for my undergraduate statistics courseat Stanford, which I started teaching last year. The first time around I used Andy Field’s An Adventure in Statistics, which I really like but most of my students disliked because the statistical content was buried within a lot of story. In addition, there are a number of topics (overfitting, cross-validation, reproducibility) that I wanted to cover in the course but weren’t covered deeply in the book. So I decided to write my own, basically transcribing my lecture slides into a set of RMarkdown notebooks and generating a book using Bookdown.There are certainly many claims in the book that are debatable, and almost certainly things that I have gotten wrong as well, given that I Am Not A Statistician. If you have the time and energy, I’d love to hear your thoughts/suggestions/corrections - either by emailing me, or by posting issues at the github repo. I am currently looking into options for publishing this book in low-cost paper form - if you would be interested in using such a book for a course you teach, please let me know. Either way, the electronic version will remain freely available online."
  },
  {
    "objectID": "posts/statistical-thinking-for-21st-century/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/statistical-thinking-for-21st-century/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Statistical Thinking for the 21st Century - a new intro statistics book",
    "section": "5 comments captured from original post on Blogger",
    "text": "5 comments captured from original post on Blogger\nUnknown said on 2018-11-20\nI teach undergrad stats tobpsych majors… Id be interested in the paper version.\nUnknown said on 2018-11-20\nGive a free copy and all Indians learn it freely.\nOpher Donchin said on 2018-11-21\nHello Russ, this book looks really good. I have been teaching statistics to biomedical engineers out of Slinker and Glanz Primer of Applied Regression for many years now and your book has a bunch of advantages. There are two things that will slow me down and I wonder if they aren’t easy fixes (or things I could help with, if you’re interested): (1) exercises at the end of chapters help enormously when using a textbook; I understand you may have these available from teaching the course. (2) slightly more mathematical sophistication, especially in building the relationship between the chi squared distribution and the F and t tests and for understanding the relationship between the least squared fits and the parameters estimates in a regression model. I can understand the math might make more sense as optional chapters, given the spirit of the book, but I think they’re important for an audience with a little more technical background.One more thing that I thought just now is that the Bayesian chapter seems to me to undersell the power of the Bayesian approach. Bayesian methods seem to me to take their power from the shift in emphasis from hypothesis testing to parameter estimation and from the flexibility in model selection that comes with an approach based in JAGS or Stan. I’m not sure how to fix this, but the current chapter seems problematic to me.I solve the Bayes problem by teaching a classical course to undergraduates and a Bayesian course to graduate students. It gives me the opportunity to pretend I’m like the physicists who get to introduce modern physics by telling the students everything they learned so far is completely wrong.\nRuss Poldrack said on 2018-11-21\nThanks Opher! I agree that it would be nice to have more mathematical sophistication as well as exercises. Since I haven’t used the book yet in the course (will do so starting in January) I don’t have a well fleshed out set of exercises, but I hope to add them in the future. re: the Bayes chapter, I agree that it’s limited, but that’s largely due to the fact that this is meant for an audience without the level of mathematical or computational sophistication to handle working directly with something like JAGS or STAN. I wanted to still give them a feel for the Bayesian approach, without scaring them off. I can see a couple of alternatives - one would be to include some additional sections that one could unfold for greater detail (I think that’s possible with Gitbooks but need to figure out details). Another would be to have multiple versions of the book, with different degrees of mathematical detail. Once I’ve taught with it this winter, I will reconsider these options and hope to get back to you at that point. would love your input and contributions!\nRuss Poldrack said on 2018-11-21\ngreat, thanks Ben!"
  },
  {
    "objectID": "posts/scam-journals-will-literally-publish/index.html",
    "href": "posts/scam-journals-will-literally-publish/index.html",
    "title": "Scam journals will literally publish crap",
    "section": "",
    "text": "This was originally posted on blogger here.\nIn the last couple of years, researchers have started to experience an onslaught of invitations to attend scam conferences and submit papers to scam journals. Many of these seem to emanate from the OMICS group of Henderson, NV and its various subsidiaries. A couple of months ago I decided to start trolling these scammers, just to see if I could get a reaction. After sending many of these, I finally got a response yesterday, which speaks to the complete lack of quality of these journals. This was the solicitation:On May 20, 2016, at 12:55 AM, Abnormal and Behavioural Psychology &lt;behaviouralpsychol@omicsinc.com&gt; wrote: Dear Dr. Russell A. Poldrack,Greetings from the Journal of Abnormal and Behavioural PsychologyJournal of Abnormal and Behavioural Psychology is successfully publishing quality articles with the support of eminent scientists like you.We have chosen selective scientists who have contributed excellent work, Thus I kindly request you to contribute a (Research, Review, Mini Review, Short commentary) or any type of article.The Journal is indexed in with EBSCO (A-Z), Google Scholar, SHERPA-Romeo, Open J-gate, Journal Seek, Electronic Journals Library, Academic Keys, Safety Lit and many more reputed indexing databases. We publish your manuscript within seven days of Acceptance. For your Excellent Research work we are offering huge discount in the publishing fee (70%). So, we will charge you only 300 USD. This huge offer we are giving in this month only. …With kind regardsSincerely,Joyce V. AndriaI had previously received exactly this same solicitation about a month ago, to which I had responded like this:Dear Ms Andria, Thanks for your message. I just spent three minutes reading and thinking about your email. My rate for commercial consulting is $500/hour. Can you please remit your payment of $25 to me at the address below? I’m sure you can understand that the messages from your organization take valuable time away from scientists, and that you would agree that it’s only fair to renumerate us for this time.I look forward to receiving your payment promptly. If you do remit within 30 days I will be forced to send this invoice out for collection.Sincerely,Russ PoldrackI got no response to that message. So when I received the new message, I decided to step up my troll-fu:Dear Ms. Andria,Many thanks for your message soliciting a (Research, Review, Mini Review, Short commentary) or any type of article for your journal. I have a paper that I would like to submit but I am not sure what kind of article it qualifies as. The title is “Tracking the gut microbiome”. The paper does not include any text; it is composed entirely of photos of my bowel movements taken every morning for one year. Please let me know if your journal has the capability to publish such a paper; I have found that many other journals are not interested.Sincerely,Russell PoldrackWithin 12 hours, I had a response: From: Abnormal and Behavioural Psychology behaviouralpsychol@omicsinc.comSubject: RE: Appreciated your Excellent Research workDate: May 20, 2016 at 9:47:28 PM PDTTo: “‘Russell Alan Poldrack’” russpold@stanford.eduDear Dr. Russell A. Poldrack,Greetings from the Journal of Abnormal and Behavioural PsychologyThank you for your reply.I hereby inform you that your article entitled: “Tracking the gut microbiome” is an image type article.We are happy to know that you want to publish your manuscript with us.We are waiting for your earliest submission.We want to introduce your research work in this month to our Journal. We will be honored to be a part of your scientific journey.Kindly submit your article on before 26th may, 2016.Awaiting your response.,With kind regardsSincerely,Anna WatsonJournal CoordinatorJournal of Advances in Automobile EngineeringThere you have it: These journals will literally publish complete crap. I hope the rest of you will join me in trolling these parasites - post your trolls and any results in the comments."
  },
  {
    "objectID": "posts/scam-journals-will-literally-publish/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/scam-journals-will-literally-publish/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Scam journals will literally publish crap",
    "section": "4 comments captured from original post on Blogger",
    "text": "4 comments captured from original post on Blogger\nyarikoptic said on 2016-05-21\nI am pleased that you took my comment to your tweet about http://www.theverge.com/2016/5/4/11581994/fmt-fecal-matter-transplant-josiah-zayner-microbiome-ibs-c-diff so seriously :-)\nC.A. said on 2016-05-24\nI had this little exchange with the journal \"Translational Brain Rhythmicity\":———————————————-Dear Dr. Carsten AllefeldGreetings from Translational Brain RhythmicityHope everything is well with you!I would like to invite you to contribute a manuscript for theupcoming issue of the journal. All submitted manuscripts will bereviewed under the supervision of Dr. Marco Weiergräber.[…]Regards,David Randall———————————————-My reply:———————————————-Dear David,thank you for your invitation. A question: Does your journal also accept contributions in the field of rotational brain rhythmicity?Best,Carsten———————————————-An theirs:———————————————-Yes, we do accept papers on rotational brain rhythmicity. Please, let me know by what date you will be able to submit the manuscript.———————————————-;-)\nRuss Poldrack said on 2016-05-24\nawesome!\nUnknown said on 2017-02-09\nI received one this morning from Anna Watson. Then I googled this name and the Journal name Advances in Automobile engineering. On the Journal’s website, I can’t find his name, making me feel suspicious. Then I saw your blog. Thanks, Russ!"
  },
  {
    "objectID": "posts/error-tight/index.html",
    "href": "posts/error-tight/index.html",
    "title": "Reading ‘Error Tight’ by Julia Strand",
    "section": "",
    "text": "Today in our lab meeting we read “Error Tight: Exercises for Lab Groups to Prevent Research Mistakes” by Julia Strand. It’s a great paper and led to some really fun discussions!\nThe paper makes the point that errors are a necessary feature of human activity, including science, and that there are well-established methods from human factors research to reduce the frequency and impact of errors, which she refers to as “safety culture.” If you wonder why airplanes don’t crash very often, safety culture is an important part of that. Julia lays out a number of learnings from this work that we can bring to bear on making our science better. I think that our lab has already incorporated a lot of these ideas into our everyday workflows, and in particular we have tried to make the lab a “blame-free zone” for reporting errors, as we discussed previously. Here were a few points that came up in our discussion.\n\nThere is an inherent tradeoff between agility and documentation. One wants to document everything important and nothing more, since documentation takes time and effort; the question is what is important to document? It wasn’t clear that the proprosed log in Table 1 was at exactly the right level. I like the analogy to code commenting: One shouldn’t need to comment on things that are clear from the code itself. Rather, one should comment on the intention and rationale of the code, and try to tell the reader everything that the writer would want them to know about it that isn’t self-evident from reading the code. But it’s a difficult line to draw, and in general we liked the ideas that were laid out in the paper.\nA second long discussion centered on the idea of a “blame-free” zone - which has been the explicit policy in our lab for a while. There was some concern that this might lead to moral hazard, where people don’t worry about making mistakes because they know there will not be any adverse consequences. My feeling is that we need to separate the system aspects from the individual aspects. From a system level, we want to know about errors as early as possible, which is incentivized by the blame-free system. However, we need to simultaneously build the expectation that people should try to minimize errors, and an understanding that repeated errors are a signal of a problem that may have consequences if it’s not remediated.\nAnother point that was raised was that there is almost necessarily a tradeoff between speed and accuracy in science just as there is in human performance more generally. Thus, we need to realize that reducing the likelihood of errors will necessarily result in slower science. In some cases (e.g. having multiple people analyze each dataset independently) it was felt by some in the group that this might be too much of a cost for not enough benefit. We definitely need to think about how to calibrate the level of effort/time spent to the likelihood and costs of any potential errors in each domain.\nWe had a lively discussion around the distinction between errors and fraud. If someone were to commit fraud then we would almost certainly not want to take a blame-free approach: we must have zero tolerance for misconduct. But what counts as misconduct? Are QRPs like p-hacking different from intentional falsification? And does that depend on whether one actually knows that the QRP’s are bad? There was also some disagreement in the group about whether intention actually matters from the standpoint of the system. Does a QRP go from a regrettable mistake to intention misconduct once one knows that it’s a questionable practice. Can’t say we came up with any great answers to these questions.\n\nI’ll add here that I have generally tried to avoid discussing fraud in the context of reproducibility. While I know that intentional fraud (specifically, falsification of data or results) happens, I am not sure that there is anything that we could do about it that wouldn’t end up making science worse, and I’m pretty sure that someone who wants to commit fraud will always find a way around whatever safeguards we put in place anyway. I have always worked on the assumption that my fellow scientists are well-intentioned, in part because I don’t think I could go on doing science if I believed otherwise. The goal of our work has always been to provide people with the tools and knowledge they need to do the best science they can, and to work to align the incentives so that the best science is rewarded (e.g. through the HELIOS initiative).\nOverall we thought this was a great paper and would strongly recommend it for other labs to read!"
  },
  {
    "objectID": "posts/error-tight/index.html#reading-error-tight-by-julia-strand",
    "href": "posts/error-tight/index.html#reading-error-tight-by-julia-strand",
    "title": "Reading ‘Error Tight’ by Julia Strand",
    "section": "",
    "text": "Today in our lab meeting we read “Error Tight: Exercises for Lab Groups to Prevent Research Mistakes” by Julia Strand. It’s a great paper and led to some really fun discussions!\nThe paper makes the point that errors are a necessary feature of human activity, including science, and that there are well-established methods from human factors research to reduce the frequency and impact of errors, which she refers to as “safety culture.” If you wonder why airplanes don’t crash very often, safety culture is an important part of that. Julia lays out a number of learnings from this work that we can bring to bear on making our science better. I think that our lab has already incorporated a lot of these ideas into our everyday workflows, and in particular we have tried to make the lab a “blame-free zone” for reporting errors, as we discussed previously. Here were a few points that came up in our discussion.\n\nThere is an inherent tradeoff between agility and documentation. One wants to document everything important and nothing more, since documentation takes time and effort; the question is what is important to document? It wasn’t clear that the proprosed log in Table 1 was at exactly the right level. I like the analogy to code commenting: One shouldn’t need to comment on things that are clear from the code itself. Rather, one should comment on the intention and rationale of the code, and try to tell the reader everything that the writer would want them to know about it that isn’t self-evident from reading the code. But it’s a difficult line to draw, and in general we liked the ideas that were laid out in the paper.\nA second long discussion centered on the idea of a “blame-free” zone - which has been the explicit policy in our lab for a while. There was some concern that this might lead to moral hazard, where people don’t worry about making mistakes because they know there will not be any adverse consequences. My feeling is that we need to separate the system aspects from the individual aspects. From a system level, we want to know about errors as early as possible, which is incentivized by the blame-free system. However, we need to simultaneously build the expectation that people should try to minimize errors, and an understanding that repeated errors are a signal of a problem that may have consequences if it’s not remediated.\nAnother point that was raised was that there is almost necessarily a tradeoff between speed and accuracy in science just as there is in human performance more generally. Thus, we need to realize that reducing the likelihood of errors will necessarily result in slower science. In some cases (e.g. having multiple people analyze each dataset independently) it was felt by some in the group that this might be too much of a cost for not enough benefit. We definitely need to think about how to calibrate the level of effort/time spent to the likelihood and costs of any potential errors in each domain.\nWe had a lively discussion around the distinction between errors and fraud. If someone were to commit fraud then we would almost certainly not want to take a blame-free approach: we must have zero tolerance for misconduct. But what counts as misconduct? Are QRPs like p-hacking different from intentional falsification? And does that depend on whether one actually knows that the QRP’s are bad? There was also some disagreement in the group about whether intention actually matters from the standpoint of the system. Does a QRP go from a regrettable mistake to intention misconduct once one knows that it’s a questionable practice. Can’t say we came up with any great answers to these questions.\n\nI’ll add here that I have generally tried to avoid discussing fraud in the context of reproducibility. While I know that intentional fraud (specifically, falsification of data or results) happens, I am not sure that there is anything that we could do about it that wouldn’t end up making science worse, and I’m pretty sure that someone who wants to commit fraud will always find a way around whatever safeguards we put in place anyway. I have always worked on the assumption that my fellow scientists are well-intentioned, in part because I don’t think I could go on doing science if I believed otherwise. The goal of our work has always been to provide people with the tools and knowledge they need to do the best science they can, and to work to align the incentives so that the best science is rewarded (e.g. through the HELIOS initiative).\nOverall we thought this was a great paper and would strongly recommend it for other labs to read!"
  },
  {
    "objectID": "posts/nyt-letter-to-editor-uncut-version/index.html",
    "href": "posts/nyt-letter-to-editor-uncut-version/index.html",
    "title": "NYT Letter to the Editor- The uncut version",
    "section": "",
    "text": "This was originally posted on blogger here.\nThe NY Times has now printed our letter to the editor regarding the Lindstrom article. However, the published version is an edited and shortened version of our original letter, which I am posting here for the record.Dear Editor,The Op-Ed “You Love Your iPhone, Literally” by Martin Lindstrom purports to show, using brain imaging, that our attachment to digital devices, reflects not addiction but instead the same kind of emotion that we feel for human loved ones. However, the evidence the author presents does not show this. The region that he points to as being “associated with feelings of love and compassion” (the insular cortex) is a brain region that is active in as many as one third of all brain imaging studies. Further, in studies of decision making the insula is more often associated with negative than positive emotions. The kind of reasoning that Lindstrom uses is well known to be flawed, because there is rarely a one-to-one mapping between any brain region and a single mental state; insula activity could reflect one or more of several psychological processes. This same point was made by some of us regarding a similar Op-Ed piece in 2007). We are disappointed that the Times has published extravagant claims based on scientific data that have not been subjected to the standard scientific review process, especially considering how often its pages exhort policy makers to pay more attention to peer-reviewed scientific evidence and disregard specious claims.Sincerely,Russell Poldrack, Ph.D., University of Texas at AustinGeoffrey K Aguirre, M.D., Ph.D., University of PennsylvaniaAdam Aron, Ph.D., University of California at San DiegoLisa Feldman Barrett, Ph.D., Northeastern UniversityMark G. Baxter, Ph.D., Mount Sinai School of MedicineSusan Bookheimer, Ph.D., University of California at Los AngelesColin Camerer, Ph.D., California Institute of TechnologyMcKell Carter, Ph.D., Duke UniversityChristopher Chabris, Ph.D., Union CollegeMolly Crockett, Ph.D., University of Zurich, SwitzerlandNathaniel Daw, Ph.D., New York UniversityPaul Downing, Ph.D., University of Bangor, Wales, UKRussell Epstein, Ph.D., University of PennsylvaniaMichael Frank, Ph.D., Brown UniversityJanet Frick, Ph.D., University of GeorgiaPaul Glimcher, Ph.D., New York UniversityTom Hartley, Ph.D., University of York, UKBenjamin Hayden, Ph.D., University of RochesterHauke R. Heekeren, M.D., Freie Universität Berlin, GermanySimon Hjerrild, M.D., University of Aarhus, DenmarkScott Huettel, Ph.D., Duke UniversityNancy Kanwisher, Ph.D., Massachusetts Institute of TechnologyBrian Knutson, Ph.D., Stanford UniversityJohn Kubie, Ph.D., SUNY Downstate Medical CenterMichael V. Lombardo, Ph.D., University of Cambridge, UKKen Norman, Ph.D., Princeton UniversityOlivier Oullier, Ph.D., Aix-Marseille University, FranceSteven Petersen, Ph.D., Washington UniversityElizabeth Phelps, Ph.D., New York UniversityRajeev Raizada, Ph.D., Cornell UniversityAntonio Rangel, Ph.D., California Institute of TechnologyPeter B. Reiner, Ph.D., University of British Columbia, CanadaGregory Samanez-Larkin, Ph.D., Vanderbilt UniversityGeoff Schoenbaum, M.D., Ph.D., University of MarylandDaphna Shohamy, Ph.D., Columbia UniversityJon Simons, Ph.D., University of Cambridge, UKPeter Sokol-Hessner, Ph.D., California Institute of TechnologyDavid Somers, Ph.D., Boston UniversityDamian Stanley, Ph.D., California Institute of TechnologyJohn Van Horn, Ph.D., University of California at Los AngelesBradley Voytek, Ph.D., University of California, San FranciscoAnthony Wagner, PhD, Stanford University.Daniel Willingham, Ph.D., University of VirginiaTal Yarkoni, Ph.D., University of Colorado BoulderJeff Zacks, Ph.D., Washington UniversityJamil Zaki, Ph.D., Harvard University"
  },
  {
    "objectID": "posts/the-perils-of-leave-one-out/index.html",
    "href": "posts/the-perils-of-leave-one-out/index.html",
    "title": "The perils of leave-one-out crossvalidation for individual difference analyses",
    "section": "",
    "text": "This was originally posted on blogger here.\nThere is a common tendency of researchers in the neuroimaging field to use the term “prediction” to describe observed correlations. This is problematic because the strength of a correlation does not necessarily imply that one can accurately predict the outcome of new (out-of-sample) observations. Even if the underlying distributions are normally distributed, the observed correlation will generally overestimate the accuracy of predictions on out-of-sample observations due to overfitting (i.e., fitting the noise in addition to the signal). In degenerate cases (e.g., when the observed correlation is driven by a single outlier), it is possible to observe a very strong in-sample correlation with almost zero predictive accuracy for out-of-sample observations.The concept of crossvalidation provides a way out of this mess; by fitting the model to subsets of the data and examining predictive accuracy on the held-out samples, it’s possible to directly assess the predictive accuracy of a particular statistical model. This approach has become increasingly popular in the neuroimaging literature, which should be a welcome development. However, crossvalidation in the context of regression analyses turns out to be very tricky, and some of the methods that are being used in the literature appear to be problematic.One of the most common forms of crossvalidation is “leave-one-out” (LOO) in which the model is repeatedly refit leaving out a single observation and then used to derive a prediction for the left-out observation. Within the machine learning literature, it is widely appreciated that LOO is a suboptimal method for cross-validation, as it gives estimates of the prediction error that are more variable than other forms of crossvalidation such as K-fold (in which the training/testing is performed after breaking the data into K groups, usually 5 to 10) or bootstrap; see Chapter 7 in Hastie et al. for a thorough discussion of this issue). There is another problem with LOO that is specific to its use in regression. We discovered this several years ago when we started trying to predict quantitative variables (such as individual learning rates) from fMRI data. One thing that we always do when running any predictive analysis is to perform a randomization test to determine the distribution of performance when the relation between the data and the outcome is broken (e.g., by randomly shuffling the outcomes). In a regression analysis, what one expects to see in this randomized-label case is a zero correlation between the predicted and actual values. However, what we actually saw was a very substantial negative bias in the correlation between predicted and actual values. After lots of head-scratching this began to make sense as an effect of overfitting. Imagine that you have a two-dimensional dataset where there is no true relation between X and Y, and you first fit a regression line to all of the data; the slope of this line should on average be zero, but will likely deviate from zero on each sample due to noise in the data (an effect that is bigger for smaller sample sizes). Then, you drop out one of the datapoints and fit the line again; let’s say you drop out one of the observations at the extreme end of the X range. On average, this is going to have the effect of bringing the estimated regression line closer to zero than the full estimate (unless your left-out point was right at the mean of the Y distribution). If you do this for all points, you can then see how this would result in a negative correlation between predicted and actual values when the true slope is zero, since this procedure will tend to pull the line towards zero for the extreme points. To see an example of this fleshed out in an ipython notebook, visit http://nbviewer.ipython.org/4221361/ - the full code is also available at https://github.com/poldrack/regressioncv. This code creates random X and Y values and then tests several different crossvalidation schemes to examine their effects on the resulting correlations between predicted and true values. I ran this for a number of different samples sizes, and the results are shown in the figure below (NB: figure legend colors fixed from original post).The best (i.e. least biased) performance is shown by the split half method. Note that this is a special instance of split half crossvalidation with perfectly matched X distributions, because it has exactly the same values on the X variable for both halves. The worst performance is seen for leave-one-out, which is highly biased for small N’s but shows substantial bias even for very large N’s. Intermediate performance is seen when a balanced 8-fold crossvalidation scheme is used; the “hi” and “lo” versions of this are for two different balancing thresholds, where “hi” ensures fairly close matching of both X and Y distributions across folds whereas “lo” does not. We have previously used balanced crossvalidation schemes on real fMRI data (Cohen et al., 2010) and found them to do a fairly good job of removing bias in the null distributions, but it’s clear from these simulations that bias can still remain.As an example using real data, I took the data from a paper entitled “Individual differences in nucleus accumbens activity to food and sexual images predict weight gain and sexual behavior” by Demos et al. The title makes a strong predictive claim, but what the paper actually found was an observed correlation of r=0.37 between neural response and future weight gain. I ripped the data points from their scatterplot using PlotDigitizer and performed a crossvalidation analysis using leave-one-out and 4-fold crossvalidation either with or without balancing of the X and Y distributions across folds (see code and data in the github repo). The table below shows the predictive correlations obtained using each of the measures (the empirical null was obtained by resampling the data 500 times with random labels; the 95th percentile of this distribution is used as the significance cutoff):CV methodr(pred,actual)r(pred,actual) with random labels95%ileLOO0.176395-0.2769770.1549754-fold0.183655-0.1480190.160695Balanced 4-fold0.194456-0.0662610.212925This analysis shows that, rather than the 14% of variance implied by the observed correlation, one can at best predict about 4% of the variance in weight gain from brain activity; this result is not significant by a resampling test, though the LOO and 4-fold results, while weaker numerically, are significant compared to their respective empirical null distributions. (Note that a small amount of noise was likely introduced by the data-grabbing method, so the results with the real data might be a bit better.)UPDATE: As noted in the comments, the negative bias can be largely overcome by fixing the intercept to zero in the linear regression used for prediction. Here are the results obtained using the zero-intercept model on the Demos et al. data:CV methodr(pred,actual)r(pred,actual) with random labels95%ileLOO0.258-0.0670.1894-fold0.263-0.0550.192Balanced 4-fold0.256-0.0370.213This gets us up to about 7% variance accounted for by the predicted model, or about half of that implied by the in-sample correlation, and now the correlation is significant by all CV methods.Take-home messages:Observed correlation is generally larger than predictive accuracy for out-of-sample observations, such that one should not use the term “predict” in the context of correlations.Cross-validation for predicting individual differences in fMRI analysis is tricky. Leave-one-out should probably be avoided in favor of balanced k-fold schemesOne should always run simulations of any classifier analysis stream using randomized labels in order to assess the potential bias of the classifier. This means running the entire data processing stream on each random draw, since bias can occur at various points in the stream.PS: One point raised in discussing this result with some statisticians is that it may reflect the fact that correlation is not the best measure of the match between predicted and actual outcomes. If someone has a chance to take my code and play with some alternative measures, please post the results to the comments section here, as I don’t have time to try it out right now.PPS: I would be very interested to see how this extends to high-dimensional data like those generally used in fMRI. I know that the bias effect occurs in that context given that this is how we discovered it, but I have not had a chance to simulate its effects."
  },
  {
    "objectID": "posts/the-perils-of-leave-one-out/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/the-perils-of-leave-one-out/index.html#comments-captured-from-original-post-on-blogger",
    "title": "The perils of leave-one-out crossvalidation for individual difference analyses",
    "section": "12 comments captured from original post on Blogger",
    "text": "12 comments captured from original post on Blogger\nKevin Mitchell said on 2012-12-17\nReally interesting post, which I think goes beyond problems with leave-one out cross-validation and highlights the general pitfalls of relying exclusively on statistical methods to infer predictive power from a set of exploratory data. What is needed is replication with an independent sample. (As geneticists have been requiring for quite a while now).\nyarikoptic said on 2012-12-17\nnbviewer link seems to be 404why not to add the .ipynb into the git repository?\nRuss Poldrack said on 2012-12-17\nThanks Yarick - nbviewer seems a bit flaky. I’ve added the ipynb file to the git repo.\nUnknown said on 2012-12-17\nI think they were using \"prediction\" in the temporal sense…(i.e., the sampling occurred before the behavior). It would be great though to explicitly specify what one means by \"prediction\"(for instance, within or out of sample).\nRuss Poldrack said on 2012-12-17\nAgreed, there are many different senses of the term \"predict\". My main point here is that correlation (even if one variable precedes the other in time) does not imply predictive accuracy in a statistical sense. I think a lot of people in our field don’t appreciate the \"out-of-sample\" problem (discussed very nicely, BTW, in Nate Silver’s book)\nyarikoptic said on 2012-12-18\nok – played with it more and now negative bias (when you fit withfitting intercept) makes perfect sense to me if you computecorrelation between full original time series and target one,predicted in cross-validated fashion.Reason is actually quite simple: in every cross-validation fold, bytaking \"testing\" set out, mean of the training data changes from the‘grand mean’ in the opposite direction to the mean of the taken out(for testing) data [e.g. for split-half, grand mean was = 0, mean oftesting data 0.1, mean of training data becomes -0.1]By training you are fitting the line to the training data, which is\"offset\" from the grand mean so it is likely to obtain a line offsetin the same direction [in our example it is likely to be a line BELOW\"grand\" line, and having negative intercept]. And per \"construction\"it would be in the opposite direction from the grand mean than theleft-out testing samples.For another split of the data, you are likely to get offset in theopposite direction [.e.g in our example it would be testing gets -0.1,while training 0.1], and result would be as before – predicted datahas an opposing offset from grand mean than testing data.Therefore if you are computing correlation later on the whole seriesof predicted/original values (not like I suggested – mean ofestimates in each split) – you are likely to obtain negativecorrelation due to the tendency of predicted data being in oppositedirection from the original one merely due to difference in the means.Without intercept linear model looses this \"flexibility\" of choosing\"the other side\", so it becomes less prominent (but imho it is stillpresent one way (mean) or another (variance)).Really crude example is here (disregard initial scatter plots –absent shuffle for cross-validation somehow plays an interestingrole. and I had to disable shuffling for my example below)http://nbviewer.ipython.org/url/www.onerussian.com/tmp/regression_cv_demos_normal_noisy100_nofit_intercept.ipynbIt is left to figure out on the strong bimodal distribution of themeans. They are somewhat surprising to me since I haven’t observedthem before when in a searchlight using cross-validation oncorrelations [e.g. figure 5 inhttp://onerussian.com/tmp/transfusion-20120411.pdf , disregard thefootnotes – it wasn’t submitted]. But probably that was because datawas not actually random and did have a strong positive bias ;)\nRuss Poldrack said on 2012-12-18\nThanks Yarick - what do you think the takeaway is? Perhaps that correlation is a the wrong measure to use for assessing predictive accuracy of regression analyses?\nRuss Poldrack said on 2012-12-20\nNiko - thanks for your comments. The exploration issue is completely separate from what I am talking about here - in fact this problem initially arose for us in the context of whole-brain analyses where we were trying to predict some behavioral measure from whole-brain activation. Some further explorations (based on suggestions by Sanmi Koyejo, mentioned in Yarick’s comment and implemented I think in the latest code on the repo, but not really discussed explicitly) show that the problem is due to the intercept term in the linear regression. I was not suggesting that the correlation estimates are biased across separate samples from a population; rather, I was highlighting the negative dependency that you mention, which is seen in a bias in the correlation between predicted and actual outcomes across folds. I’m not sure that \"crossvalidated correlation\" is the right term for what I computed. I basically perform a linear regression on the training set and then use that regression solution to predict the values of the test set. I then compute the correlation between the predicted and actual values - it’s this correlation that I find to be biased, but whose bias appears to largely go away when the intercept is fixed to zero in the regression. You may be right that a Bayesian approach is a better way to address this - if only I had time to give it a try!\nAaron Schurger said on 2013-01-12\nHi, Russ,I’ve also run into a similar issue with the LOO procedure, after I was alerted to the potential bias by Lucas Parra. In our case we are using LOO with matched filtering (essentially Fisher’s linear discriminant, but without taking the noise covariance into account). We are not doing classification (as in LDA), but just projecting onto the (normalized) discriminant vector and then looking at the continuous valued result. In this case the mean is unbiased (as best I can tell after lots of testing), but the distribution is skewed to the left, so the median is always &gt; 0. As a result a sign test or signed rank test is highly significant even for gaussian random data. This is especially pronounced when there are very few dimensions (like &lt; 10), but for about 20 - 100 dimensions the skew becomes negligible (things might be different when you have even more dimensions - I haven’t tried). In any event, because the mean seems to remain unbiased (in this case at least - not for correlation as you’ve found), then analyses across subjects should be OK. Within subject, you would want to do a resampling test (which I guess you should always do anyway). Again, thanks to Lucas Parra for pointing this out to us.\nnot2Hastie said on 2013-07-27\nMy student Will Fithian and I have responded to this interesting phenomenon. Since I could not figure out how to use Mathjax in blogger, I ended up entering the material in tumblr. Here is the url: http://not2hastie.tumblr.com/Sorry for the inconvenience, but please have a lookTrevor HastieStanford Statistics\nRuss Poldrack said on 2013-07-27\nTrevor - many thanks for digging into this!\nUnknown said on 2017-04-20\nThis is an interesting post! I observed this as well, and puzzled over it, when doing analyses for our 2011 J Neuro paper predicting placebo responses. I arrived at the conclusion that the correlation metric is not a good outcome metric for optimizing models, for two reasons. - Assessing the correlation involves fitting another model on the cross-validated predictive estimates, with one slope parameter and two variance parameters- The null hypothesis is not exactly zero, for reasons you and other commentators have raised. (Yes, I think ithe same fundamental principle applies to cases with unbalanced LOO cross-validation with two groups and LOO with regression, as Niko pointed out).For that reason, we optimized the models minimizing absolute prediction error, and reported error. We’ve been using permutation tests to check/validate what the null distribution is, and for prediction error (not correlation) these are zero with a valid LOO cross-validation on independent observations. But correlations are intuitive and people generally understand what the values mean (though point taken that they can be somewhat misleading if the null is not zero!), so we reported those as well and continue to do so for ease of communication when we feel that it’s an adequate representation of prediction performance.A few other reflections and beliefs, appreciated after reading the Hastie book and some other literature:- All cross-validated estimates are biased towards zero relative to the performance of the full sample, because you’re excluding data points- Among cross-val strategies, LOO is minimum bias but max variance, because the training samples are so similar (there is dependency among training samples across folds). The \"minimum bias\" here is independent of any bias induced by estimating correlations post hoc on cross-validated estimates, which does indeed induce a bias from a different source.- k-fold with fewer folds biases accuracy estimates more strongly towards zero, but reduces variance. So there is a bias-variance tradeoff.- The optimal cross-validation holdout set depends on sample size, and probably other things. With larger samples, you can get away with fewer folds without damaging model development (and thus performance) as much.- Correlations are unbiased if only a single measure is tested, without any fishing.- I generally like the idea of 5-fold or so, but for small samples, LOO seems OK to me, with appropriate checks and caveats – i.e., observations must be truly independent, permutation test-based checks of the null distribution, and more. These days I figure that unless one tests peformance prospectively in truly independent datasets, trust in how well the model performs is limited anyway. So we’ve been trying to focus more on that."
  },
  {
    "objectID": "posts/why-i-cut-carbs/index.html",
    "href": "posts/why-i-cut-carbs/index.html",
    "title": "why I cut the carbs",
    "section": "",
    "text": "This was originally posted on blogger here.\nThe last few months have seen a fairly major transformation in what I eat, with pretty amazing results. For the last ~20 years my diet has been mostly vegetarian + seafood, and my weight has been very steady, between 160 and 170. I have always been a major sugar fiend, with my college-age penchant for Skittles morphing into a significant chocolate habit in the last few years. My exercise habits have always been hit-and-miss, until the last year when I became much more regular in my workouts, which at this point I am doing 6-7 times a week. Despite this substantial increase in my exercise frequency, I didn’t really see any changes in my body composition. I felt better, but the fat around my belly just wouldn’t go away. My first step on the road to a new diet was my personal trainer’s suggestion that I increase the protein in my diet, after I complained to her that all of my work in the gym was not showing results on my body. As I began to change my diet to include more protein, I also discovered Gary Taubes for the first time, and read Why We Get Fatfollowed by Good Calories, Bad Calories. Taubes is a writer for Science magazine who has spent the last few years digging into the science of weight loss, and his books are pretty startling because they call into question nearly everything that we think we know about what is good for us. In short, he argues persuasively that there is no scientific evidence for the common belief that saturated fat and dietary cholesterol are the causes of heart disease, and he also provides a clear physiological explanation for why carbohydrates are the thing that make us fat, as opposed to the “calories in, calories out” idea that most of us have (I certainly did). In addition, he points out evidence that carbs not only make us fat, but also may contribute to other “diseases of civilization”. Most interestingly, he discusses work that I had not been aware of that links Alzheimer’s disease to insulin, which is driven by carbohydrate intake. The Taubes books really pushed me to rethink what I eat, and I decided to take the plunge and cut carbs from my diet as completely as possible. I’ve been pretty successful at this, and the results have been pretty amazing. I am down to 150 pounds since cutting carbs from my diet, which is pretty impressive given that my weight has never fluctuated more than about 5 pounds from my long-term average of 165, despite pretty radical changes in caloric intake across the years. I can see the difference too; the fat around my belly is noticeably reduced, as well as in other places around my body. Perhaps the biggest change for me was starting to eat meat. I still don’t eat much meat, and when I do I try to make sure that it was produced as naturally and ethically as possible, but nonetheless I am now eating meat several times a week (mostly bacon or proscuitto), as well as eating eggs almost every day. I was very curious to see how this affected my cholesterol numbers when I went for my annual checkup this year, a few months into the new diet. The results were pretty astounding:Total cholesterol: 234 (2010) to 199 (2011)LDL: 147 (2010) to 121 (2011)HDL: 57 (2010) to 65 (2011)Triglycerides: 148 (2010) to 64 (2011)Obviously it’s hard to know how much of this to attribute to diet and how much to other factors such as exercise, but these are the best numbers I’ve ever seen in my life, after years of being a vegetarian. I also don’t put too much stock in these numbers; as Taubes points out in his books, the science connecting cholesterol levels to heart disease is pretty weak, except at the extremes. However, my triglycerides, which are probably much more important, are clearly off-the-charts better after years of being right at the borderline of too high.I’ll write more soon about the interesting experience of deleting some seriously addictive foods from one’s diet."
  },
  {
    "objectID": "posts/why-i-cut-carbs/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/why-i-cut-carbs/index.html#comments-captured-from-original-post-on-blogger",
    "title": "why I cut the carbs",
    "section": "9 comments captured from original post on Blogger",
    "text": "9 comments captured from original post on Blogger\nJanet said on 2011-04-11\nGreat post. But don’t you think these could be just as much from (I presume) cutting sugar and processed carbs from your diet, as from increasing meat? Triglycerides are clearly tied to sugar intake. That has nothing to do with meat.I’d be curious what your results would be from a very clean, whole-foods, vegetable/bean/nuts/seeds/maybe-some-eggs based diet. i.e. limit grains and sugars, but not vegetables. I’d also be interested in whether you’ve cut foods like greens, broccoli, berries, and if so, why. Thanks for the post!\nRuss Poldrack said on 2011-04-11\n@janet - I agree, it’s the change in refined carbs that has likely had the biggest effect on my lipid numbers. I have not limited green vegetables or berries and also eat some occasional citrus; in fact, what I am shooting for is exactly the kind of whole-foods diet that you mention, with some meat and fish thrown in. The main effect of eating meat and eggs, I think, has been to keep me from feeling hungy.\nJanet said on 2011-04-11\nThat’s great to hear. I hear too many people talk about \"cutting carbs\" indiscriminately and including green veggies. I agree we need far less fruit than many of us eat (myself included) but I’m of the camp that some low-glycemic veggies (cucumbers, snap peas, etc) are really important. Besides as I’m sure you know, calorie for calorie, broccoli and spinach are higher in protein than sirloin steak. Hooray for photosynthesis! :) still, your overall post is very inspiring. thanks!\nUnknown said on 2011-04-11\nWhat fortuitous timing - I’ve been thinking about this for a few weeks but haven’t been motivated to change much about my diet. Thanks for sharing your experience, as I now have a clear example of the benefits. Somehow it helps to change behavior if you actually know someone who has done it.\nUnknown said on 2011-04-12\nHey Russ-This is a great post! I felt like I was reading some part of my own diary reading about your weight \"issues\". My average for the past 20 years has been right at 165lbs. & when I talk about struggling with weight, people think I’m crazy…With this, I can totally relate to the \"Despite this substantial increase in my exercise frequency, I didn’t really see any changes in my body composition. I felt better, but the fat around my belly just wouldn’t go away\" part of your post. I’ve been trying things to trick my metabolism - mostly sprints, interval running and yoga. Nothing seems to do the trick of getting rid of my belly fat.Also, I was diagnosed with Celiac disease a few years ago, so my flour intake has pretty much stopped. I do still get plenty of carbs with potatoes and corn though…and ice cream…I have been on a serious \"greens\" kick for the past year with spinach smoothies. This alone has leveled out my cholesterol #’s significantly. My doctor wanted to put me on a pill, but when I went back in after a year of trying the greens, he didn’t know what to say other than, \"keep doing whatever it is you’re doing…\"I’m wondering if you’d be willing to post up a sample menu of what you eat in a week. It could be really helpful to see what you’ve had success with…I feel like we have a similar body type and it would be super helpful for me to have some ideas of how to implement some changes…Hope life is great,mike crawford\nRuss Poldrack said on 2011-04-12\nMike - hey, nice to hear from you! Here is a sample of what my menu looks like in an average recent week:breakfast: either plain yogurt (full or 2% fat) and berries or scrambled eggs and baconlunch: generally a salad with either a hard-boiled egg, marinated tofu, or tunaafternoon snack: nuts or ThinkThin bars (these are the only non-whole food I have been eating recently - they are just too convenient!)dinner: some things we eat regularly:- whole wheat pizza cooked on the stone, very thin crust- vegetable \"lasagna\" using zucchini slices instead of pasta- vegetable curry with tofu (either pan-fried or baked)- fish, either grilled, smoked in our stovetop smoker, or blackened, with some veggies on the side- vegetable gratin (basically a casserole of vegetables and cheese) I’d be interested to hear any other dinner ideas that people have.\nDaniel Barsky said on 2011-04-12\nHow tall are you, Russ? One suggestion: to limit hunger, eat nuts (rather than meat). Find some that you love. My favorites are almonds (raw) and pistachios (roasted and lightly salted).\nRuss Poldrack said on 2011-04-12\n@daniel: I’m just under 6’. I do eat nuts pretty often during the day, I agree that they are great to stave off hunger.\nJanet said on 2011-04-13\nMy go-to quick lunch when I’m pressed for time and looking for something quick and dirty to fix out of the pantry is to cook some quinoa in the microwave (2 parts water to 1 part quinoa, put in a microwave safe casserole and nuke for 8 minutes, let sit undisturbed for another 5 min or so, and voila), then mix together quinoa, black beans, salsa, and maybe some salad greens. Lots of good fiber and protein in that. I’m also getting more into cooking tempeh lately, and have found some good recipes for curried tempeh or tempeh with peanut sauce, which could be served with some veggies and greens of quinoa. vegweb.com is a great source for recipes, as is the vegan dad blog (I’m vegan, but looking to cut processed carbs). thanks for this thread."
  },
  {
    "objectID": "posts/new-course-on-decision-making-seeking/index.html",
    "href": "posts/new-course-on-decision-making-seeking/index.html",
    "title": "New course on decision making- Seeking feedback",
    "section": "",
    "text": "This was originally posted on blogger here.\nI am currently developing a new course on the psychology of decision making that I will teach at Stanford in the Spring Quarter of 2016. I’ve looked at the various textbooks on this topic and I’m not particularly happy with any of them, so I am rolling my own syllabus and will use readings from the primary literature. I have developed a draft syllabus and would love to get feedback: Are there important topics that I am missing? Different readings that I should consider? Topics I should consider dropping? Please leave comments with your suggestions, or email me at poldrack@gmail.com!Part 1: What is a decision? 1. Varieties of decision making (overview of course)Part 2: Normative decision theory: How an optimal system should make decisions2. axiomatic approach from economics- TBD reading on expected utility theory3. Bayesian decision theoryKörding, K. P. (2007). Decision Theory: What “Should” the Nervous System Do? Science, 318(5850), 606–610. http://doi.org/10.1126/science.11429984. Information accumulationSmith & Ratcliff, 2004, Psychology and neurobiology of simple decisions. TINS.Part 3: Psychology: How humans make decisions5. Anomalies: the ascendence of psychology and behavioral economicsKahneman, D. (2003). A perspective on judgment and choice. American Psychologist,58, 697-7206. Judgment: Anchoring and adjustmentChapman, G.B. & Johnson, E.J. (2002). Incorporating the irrelevant: Anchors injudgment of belief and value7. Heuristics: availability, representativenessTversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases.Science, 185, 1124-1131. 8. Risk and uncertainty: Risk perception, risk attitudesSlovic, P. (1987). Perception of risk. Science, 236, 280-2859. Prospect theory Kahneman, D. & Tversky A. (1984). Choices, values, and frames. AmericanPsychologist, 39, 341–350.10. Framing, endowment effects, and applications of prospect theoryKahneman, D., Knetsch, J.L., & Thaler, R.H. (1991). The endowment effect, lossaversion, and status quo bias. Journal of Economic Perspectives, 5, 193-206.11. Varieties of utilityKahneman, Wakker, & Sarin (1997). Back to Bentham: Explorations of experienced utility. Quarterly Journal of Economics.12. Intertemporal choice and self-controlMischel, W., Shoda, Y., & Rodriguez, M.L. (1989). Delay of gratification in children. Science, 244, pp. 933-938.13. Emotion and decision makingRottenstreich, Y. & Hsee, C.K. (2001). Money, kisses and electric shocks: On theaffective psychology of risk. Psychological Science, 12, 185-190.14. Social decision making and game theoryTBDPart 4: Neuroscience of decision making15. Neuroscience of simple decisionsSugrue, Corrado, & Newsome (2005). Choosing the greater of two goods: neural currencies for valuation and decision making. Nature Reviews Neuroscience.16. Neuroscience of Value-based decision makingRangel et al., 2008, A framework for studying the neurobiology of value-based decision making17. Reinforcement learning and dopamine, wanting/likingSchultz, Montague, and Dayan (1997) A neural substrate of prediction and reward18. Decision making in simple organismsReading TBD (c. elegans, snails, slime mold, etc)possibilities:http://www.ncbi.nlm.nih.gov/pubmed/10737805http://www.sciencemag.org/content/311/5767/1613.fullhttp://rspb.royalsocietypublishing.org/content/278/1703/307Part 5: Ethical issues19. Free willRoskies (2006) Neuroscientific challenges to free will and responsibility.OR:Shadlen & Roskies (2012). The neurobiology of decision-making and responsibility: reconciling mechanism and mindedness."
  },
  {
    "objectID": "posts/new-course-on-decision-making-seeking/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/new-course-on-decision-making-seeking/index.html#comments-captured-from-original-post-on-blogger",
    "title": "New course on decision making- Seeking feedback",
    "section": "3 comments captured from original post on Blogger",
    "text": "3 comments captured from original post on Blogger\nUnknown said on 2015-09-03\nYou might consider something on evidence accumulation - Ratcliffe 1978, Gold & Shadlen 2007\nChristina said on 2015-09-30\n\nPerceptual decision making and how it relates to value-based decision making (http://www.nature.com/nature/journal/v431/n7010/full/nature02966.html)2) We had a pretty good discussion on neuromarketing in our decision-making class (http://www.ncbi.nlm.nih.gov/pubmed/20197790). Might fit into \"ethics\". Cheers from Berlin! Christina\n\nDoctor Spurt said on 2015-12-14\nYou might find some interesting stuff in some old (late 1970s) Quantitative Ethology literature, in particular McFarland & Sibley’s \"The Behavioural Final Common Path\". (http://www.ncbi.nlm.nih.gov/pubmed/239416) And even if you don’t, ethology seems to be missing from your working outline."
  },
  {
    "objectID": "posts/recent-reads/index.html",
    "href": "posts/recent-reads/index.html",
    "title": "Recent Reads",
    "section": "",
    "text": "This was originally posted on blogger here.\nHere are my thoughts about some books I have read recently.Hits:The Emperor of All Maladies: A Biography of Cancer: An absolutely fascinating book. It traces the history of cancer and its treatment, highlighting all of the ways in which medical science has gotten things wrong in the past and how cancer treatment has only recently evolved beyond blindly poisoning the body. Not a lot of scientific content, but more than enough to keep me interested. Food and Western Disease: Staffan Lindenberg is a physician from Sweden who is best known for the Kitava Study. This book is a systematic review of the evidence regarding dietary causes of western disease such as cancer, diabetes, and heart disease. He’s clearly an advocate for the paleo diet, but he seems pretty fair in saying when the evidence does or does not fit with that approach. Definitely worth reading.The Queen of Fats: This is a relatively obscure book from a university press, but turned out to be a great read. The writer is not a scientist but does a decent job of covering the science, while still making the story engaging and fun. It has definitely changed how I think about food.The Immortal Life of Henrietta Lacks: I found this book fairly engaging, though it was a bit too tilted towards the drama of the writer’s journey versus the story itself. What I found most interesting were the stories about how absolutely messed up the Lacks family was (e.g., the children were all partly deaf due to a combination of inbreeding and neurosyphilis). However, I would have liked more scientific content. I would definitely recommend this book as background for human researchers as it really highlights the reasons why we have such strict control over human research today. Superbug: The Fatal Menace of MRSA: I know that I am crazy for reading these kinds of books as they just make one paranoid about infections, but nonetheless I found it really engaging and interesting. Not for the squeamish or hypochondriacal, however. Do The Work: This is a motivational book targeted largely at writers, but would be really useful for many researchers to read. It discusses how we often get in our own way, and how to get around it. It’s definitely a different kind of book from the rest of these, but for anyone who is having trouble getting things done I would highly recommend it.Misses:The Information - A History, A Theory, A Flood: This book is about the history of information theory, and thus will appeal to my geekier friends. Gleick is a great writer, and it starts really strong and does a good job of explaining lots of concepts, but I thought that the end fell really flat. I would recommend only if you are really seriously interested in the topic.Bossypants: I am generally a big Tina Fey fan and I really wanted to like this, but it just didn’t work for me. I made it about 1/3 of the way in before I gave up. Everything is obvious: *once you know the answer: I absolutely loved Duncan Watts’ earlier book, Six Degrees, so I was really excited to read this. In addition, the topic is really fascinating and ripe for the picking. However, I was really disappointed and barely made it through a couple of chapters. The problem is that it’s written for executives, and thus is peppered with dumbed-down examples without enough deep explanation. I think it’s a great example of how not to write a popular science book."
  },
  {
    "objectID": "posts/recent-reads/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/recent-reads/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Recent Reads",
    "section": "1 comments captured from original post on Blogger",
    "text": "1 comments captured from original post on Blogger\nBob said on 2011-06-30\nI like Tina Fey, but generally as soon was someone starts blabbering about their kids or parenting I am OUT!"
  },
  {
    "objectID": "posts/moving-from-keynote-to-markdown/index.html",
    "href": "posts/moving-from-keynote-to-markdown/index.html",
    "title": "Moving from Keynote to Markdown-based presentations",
    "section": "",
    "text": "Those of you who know me are already aware that I tend to make life much more difficult for myself than I need to. This is the first of what I expect to be a series of blog posts about my recent move towards a more fully open source software stack.\nI have used Keynote as my presentation platform for more than a decade, but have at various points toyed with moving to a text-based solution, for two main reasons:"
  },
  {
    "objectID": "posts/moving-from-keynote-to-markdown/index.html#building-my-first-talk-using-quarto-reveal.js",
    "href": "posts/moving-from-keynote-to-markdown/index.html#building-my-first-talk-using-quarto-reveal.js",
    "title": "Moving from Keynote to Markdown-based presentations",
    "section": "Building my first talk using Quarto + reveal.js",
    "text": "Building my first talk using Quarto + reveal.js\nI decided to take the plunge and build my presentation for an upcoming talk (a relatively short one at ~15 minutes) using Quarto + reveal.js. My usual MO when moving into a new area like this is to poke around and find some good examples that I like, and then use those as the basis for my first attempt. In this case, the Quarto gallery had a numnber of nice presentations, with linked code, that were helpful in getting started. I also had to learn a bit of CSS, and a suggestion from my colleage Nell on Mastodon pointed me to https://web.dev/learn/css/ which I found very helpful to build a mental model of how CSS works.\nWith all this in place, I started building the talk. The main challenge that I found was to go beyond the very basic styling that one gets from a standard Markdown presentation, such as changing image/text position size in a precise way. After a substantial bit of futzing I learned how to use CSS to get pretty much what I wanted. It’s definitely more work than just dragging things around in Keynote, but I am hopeful that 1) I will get better at it with practice, and 2) the payoff will be worth it.\nIf you’d like to have a look at the talk (which is still a work in progress), the source code is here and the rendered talk can be found here. One of the great things about hosting the talk on github is that it makes it very easy to share the latest version.\n\nGenerating PDF files\nI also like to have a PDF backup for all of my talks in case something goes wrong. There is no seemingly simple way to automatically render a talk using reveal.js from Quarto into PDF, but I found that it was easily done using the decktape package.\nI’ll have more to say in upcoming posts about how I am automating and organizing my talks."
  },
  {
    "objectID": "posts/the-principle-of-assumed-error/index.html",
    "href": "posts/the-principle-of-assumed-error/index.html",
    "title": "The principle of assumed error",
    "section": "",
    "text": "This was originally posted on blogger here.\nI’m going to be talking at the Neurohackweek meeting in a few weeks, giving an overview of issues around reproducibility in neuroimaging research. In putting together my talk, I have been thinking about what general principles I want to convey, and I keep coming back to the quote from Richard Feynman in his 1974 Caltech commencement address: “The first principle is that you must not fool yourself and you are the easiest person to fool.” In thinking about how can we keep from fooling ourselves, I have settled on a general principle, which I am calling the “principle of assumed error” (I doubt this is an original idea, and I would be interested to hear about relevant prior expressions of it). The principle is that whenever one finds something using a computational analysis that fits with one’s predictions or seems like a “cool” finding, they should assume that it’s due to an error in the code rather than reflecting reality. Having made this assumption, one should then do everything they can to find out what kind of error could have resulted in the effect. This is really no different from the strategy that experimental scientists use (in theory), in which upon finding an effect they test every conceivable confound in order to rule them out as a cause of the effect. However, I find that this kind of thinking is much less common in computational analyses. Instead, when something “works” (i.e. gives us an answer we like) we run with it, whereas when the code doesn’t give us a good answer then we dig around for different ways to do the analysis that give a more satisfying answer. Because we will be more likely to accept errors that fit our hypotheses than those that do not due to confirmation bias, this procedure is guaranteed to increase the overall error rate of our research. If this sounds a lot like p-hacking, that’s because it is; as Gelman & Loken pointed out in their Garden of Forking Paths paper, one doesn’t have to be on an explicit fishing expedition in order to engage in practices that inflate error due to data-dependent analysis choices and confirmation bias. Ultimately I think that the best solution to this problem is to always reserve a validation dataset to confirm the results of any discovery analyses, but before one burns their only chance at such a validation, it’s important to make sure that the analysis has been thoroughly vetted.Having made the assumption that there is an error, how does one go about finding it? I think that standard software testing approaches offer a bit of help here, but in general it’s going to be very difficult to find complex algorithmic errors using basic unit tests. Instead, there are a couple of strategies that I have found useful for diagnosing errors.Parameter recoveryIf your model involves estimating parameters from data, it can be very useful to generate data with known values of those parameters and test whether the estimates match the known values. For example, I recently wrote a python implementation of the EZ-diffusion model, which is a simple model for estimating diffusion model parameters from behavioral data. In order to make sure that the model is correctly estimating these parameters, I generated simulated data using parameters randomly sampled from a reasonable range (using the rdiffusion function from the rtdists R package), and then estimated the correlation between the parameters used to generate the data and the model estimates. I set an aribtrary threshold of 0.9 for the correlation between the estimated and actual parameters; since there will be some noise in the data, we can’t expect them to match exactly, but this seems close enough to consider successful. I set up a test using pytest, and then added CircleCI automated testing for my Github repo (which automatically runs the software tests any time a new commit is pushed to the repo)1. This shows how we can take advantage of software testing tools to do parameter recovery tests to make sure that our code is operating properly. I would argue that whenever one implements a new model fitting routine, this is the first thing that should be done. Imposing the null hypothesisAnother approach is to generate data for which the null hypothesis is true, and make sure that the results come out as expected under the null. This is a good way to protect one from cases where the error results in an overly optimistic result (e.g. as I discussed here previously). One place I have found this particularly useful is in checking to make sure that there is no data peeking when doing classification analysis. In this example (Github repo here), I show how one can use random shuffling of labels to test whether a classification procedure is illegally peeking at test data during classifier training. In the following function, there is an error in which the classifier is trained on all of the data, rather than just the training data in each fold:def cheating_classifier(X,y): skf=StratifiedKFold(y,n_folds=4) pred=numpy.zeros(len(y)) knn=KNeighborsClassifier() for train,test in skf: knn.fit(X,y) # this is training on the entire dataset! pred[test]=knn.predict(X[test,:]) return numpy.mean(pred==y)Fit to a dataset with a true relation between the features and the outcome variable, this classifier predicts the outcome with about 80% accuracy. In comparison, the correct procedure (separating training and test data):def crossvalidated_classifier(X,y): skf=StratifiedKFold(y,n_folds=4) pred=numpy.zeros(len(y)) knn=KNeighborsClassifier() for train,test in skf: knn.fit(X[train,:],y[train]) pred[test]=knn.predict(X[test,:]) return numpy.mean(pred==y)predicts the outcome with about 68% accuracy. How would we know that the former is incorrect? What we can do is to perform the classification repeatedly, each time shuffling the labels. This is basically making the null hypothesis true, and thus accuracy should be at chance (which in this case is 50% because there are two outcomes with equal frequency). We can assess this using the following:def shuffle_test(X,y,clf,nperms=10000): acc=[] y_shuf=y.copy() for i in range(nperms): numpy.random.shuffle(y_shuf) acc.append(clf(X,y_shuf)) return accThis shuffles the data 10,000 times and assesses classifier accuracy. When we do this with the crossvalidated classifier, we see that accuracy is now about 51% - close enough to chance that we can feel comfortable that our procedure is not biased. However, when we submit the cheating classifier to this procedure, we see mean accuracy of about 69%; thus, our classifier will exhibit substantial classification accuracy even when there is no true relation between the labels and the features, due to overfitting of noise in the test data.Randomization is not perfect; in particular, one needs to make sure that the samples are exchangeable under the null hypothesis. This will generally be true when the samples were acquired through random sampling, but can fail when there is structure in the data (e.g. when the samples are individual subjects, but some sets of subjects are related). However, it’s often a very useful strategy when this assumption holds.I’d love to hear other ideas about how to implement the principle of assumed error for computational analyses. Please leave your comments belowThis should have been simple, but I hit some snags that point to just how difficult it can be to build truly reproducible analysis workflows. Running the code on my Mac, I found that my tests passed (i.e. the correlation between the estimated parameters using EZ-diffusion and the actual parameters used to generate the data was &gt; 0.9), confirming that my implementation seemed to be accurate. However, when I ran it on CircleCI (which implements the code within a Ubuntu Linux virtual machine), the tests failed, showing much lower correlations between estimated and actual values. Many things differed between the two systems, but my hunch was that it was due to the R code that was used to generate the simulated data (since the EZ diffusion model code is quite simple). I found that when I updated my Mac to the latest version of the rtdists package used to generate the data, I reproduced the poor results that I had seen on the CircleCI test. (I turns out that the parameterization of the function that was using had changed, leading to bad results with the previous function call.). My interim solution was to simply install the older version of the package as part of my CircleCI setup; having done this, the CircleCI tests now pass as well. ↩︎"
  },
  {
    "objectID": "posts/the-principle-of-assumed-error/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/the-principle-of-assumed-error/index.html#comments-captured-from-original-post-on-blogger",
    "title": "The principle of assumed error",
    "section": "3 comments captured from original post on Blogger",
    "text": "3 comments captured from original post on Blogger\nJohan Carlin said on 2016-08-22\nIt’s good to check all results, good or bad. I actually have the opposite bias - I tend to assume that negative results are correct. So I try hard to check this tendency by going through and checking results even when they’re disappointing and you just want to look at something more encouraging instead. On avoiding bugs in the first place, the biggest issue I see is not enough code reuse. A lot of scientists start from scratch with writing analysis code for each project (or worse yet, copy scripts around and try to edit them until they run on the new data). Such code tends to see very little use, with a small range of inputs, which makes it hard to spot inaccurate outputs. Try to cultivate your own analysis packages for running the sort of stuff your research usually involves. Even if it’s just for your own personal use, get in the habit of using version control and it will be far easier to figure out the scope of any bugs (if you know which commit introduced a bug it becomes fairly straight forward to work out which results are likely to have been affected and need to be recomputed).\nRobert said on 2016-08-23\nThis article resonantes a bit with your thoughts. http://scitation.aip.org/content/aip/magazine/physicstoday/article/65/7/10.1063/PT.3.1642In my physics days we would always test analysis pipelines with simulated (incorporating as much of our physics knowledge as possible) data to ensure what comes out is what goes in and even then you can get into a trap of esoteric coding errors that might even be non-linear, so testing as much as possible is crucial and having independent tests of the pipelines was always crucial as well. With discovery directed particle physics experiments we would make sure to have a means to recover known physics parameters (such as cross-sections of processes we know that our experiment is sensitive to but not our main interest – an imbedded control if you will). I definitely add my +1 to strict and principled approaches. The other thing, which is hard for people to sometimes accept is maybe their experiment just isn’t sensitive to the process they are investigating of hypothesizing exists. In physics this results in placing an upper limit on an effect size, which, IMHO would be a good lesson for neuroimaging experimentalists. \nRuss Poldrack said on 2016-08-23\nmany thanks!"
  },
  {
    "objectID": "posts/crowdsourcing-my-next-fmri-task/index.html",
    "href": "posts/crowdsourcing-my-next-fmri-task/index.html",
    "title": "Crowdsourcing my next fMRI task",
    "section": "",
    "text": "This was originally posted on blogger here.\nFor those of you who have been following my self-tracking study, I am happy to announce that we are once again collecting data as of April 30. I had a followup audiometry exam this week which showed that the poor reading in early March appears to have been a fluke, and that my hearing does not seem to have changed since the beginning of the study.We have decided to make a number of changes to the MRI protocol based on our analyses of the first 6 months of data. In particular, we are going to reduce the frequency of the structural and DTI acquisitions (because, as expected, we don’t see much in the way of changes over time). In addition, after acquisition of 14 sessions on a working memory task, I have decided to reduce its frequency and add another task fMRI paradigm that will be collected once a week.My question for you is: What should I do next? I have several things in mind, but I want to see if the community can do even better. If you have an idea for task fMRI paradigm where a large number of sessions from a single individual would be useful, please email me (poldrack at utexas dot edu) with a brief proposal that outlines specifically why it would be interesting to perform the task repeatedly on a single person and how many sessions it would require. I will work with the winner to implement the task, and that person will of course be offered coauthorship on any papers that include those data. There are two requirements:Each session of the task must be accomplished within a single 10-minute imaging run.The task cannot require any special response or stimulation devices (for stimulation we have video projection and headphones; for response, we have several button boxes available as well as eye tracking, noise-cancelling microphone, and physiological monitoring).I look forward to hearing your ideas!"
  },
  {
    "objectID": "posts/crowdsourcing-my-next-fmri-task/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/crowdsourcing-my-next-fmri-task/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Crowdsourcing my next fMRI task",
    "section": "5 comments captured from original post on Blogger",
    "text": "5 comments captured from original post on Blogger\nUnknown said on 2013-05-04\nAny tasks about what internal thoughts you are having and how they are affecting your well being?\nPeter A. Bandettini said on 2013-05-08\nOf course it’s most interesting to see things that change. I can think of a few: day to day stress levels, performance/structure/function with learning a challenging task, long term atrophy (hopefully not too much!), perhaps hematrocrit - but’s that’s hard to get a sample for each day. For the extra task, I might perform a difficult continuous attention task that will show day to day variation and that has enough dynamic range in performance to show changes for up to a year. From heart rate (measured during the scan as well?) you can get heart rate variability - a sensitive indicator of stress levels.\nRuss Poldrack said on 2013-05-08\nPeter - thanks for the HRV suggestion, that’s a good one. We have not been collecting HR because of the logistic hassles of setting it up, but I think it would be a good idea.\nUnknown said on 2013-06-04\nThis is an amazingly interesting study. Do you encounter also for annual fluctuations, since particularly in males hormone levels etc. underlie an annual rhythm?Also I would like to suggest a task: alternate between sessions: During the first session study a poem, during the second recall it. Two reasons for that: both tasks are hippocampus dependent, but might integrate different other areas as well and blood flow might differ strongly between study and recall sessions, 2nd it is known that learning increases grey matter in the hippocampus over time, but it has never been demonstrated in such a direct way longitudinally.\nMona said on 2013-06-09\nDear Russ, If you are still looking for ideas, - how about something from the other side of brain - something from neuro regulation For example, spent 10 min doing some sort of Vagal maneuver, choose any method that works best for you and the situation..maybe breath-holding ..measure your blood pressure/pulse before and after"
  },
  {
    "objectID": "posts/talking-remotely-lessons-learned-so-far/index.html",
    "href": "posts/talking-remotely-lessons-learned-so-far/index.html",
    "title": "Talking remotely- Lessons learned so far",
    "section": "",
    "text": "This was originally posted on blogger here.\nSince making my commitment to reduce air travel for academic purposes, I’ve been giving a lot more remote talks. In the last 5 months I have given 10 remote talks - many thanks to those who have agreed to host me virtually rather than in person: September: National Academies Data Science in the Cloud workshop, Washington, DC National Academies Brain Health Across the Lifespan workshop, Washington, DC October: Cognitive Science Colloquium, Institut d’Etudes Cognitives, École Normale Supérieure, Paris, France. Johns Hopkins University Dept. of Electrical and Computer Engineering, Distinguished Lecturer Series, Baltimore, MD NIMH Talk Series on Machine Learning in Brain Imaging, Neuroscience, and Psychology, Bethesda, MD November: Santa Fe Institute, Cognitive Regime Shift meeting, Santa Fe, NM Montreal Neurological Institute, Open Science Symposium, Montreal Johns Hopkins University Dept. of Biostatistics, Bethesda, MD January: IBI Data Standards and Sharing Working Group, Tokyo, Japan Max Planck School of Cognition, Berlin, GermanySome of these were already bunched together so they wouldn’t have required separate flights, but even considering that, my back-of-the-envelope calculation shows that these flights would have resulted almost 7 tons of CO2 being generated (as estimated using https://www.icao.int/environmental-protection/Carbonoffset/Pages/default.aspx). Not to mention lots of physiological stress from jet lag, and travel costs to be borne by my hosts. So in many ways it’s been a huge win for everyone. An important issue, however, is what the experience was like, both for my hosts and the attendees and for myself. The visits varied from talks with a short Q&A session, to extended visits in which my talk was followed by individual meetings with researchers. For me the experience has been very positive — certainly not as good as being there in some ways, but still very satisfying. The least satisfying experience for me as a speaker has been in situations where I give a talk without time for Q&A afterwards. I think that my hosts have also largely found it to be a positive experience, at least from the feedback that I’ve received. In one case, I was the pilot test for hosting extended virtual visits, and afterwards they told me that the experience had convinced them to do it regularly. Going through these talks has taught me a few lessons about how to improve the experience, both for the speaker and for the audience.Always set up a time with the host to test things out in advance in the actual venue, preferably at least a few days before the talk. On the day of the talk, arrange to meet the host online at least 15 minutes before the scheduled talk time. Even when everything is well oiled, problems can arise, and you don’t want to be debugging them in front of an audience. Give your host your cell phone number, and keep your phone handy so that they have an alternate way to contact you if necessary. In general I think it’s good for a virtual talk to be a bit shorter than a regular talk, simply because it’s easier for people to fade off when you are not present to look them in the eye. Erring on the side of going short rather than long is also a good general principle — As an audience member I have rarely been upset when a talk went shorter than expected, and it gives more time for questions, which are usually the most interesting part anyway. For longer talks (over an hour), give the audience a short intermission. For example, for my talk to the Max Planck School of Cognition (a 90 min talk with 30 mins for questions), I asked the audience to stand up and stretch out about half way through, which they seemed to appreciate.I also have several suggestions for hosts of virtual visits:Please use a standard commercial conferencing system (like Zoom or Webex) rather than a home-grown system. Especially one that requires me to install special software! Having to install new software or log into a new system is just another potential point of failure for the talk. In general I have had the best experiences when using Zoom or Skype, but I’m sure there are other systems that are also good. As a speaker I particularly like being able to see a chat window on my screen as I’m talking, so that people can post questions during the talk. This works well with systems like Zoom, but often doesn’t exist at all in home-grown systems. Please provide a camera so that the speaker can see the audience. Talking without seeing the audience is much less pleasant and also makes it impossible to tell if people are disengaged, or if there is an unexpected problem with the A/V system. Make clear to the audience up front how questions will work. I prefer having them submitted by chat window, but if they are going to be spoken, then there should be microphones explicitly for the question, and these should be tested beforehand to make sure that the speaker can hear them. For extended visits, it has worked well to have a single Zoom room for the entire day, which individuals come into or out of throughout the day for their scheduled meetings. Please remember that people sitting in front a computer have biological needs just like people who are physically present, so schedule regular bio-breaks during the day. For events that are more discussion based, it’s important to have multiple microphones spread around the room so that the virtual attendees can hear what is being said. If someone is going to be writing on a whiteboard, it’s also important to have a camera on the board.Please leave other thoughts or suggestions in the comments below!"
  },
  {
    "objectID": "posts/talking-remotely-lessons-learned-so-far/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/talking-remotely-lessons-learned-so-far/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Talking remotely- Lessons learned so far",
    "section": "1 comments captured from original post on Blogger",
    "text": "1 comments captured from original post on Blogger\nUnknown said on 2020-01-29\nThanks so much for this - we’re doing a no-aviation workshop on Friday at UCSD, and this is a really handy list from the perspective of the speaker"
  },
  {
    "objectID": "posts/how-folksy-is-psychology-linguistic/index.html",
    "href": "posts/how-folksy-is-psychology-linguistic/index.html",
    "title": "How folksy is psychology? The linguistic history of cognitive ontologies",
    "section": "",
    "text": "This was originally posted on blogger here.\nI just returned from a fabulous meeting onRethinking the Taxonomy of Psychology, hosted by Mike Anderson, Tim Bayne, and Jackie Sullivan. I think that in another life I must have been a philosopher, because I always have so much fun hanging out with them, and this time was no different. In particular, the discussions at this meeting moved from simply talking about whether there is a problem with our ontology (which is old hat at this point) to specifically how we can think about using neuroscience to revise the ontology. I was particularly excited to see all of the interest from a group of young philosophers whose work is spanning philosophy and cognitive neuroscience, who I am counting on to keep the field moving forward!I have long made the the point that the conceptual structure of current psychology is not radically different from that of William James in the 19th century. This seems plausible on its face if you look at some of the section headings from his 1890 “To How Many Things Can We Attend At Once?”“The Varieties Of Attention.”“The Improvement Of Discrimination By Practice”“The Perception Of Time.”“Accuracy Of Our Estimate Of Short Durations”“To What Cerebral Process Is The Sense Of Time Due?”“Forgetting.”“The Neural Process Which Underlies Imagination”“Is Perception Unconscious Inference?”“How The Blind Perceive Space.”“Emotion Follows Upon The Bodily Expression In The Coarser Emotions At Least.”“No Special Brain-Centres For Emotion”“Action After Deliberation”:Beyond the sometimes flowery language, there are all topics that one could imagine being topics of research papers today, but for my talk I wanted to see if there was more direct evidence that the psychological ontology is less different (and thus more “folksy”) than ontologies in other sciences. To address this, I did a set of analyses that looked at the linguistic history of terms in the contemporary psychological ontology (as defined in the Cognitive Atlas) as compared to terms from contemporary biology (as enshrined in the Gene Ontology). I started (with a bit of help from Vanessa Sochat) by examining the proportion of terms from the Cognitive Atlas that were present in James’ Principles (from the full text available here). This showed that 22.9% of the terms in our current ontology were present in James’s text (some examples are: goal, deductive reasoning, effort, false memory, object perception, visual attention, task set, anxiety, mental imagery, unconscious perception, internal speech, primary memory, theory of mind, judgment). How does this compare to biology? To ask this, I obtained two biology textbooks published around the same time as James’ Principles (T. H. Huxley’s Course of Elementary Instruction in Practical Biology from 1892, and T. J. Parker’s Lessons in Elementary Biology from 1893), which are both available in full text from Google Books. In each of these books I assessed the presence of each term from the Gene Ontology, separately for each of the GO subdomains (biological processes, molecular functions, and cellular components). Here are the results: Huxley Parker Overlap biological process (28,566) 0.09% (26) 0.1% (32) 20 molecular functions (10,057) 0 0 - cellular components (3,903) 1.05% (41) 1.01% (40) 25 The percentages of overlap are much lower, perhaps not surprisingly since the number of GO terms is so much larger than the number of Cognitive Atlas terms. But even the absolute numbers are substantially lower, and there is not one mention of any of the GO molecular functions (striking but completely unsurprising, since molecular biology would not be developed for many more decades).These results were interesting, but it could be that they are specific to these particular books, so I generalized the analysis using the Google N-Gram corpus, which indexes the presence of individual words and phrases across more than 3 million books. Using a python package that accesses the ngram viewer API, I estimated the presence of all of the Cognitive Atlas terms as well as randomly selected subsets of each of the GO subdomains in the English literature between 1800 and 2000; I’m planning to rerun the analysis on the full corpus using the downloaded version of the N-grams corpus, but using this API required throttling that prevented me from the full sets of GO terms. Here are the results for the Cognitive Atlas:It is difficult to imagine stronger evidence that the ontology of psychology is relying on pre-scientific concepts; around 80% of the one-word terms in the ontology were already in use in 1800! Compare this to the Gene Ontology terms (note that there were not enough single-word molecular function terms to get a reasonable estimate):It’s clear that the while a few of the terms in these ontologies were in use prior to the development of the biosciences, the proportion is much smaller than what one sees for psychology. In my talk, I laid out two possibilities arising from this:Psychology has special access to its ontology that obviates the need for a rejection of folk conceptsPsychology is due for a conceptual revolution that will leave behind at least some of our current conceptsMy guess is that the truth lies somewhere in between these. The discussions that we had at the meeting in London provided some good ideas about how to conceptualize the kinds of changes that neuroscience might drive us to make to this ontology. Perhaps the biggest question to come out of the meeting was whether a data-driven approach can ever overcome the fact that the data were collected from experiments that are based on the current ontology. I am guessing that it can (given, e.g. the close relations between brain activity present in task and rest), but this remains one of the biggest questions to be answered. Fortunately there seems to be lots of interest and I’m looking forward to great progress on these questions in the next few years."
  },
  {
    "objectID": "posts/how-folksy-is-psychology-linguistic/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/how-folksy-is-psychology-linguistic/index.html#comments-captured-from-original-post-on-blogger",
    "title": "How folksy is psychology? The linguistic history of cognitive ontologies",
    "section": "6 comments captured from original post on Blogger",
    "text": "6 comments captured from original post on Blogger\nMichael Frank said on 2016-04-18\nNice post! A third possibility relative to your list is that a goal of psychology is to provide a level of explanation that ties the concepts of interest to our folk psychology to a more precise set of constructs. These more precise constructs nevertheless use (\"overload\") the same terms. So although William James talked about memory and attention, we taxonomize these in ways that correspond to - but build upon - the folk concept, e.g. using terms like \"declarative memory\" to add details. Also - when you study something like language learning (like me) - it’s weird to think that you’d want to abandon the folk ontology, which includes useful concepts like words, sentences, syllables, etc. Those really are the meaningful units of behavior.\nRuss Poldrack said on 2016-04-18\ngreat points! we had a good bit of discussion about whether the current terms are used in ways that are distinct from the folk concepts. I think that you are right that in some cases they are overloaded (e.g. the fractionation of \"memory\" - though the fundamental subcomponents like recollection and habit were also clear to James). interesting point regarding language, hadn’t thought of that.\nKalinaChristoff said on 2016-04-18\nGreat post, Russ. It made me think of what’s happening in the scientific study of mind-wandering right now. Mind wandering in some form or another has been of course a folksy concept for a long time. But as psychologists and neuroscientists started to more intensely investigate it in the last 10-15 years, the scientific concept \"developed\" to become ostensibly \"better defined\": it started being defined as \"task-unrelated or stimulus-independent thought\" (a definition that is overly narrow in my opinion). Through my collaborations with philosophers, I have recently become convinced and have argued that what we actually need in the study of mind-wandering currently is precisely going back to the original \"folksy\" meaning of this term. That’s because the scientific definition has become too narrow, and too content focused, while ignoring the original \"folksy\" intuition that mind-wandering is about how thoughts move (and not what their content is about). So at least in this case, you could say progress could mean going back to the concept’s folksy origin: we argue that the study of mind-wandering is due for a conceptual revolution by returning back to its \"folksy\" roots and espousing them a bit more fully.\nRuss Poldrack said on 2016-04-19\nInteresting point. it seems to me that if the scientifically defined concepts are too narrow, then they should be extended to cover the phenomena of interest. It seems to me that you are saying that the field should retreat back into the phenomena themselves (which are what the folk terms refer to), but that doesn’t actually seem like a scientiific move to me\nBrad Buchsbaum said on 2016-04-19\nAn interesting case study in language transformation in psychology is occurring in the memory literature: consider the (meteoric?) rise of the two terms \"pattern separation\" and \"pattern completion\". These terms come from computational models of the hippocampus and are being used in a way that in some sense bypasses the inherited folk-language of the psychology of memory.Although the terms lack folk analogues, one sometimes feels as if they are being used as stand-ins for a folk term. When we have a \"recollection\" are we \"pattern completing\"? And was that really \"pattern separation\" or was it \"recollect-to-reject\"? I have seen the terms sometimes substituted for the old folk term – is this progress?I do think the move toward \"computational terms\" over phenomenological ones is probably a welcome trend – but there is going to a long period where folk terms and \"computational terms\" blend together a bit and compete for ontological real estate.\nRuss Poldrack said on 2016-04-19\nI think this is a great example!"
  },
  {
    "objectID": "posts/on-healthy-choices/index.html",
    "href": "posts/on-healthy-choices/index.html",
    "title": "On “healthy choices”",
    "section": "",
    "text": "This was originally posted on blogger here.\nThis week, I received an email describing new nutritional recommendations at one of the campus eateries:Please come by O’s Campus Cafe over the next few weeks to see how they are making healthy eating easier!O’s Campus Cafe has analyzed their menu and labeled their foods in order to make it easier for you to make the healthiest choices. All foods have been given a red, yellow, or green rating based on how nutrient dense they are. Foods that have received a “Red” rating have very little positive nutrient content, are higher in unhealthy nutrients, and should be consumed minimally. Foods that have received a “Yellow” rating have some positive nutrient content, some unhealthy nutrients, and should be consumed less often. Foods that have received a “Green” are high in positive nutrient content, have little to no unhealthy nutrients, and can be consumed frequently. The healthy nutrients evaluated in these developing these ratings are protein, fiber, unsaturated fat, vitamins A, C, E, B-12, thiamin, riboflavin, and folate, and minerals calcium, magnesium, iron, and potassium. The nutrients considered unhealthy are saturated fat, cholesterol, and sodium. I would hope that guidelines regarding “healthy” choices at a major research university would be backed up with evidence, but it turns out that they were simply based on an off-the-shelf rating system (theNutrient Rich Food Index). To see whether they are supported by actual evidence, I used PubMed to look for relevant results from randomized controlled trials; I avoided epidemiological studies given all of the difficulties in making solid conclusions from nutritional epidemiology research (e.g. http://www.nytimes.com/2007/09/16/magazine/16epidemiology-t.html). Here is what I found regarding the supposedly “unhealthy” nutrients:Reduction of saturated fat intake was associated with reduced cardiovascular events but not with any changes in all-cause or cardiovascular mortality (Hooper et al., 2012). A nice review of lipid metabolism by Lands (2008) highlights how the search for mechanisms by which saturated fats might be harmful has failed: “Fifty years later [after starting his research in the 1960’s], I still cannot cite a definite mechanism or mediator by which saturated fat is shown to kill people.” A review of the effects of dietary sodium reduction in randomized controlled trials showed “no strong evidence of any effect of salt reduction” on all-cause mortality (Taylor et al., 2011).I searched deeply on PubMed but was unable to find any systematic reviews of the effects of dietary cholesterol reduction on mortality (please let me know if you know of any); there are lots of studies looking at cholesterol reduction using statins, but that doesn’t really inform the question of dietary cholesterol reduction. However, given that serum cholesterol levels bear very little relation to dietary cholesterol intake (Gertler et al., 1950), the labeling of dietary cholesterol as “unhealthy” seems unsupported by evidence. The first two reviews listed above come from the Cochrane Collaboration, which is generally viewed as a highly objective source for systematic reviews in evidence-based medicine.What about the supposed “healthy” choices?I also looked for data relevant to the the health benefits of the supposedly healthy choices:A recent report from a randomized controlled trial showed that replacement of saturated fats with omega-6 linoleic acid (an unsaturated, and thus “healthy” fat) was associated with increased, rather than decreased all-cause mortality (Ramsden et al., 2013). This makes sense given the association of omega-6 fatty acids with inflammation (e.g. Lands, 2012) and the powerful role that inflammation plays in many diseases. Increased fiber intake in the DART (diet and reinfarction trial) of individuals who had already suffered a heart attack was associated with no effects on all-cause mortality after 10 years.(Ness et al., 2002). An earlier Cochrane review also showed no effects of fiber intake on incidence or recurrence of colon cancer (Asano & McLeod, 2002). I don’t claim to be an expert in this domain, but the best evidence available seems to point towards the lack of any scientific validity of these “health” labels. Isn’t it better than nothing?One might ask whether it’s still better to have some guidance that pushes people to try to eat more healthily, even if it’s not supported by science. If the recommendations were made with full disclosure of the lack of scientific evidence (ala the disclaimer on dietary supplements) then I would be ok with that. Instead, the recommendations are made in the context of a “nutritionism” that pretends that we will be healthy if we just get the right mix of healthy nutrients and avoid the bad ones, and also pretends to know which are which. I personally would rather have no advice than unsupported pseudoadvice.At the same time, it’s clear from the obesity epidemic that people need help in making better food choices, so what should we do instead? I would propose implementing a simple rating system based the first of Michael Pollan’s food rules: “Eat food”. Anything that has been freshly picked, harvested, or slaughtered would get a “healthy rating” and anything that comes from a box, can, or bag would get an “unhealthy” rating. It would be much easier than computing micronutrient values, and I challenge anyone to show that it would be any less objectively healthy than the proposed “healthy choices” which would label a fresh grass-fed steak as less healthy than a pile of french fries fried in corn oil."
  },
  {
    "objectID": "posts/on-healthy-choices/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/on-healthy-choices/index.html#comments-captured-from-original-post-on-blogger",
    "title": "On “healthy choices”",
    "section": "2 comments captured from original post on Blogger",
    "text": "2 comments captured from original post on Blogger\nDaniel Kessler said on 2013-05-20\nTo be fair, can you point to any scientific literature that supports your recommendation to \"Eat food\"? I imagine you can, but am curious to read some literature that supports what seems common sense to me.\nRuss Poldrack said on 2013-05-20\nfair question - I’m not sure that I can point to any specific research that would support this very general claim. would be interesting to see a randomized controlled trial that assigned people to eating highly processed vs. unprocessed foods."
  },
  {
    "objectID": "posts/obesity-health-and-gary-taubes/index.html",
    "href": "posts/obesity-health-and-gary-taubes/index.html",
    "title": "Obesity, health, and Gary Taubes",
    "section": "",
    "text": "This was originally posted on blogger here.\nI recently posted a link on Facebook to Gary Taubes’ article about why the campaign to stop America’s obesity crisis keeps failing, and my friend Scott raised the following issue:Taubes may or may not be on to something. But, he comes off like an Intelligent Design guy, “The experts are wrong, and they can’t handle the Truth that I’m bringing.”I agree that Taubes’ writings sometimes have the feel of a crazy outsider fighting against the establishment. However, both my personal experience (as well as those of a number of friends) and my (non-expert) reading of the literature both suggest that Taubes is largely right on in terms of his critique of the standard dogma regarding weight loss, food, and health.First, the testimonial. As I noted in a previous post, it was Taubes’ writings that were largely responsible for pushing me towards the low-carb way of eating that I have followed for more than a year now. After cutting carbs way down (except for my daily dose of dark chocolate, which is non-negotiable), I lost 20 pounds of fat and have kept it off, without any sense that I am being deprived; I basically eat whatever I want whenever I want, as long as it’s real food and paleo-friendly (i.e., avoiding refined sugar, grains, and seed oils). Most important, I feel great eating this way; in particular, whereas I used to get serious hunger pangs and energy dips 3-4 hours after eating, I can now easily fast for 24 hours without feeling particularly hungry. My wife Jen has also had an interesting experience on this diet. She has been able to maintain her weight or lose weight while never feeling hungry, whereas on our old carb-heavy vegetarian diet she was only able to lose weight through radical caloric restriction that left her constantly famished. Similarly, a number of friends and family members have found that they were able to lose a substantial amount of weight after reducing carbs, while still feeling like they were able to eat to satiety. I have never heard anyone say that they went on a low-carb diet and gained weight; more often, I have heard from people that they went on a low-carb diet and lost weight but then were afraid that all of the saturated fat was going to cause them to have a heart attack any day.This brings us to one of the central points of Taubes’ writings, which is that the standard story about what comprises a healthy diet, namely the link between heart disease, cholesterol, and saturated fat, is just plain wrong. If you want a good overview of his general narrative without reading the books, I would suggest three NY Times pieces: one on the relation between dietary fat and disease from 2002, a blistering critique of epidemiological research from 2007, and his piece “Is Sugar Toxic” from 2011. There are a lot of claims in the Taubes books, and I have not looked into all of them. However, to the degree that I have looked into the claims that I found most important and relevant to my own diet, I have found them to all have fairly compelling scientific bases. The most important regards the relation between heart disease, cholesterol, and saturated fat. It is amazing how the supposed unhealthiness of dietary cholesterol and saturated fat has become a “fact” that is repeated almost reflexively (e.g., most recently I encountered it in Tyler Cowen’s “An Economist Gets Lunch”). I think that in part it is due to the visual similarity of saturated fat in meat and the plaques that are seen in atherosclerosis; it’s just too easy to believe that the saturated fat that we eat is “clogging our arteries.” The data appear to say otherwise. First, it has been known since 1950 that serum cholesterol bears little relation to dietary cholesterol; the mechanisms behind this are laid out nicely in Peter Attia’s recent series on cholesterol. Second, a large recent meta-analysis (including data from more than 347,000 individuals across 21 published studies) found no relation between saturated fat intake and heart disease or stroke. Similarly, a recent Cochrane Collaborative meta-analysisof intervention studies showed that there was no significant reduction of total or cardiovascular mortality due to changes in dietary fat. Although I think Taubes is correct in his arguments that epidemiological studies are hugely problematic (which I will discuss some other time), I trust these large meta-analyses much more than I trust any individual study (e.g., the China Study), especially when they show no effect (given all of the biases towards publishing positive effects). I have also put my money where my mouth is: I now eat a high-fat diet including full-fat yogurt, eggs, and bacon almost every morning. It look me several months to stop craving sugar, but I’m now perfectly happy to a dinner without dessert, and in fact I no longer have a taste for foods that are extremely sugary. Another of Taubes’ main assertions is that obesity is caused primarily by carbohydrates (fleshed out in his book Why We Get Fat). My feeling here is that obesity is an incredibly complex problem that involves both the body and the brain, and any story that tries to simplify it to a single component of our environment is bound to be wrong. That said, it’s clear to me from the person experience described above that the “calories in, calories out” story is wrong, and that it does matter a lot what one eats, not just how much. There has recently been a big argument in the blogosphere recently between Stephan Guyenetand Taubes over the relative importance of peripheral factors (e.g., insulin’s effects on fat storage) versus neural factors (e.g., the role of satiety hormones and reward pathways); both of them have staked out strong positions, and I think that the truth is likely to fall somewhere in the middle (as usual). As a neuroscientist I clearly think that the brain plays an important role; I won’t talk more about that here, maybe some time soon. I have not dug very deeply into the science of feeding trials in animals, in part because my reading of several summaries of this work suggests that the details can be very tricky but very important. In particular, without a great deal of control over exactly what kinds of nutrients are in the food, it can be very difficult to make any conclusions from the data. There are however some individual studies in humans that do provide support for Taubes’ claims. For example, the A to Z Weight Loss Study showed that subjects assigned to the Atkins diet lost more weight and had better metabolic outcomes than people assigned to low-fat/high-carb diets like the Ornish Diet. It’s just one study, and it would be good to see more, but this along with my personal experience is enough to convince me. Later in the discussion that I mentioned above, Scott made the following additional observation:But the question remains, why doesn’t the science on the cellular level ever reach the public health scientific community? I’m usually very skeptical when some sort of conspiracy is trotted out to explain the lack of uptake.There are a lot of answers to this, which Taubes goes into great detail to discuss in his books. But the general problem is that science itself can be a slow-moving ship; when it gets drawn well off course (as it appears to have been by the anti-fat arguments of Keys and others), it can take a long time to get back, and even longer for that new scientific knowledge to get translated into medical education and practice. What is most striking to me is how studies whose data seem clearly inconsistent with the standard view are often presented in a way that suggests that they support the view, often by picking and choosing specific conditions. Taubes gives numerous examples of this, as does Denise Minger’s detailed analysis of the China Study. I’ve also noticed it on a number of occasions when reading papers in this literature. Thus, unless one is reading the papers closely (or following others who do this), it can be easy to continue to think that the standard model remains valid; you just can’t take the abstract (or even the results section) at face value. However, the fact that the rise in obesity has occurred alongside declining fat intake (coupled with increasing carb intake)over the last 40 years makes it pretty clear to me that the standard theory is just plain wrong, and that the carb theory is a viable alternative that needs to be studied more intently. Unfortunately, many of the thought leaders in this area continue to expound solutions based on the “calories-in/calories-out” and low-fat ideas that got us here in the first place."
  },
  {
    "objectID": "posts/obesity-health-and-gary-taubes/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/obesity-health-and-gary-taubes/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Obesity, health, and Gary Taubes",
    "section": "7 comments captured from original post on Blogger",
    "text": "7 comments captured from original post on Blogger\n…d said on 2012-08-01\nHi Russ, reading your article put me onto Taubes’ eye opening work, so thanks! However, I recently found an article via boingboing critiquing Taubes’ argument and wondered what you thought about it:http://wholehealthsource.blogspot.com.au/2011/08/carbohydrate-hypothesis-of-obesity.html\nRuss Poldrack said on 2012-08-02\nI’m not a physiologist so I can’t speak with any expertise regarding this issue. My impression is that Taubes’ insulin story is overly simplistic, and Guyenet is certainly right that the brain plays a central role in obesity. However, it’s clear that carb calories are not the same as fat or protein calories in their metabolic effects (cf. the recent work by Ludwig’s group on resting metabolism), so I’m with Taubes on that.\n…d said on 2012-08-02\nI also think the A to Z Weight Loss study was pretty convincing with regard to low-carb diet metabolic outcomes (not just weight but HDL, triglycerides etc). I don’t suppose you could let me know the title of the study from Ludwig’s group. I couldn’t find it from searching Scholar.\nRuss Poldrack said on 2012-08-02\nHere is the Ludwig paper: http://www.ncbi.nlm.nih.gov/pubmed/22735432\n…d said on 2012-08-03\nThanks!\nRogier said on 2013-03-13\nVery interesting! Are you planning to look at seasonal variation also? There’s some evidence that, for instance, BDNF is quite variable throughout the year (http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0048046) , would be fascinating if something similar is true for resting state BOLD response (which if true could have a variety of explanations of course, including sub-threshold SAD).\nRuss Poldrack said on 2013-03-13\nperhaps you should remove this post from here and re-post to the comments at http://www.russpoldrack.org/2013/03/my-adventures-in-self-quantification.html ?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Russ Poldrack’s blog",
    "section": "",
    "text": "An update on moving from MacOS to Linux on the desktop\n\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2023\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nTeaching the Psychology of the Climate Crisis\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2023\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nMaking the move from Mac to Linux on the desktop - finally!\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2022\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nOrganizing Markdown-based talks using Github\n\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nMoving from Keynote to Markdown-based presentations\n\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nAdvice for getting a job as an interdisciplinary researcher\n\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2022\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nReading ‘Error Tight’ by Julia Strand\n\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2022\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nTeaching statistics online (for the first time)- Lessons learned\n\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2021\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nEditing lecture videos using Davinci Resolve\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2020\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nA quick and dirty workflow for creating lecture videos\n\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2020\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nHome office setup\n\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2020\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nVacation fun- Making traditional Texas chili con carne\n\n\n\n\n\n\n\n\n\n\n\n\nJul 23, 2020\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nTalking remotely- Lessons learned so far\n\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2020\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nComputing models for a neuroimaging lab\n\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2019\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nWhy I will be flying less\n\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2019\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nProductivity stack for 2019\n\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2018\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nAutomated web site generation using Bookdown, CircleCI, and Github\n\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2018\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nStatistical Thinking for the 21st Century - a new intro statistics book\n\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2018\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nHow can one do reproducible science with limited resources?\n\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2018\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nTo Code or Not to Code (in intro statistics)?\n\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2018\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nDefaults in R can make debugging incredibly hard for beginners\n\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2018\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nThe NIH should stop penalizing collaborative research\n\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2016\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nWhy preregistration no longer makes me nervous\n\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2016\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nInterested in the Poldrack Lab for graduate school?\n\n\n\n\n\n\n\n\n\n\n\n\nAug 24, 2016\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nThe principle of assumed error\n\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2016\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nHaving my cake and eating it too?\n\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2016\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nScam journals will literally publish crap\n\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2016\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nAdvice for learning to code from scratch\n\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2016\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nHow folksy is psychology? The linguistic history of cognitive ontologies\n\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2016\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nReproducibility and quantitative training in psychology\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2016\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nReproducible analysis in the MyConnectome project\n\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2015\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nAre good science and great storytelling compatible?\n\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2015\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nNew course on decision making- Seeking feedback\n\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2015\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\n532 days of self-examination\n\n\n\n\n\n\n\n\n\n\n\n\nMar 16, 2014\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nA discussion of causal inference on fMRI data\n\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2013\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nCrowdsourcing my next fMRI task\n\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2013\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nThe dimensional approach to studying mental illness\n\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2013\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nOn “healthy choices”\n\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2013\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nHow well can we predict future criminal acts from fMRI data?\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2013\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nMy adventures in self-quantification\n\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2013\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nAnatomy of a coding error\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2013\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nIs reverse inference a fallacy? A comment on Hutzler\n\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2013\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nThe perils of leave-one-out crossvalidation for individual difference analyses\n\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2012\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nObesity, health, and Gary Taubes\n\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2012\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nThings I like to do in Beijing\n\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2012\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nSkeletons in the closet\n\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2012\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nQuitting cable\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2012\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\n2011 in review\n\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2011\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nNYT Letter to the Editor- The uncut version\n\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2011\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nSigners of letter to the editor of the New York Times\n\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2011\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nNYT Op-Ed + fMRI = complete crap\n\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2011\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nMy analysis of OHBM 2011 abstracts\n\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2011\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nRecent Reads\n\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2011\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nwhy I cut the carbs\n\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2011\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nGoals for the new year\n\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2011\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\non carbon footprints and biased judgments\n\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2010\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nMy personal computing history\n\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2010\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nBecoming an academic road warrior\n\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2010\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nMy Productivity Toolbox\n\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2010\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nstatistical redistricting- how to save lots of time and money and get just about the same result\n\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2010\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nMy workflow for writing papers (or, why I switched to LaTeX)\n\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2010\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nTemplate- russpoldrack.org\n\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2010\n\n\nRuss Poldrack\n\n\n\n\n\n\n  \n\n\n\n\nWelcome\n\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2010\n\n\nRuss Poldrack\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/goals-for-new-year/index.html",
    "href": "posts/goals-for-new-year/index.html",
    "title": "Goals for the new year",
    "section": "",
    "text": "This was originally posted on blogger here.\nAlthough I’ve never been a big fan of new year’s resolutions, I do like to set some goals at the beginning of the year so that I have measureable objectives to strive for. My main goal for 2010 was to exercise at least 4 times a week. That was difficult during the spring because I was traveling a ton, but since August I have done a pretty good job of exercising 5-6 times a week. The main thing I have done to accomplish this is to make going to the gym a daily morning ritual. It can be very hard to make myself go to the gym if I’ve not been doing it regularly, but once I’m in the habit then it doesn’t really take much gumption to get myself there.A second goal, somewhat harder to quantify, was “say no more often.” I apologize to those of you who might have borne the brunt of this goal, but I think I have been rather successful. I said no to a number of talk invitations, skipped two conferences (SFN and MDRS), and have refused an increasingly proportion of review requests. Then again, there were a lot of things I did not say no to, reflected in the fact that I flew more than 100,000 miles in 2010 (hello Platinum Status!)For 2011 I am setting three new goals. Work toward a travel moratorium for 2012. I have traveled a lot in the past few years, and 2011 will be no different. I love traveling and have learned to deal with the stresses of air travel. However, the physical toll of heavy travel can be substantial, because it makes it very hard to keep up my exercise routine and also makes it hard to stick to a healthy eating plan. I will never be able to go a year without any travel, due to my various professional responsibilities. However, the majority of my travel is elective, in the sense that I could say no without greatly harming my career. During 2011, my default answer for any invitations to travel during 2012 will be “no”. Improve climbing skills well enough to lead climb. One big revelation for me in 2010 was the fun of rock climbing, which one of my friends has been kind enough to introduce me to over the last couple of months. So far I have only done “top rope” climbing, where someone else (the “lead climber”) first ascends the face and puts the rope in place from the top, and then I climb the same route. By the end of the year I would like to be able to lead a route myself, which will require a combination of climbing skill and nerves (since one can fall much farther while lead climbing compared to top rope climbing). No new web projects. Over the last few years my students and I have started a number of web projects. Some of these, such as The Cognitive Atlasand PubBrain are fairly well established and get a decent bit of traffic, while others such as fmrimethods.org have languished. In 2010 I started openfmri.org which is a project to support sharing of raw brain imaging data; this project requires a good bit of work because the data have to be organized and validated. For 2011 I am placing a moratorium on new web projects, so that I can focus on the projects that I already have in place."
  },
  {
    "objectID": "posts/goals-for-new-year/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/goals-for-new-year/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Goals for the new year",
    "section": "1 comments captured from original post on Blogger",
    "text": "1 comments captured from original post on Blogger\nAndrew Eskind said on 2011-10-05\nAs a layperson, I appreciate the way you have debunked the pretentious NYT piece."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Template- russpoldrack.org",
    "section": "",
    "text": "This was originally posted on blogger here."
  },
  {
    "objectID": "posts/index.html#section",
    "href": "posts/index.html#section",
    "title": "Template- russpoldrack.org",
    "section": "/*",
    "text": "/*\nBlogger Template Style Name: Simple Designer: Blogger URL: www.blogger.com ———————————————– */\n/* Variable definitions ==================== \n   \n    \n    \n   \n  \n    \n   \n  \n       \n    \n    \n    \n   \n \n \n     \n\n    \n  \n\n    \n\n \n \n   \n\n\n  \n  */\n/* Content ———————————————– */ body { font: $(body.font); color: $(body.text.color); background: $(body.background); padding: 0 $(content.shadow.spread) $(content.shadow.spread) $(content.shadow.spread); $(body.background.override) }\nhtml body $(page.width.selector) { min-width: 0; max-width: 100%; width: $(page.width); }\nh2 { font-size: 22px; }\na:link { text-decoration:none; color: $(link.color); }\na:visited { text-decoration:none; color: $(link.visited.color); }\na:hover { text-decoration:underline; color: $(link.hover.color); }\n.body-fauxcolumn-outer .fauxcolumn-inner { background: transparent $(body.background.gradient.tile) repeat scroll top left; _background-image: none; }\n.body-fauxcolumn-outer .cap-top { position: absolute; z-index: 1; height: 400px; width: 100%; }\n.body-fauxcolumn-outer .cap-top .cap-left { width: 100%; background: transparent $(body.background.gradient.cap) repeat-x scroll top left; _background-image: none; }\n.content-outer { -moz-box-shadow: 0 0 $(content.shadow.spread) rgba(0, 0, 0, .15); -webkit-box-shadow: 0 0 $(content.shadow.spread.webkit) rgba(0, 0, 0, .15); -goog-ms-box-shadow: 0 0 $(content.shadow.spread.ie) #333333; box-shadow: 0 0 $(content.shadow.spread) rgba(0, 0, 0, .15);\nmargin-bottom: 1px; }\n.content-inner { padding: $(content.padding) $(content.padding.horizontal); }\n$(content.background.color.selector) { background-color: $(content.background.color); }\n/* Header ———————————————– */ .header-outer { background: $(header.background.color) $(header.background.gradient) repeat-x scroll 0 -400px; _background-image: none; }\n.Header h1 { font: $(header.font); color: $(header.text.color); text-shadow: $(header.shadow.offset.left) $(header.shadow.offset.top) $(header.shadow.spread) rgba(0, 0, 0, .2); }\n.Header h1 a { color: $(header.text.color); }\n.Header .description { font-size: $(description.text.size); color: $(description.text.color); }\n.header-inner .Header .titlewrapper { padding: 22px $(header.padding); }\n.header-inner .Header .descriptionwrapper { padding: 0 $(header.padding); }\n/* Tabs ———————————————– */ .tabs-inner .section:first-child { border-top: $(header.bottom.border.size) solid $(tabs.border.color); }\n.tabs-inner .section:first-child ul { margin-top: -$(header.border.size); border-top: $(header.border.size) solid $(tabs.border.color); border-left: $(header.border.horizontalsize) solid $(tabs.border.color); border-right: $(header.border.horizontalsize) solid $(tabs.border.color); }\n.tabs-inner .widget ul { background: $(tabs.background.color) $(tabs.background.gradient) repeat-x scroll 0 -800px; _background-image: none; border-bottom: $(tabs.border.width) solid $(tabs.border.color);\nmargin-top: \\((tabs.margin.top);  margin-left: -\\)(tabs.margin.side); margin-right: -$(tabs.margin.side); }\n.tabs-inner .widget li a { display: inline-block;\npadding: .6em 1em;\nfont: $(tabs.font); color: $(tabs.text.color);\nborder-$startSide: $(tabs.border.width) solid \\((content.background.color);  border-\\)endSide: $(tabs.bevel.border.width) solid $(tabs.border.color); }\n.tabs-inner .widget li:first-child a { border-$startSide: none; }\n.tabs-inner .widget li.selected a, .tabs-inner .widget li a:hover { color: $(tabs.selected.text.color); background-color: $(tabs.selected.background.color); text-decoration: none; }\n/* Columns ———————————————– */ .main-outer { border-top: $(main.border.width) solid $(body.rule.color); }\n.fauxcolumn-left-outer .fauxcolumn-inner { border-right: 1px solid $(body.rule.color); }\n.fauxcolumn-right-outer .fauxcolumn-inner { border-left: 1px solid $(body.rule.color); }\n/* Headings ———————————————– */ div.widget &gt; h2, div.widget h2.title { margin: 0 0 1em 0;\nfont: $(widget.title.font); color: $(widget.title.text.color); }\n/* Widgets ———————————————– */ .widget .zippy { color: $(widget.alternate.text.color); text-shadow: 2px 2px 1px rgba(0, 0, 0, .1); }\n.widget .popular-posts ul { list-style: none; }\n/* Posts ———————————————– */ h2.date-header { font: $(date.header.font); }\n.date-header span { background-color: $(date.header.background.color); color: $(date.header.color); padding: $(date.header.padding); letter-spacing: $(date.header.letterspacing); margin: $(date.header.margin); }\n.main-inner { padding-top: $(main.padding.top); padding-bottom: $(main.padding.bottom); }\n.main-inner .column-center-inner { padding: 0 $(main.padding); }\n.main-inner .column-center-inner .section { margin: 0 $(main.section.margin); }\n.post { margin: 0 0 $(post.margin.bottom) 0; }\nh3.post-title, .comments h4 { font: $(post.title.font); margin: .75em 0 0; }\n.post-body { font-size: 110%; line-height: 1.4; position: relative; }\n.post-body img, .post-body .tr-caption-container, .Profile img, .Image img, .BlogList .item-thumbnail img { padding: $(image.border.small.size);\nbackground: $(image.background.color); border: 1px solid $(image.border.color);\n-moz-box-shadow: 1px 1px 5px rgba(0, 0, 0, .1); -webkit-box-shadow: 1px 1px 5px rgba(0, 0, 0, .1); box-shadow: 1px 1px 5px rgba(0, 0, 0, .1); }\n.post-body img, .post-body .tr-caption-container { padding: $(image.border.large.size); }\n.post-body .tr-caption-container { color: $(image.text.color); }\n.post-body .tr-caption-container img { padding: 0;\nbackground: transparent; border: none;\n-moz-box-shadow: 0 0 0 rgba(0, 0, 0, .1); -webkit-box-shadow: 0 0 0 rgba(0, 0, 0, .1); box-shadow: 0 0 0 rgba(0, 0, 0, .1); }\n.post-header { margin: 0 0 1.5em;\nline-height: 1.6; font-size: 90%; }\n.post-footer { margin: 20px -2px 0; padding: 5px 10px;\ncolor: $(post.footer.text.color); background-color: $(post.footer.background.color); border-bottom: 1px solid $(post.footer.border.color);\nline-height: 1.6; font-size: 90%; }\n#comments .comment-author { padding-top: 1.5em;\nborder-top: 1px solid $(body.rule.color); background-position: 0 1.5em; }\n#comments .comment-author:first-child { padding-top: 0; border-top: none; }\n.avatar-image-container { margin: .2em 0 0; }\n#comments .avatar-image-container img { border: 1px solid $(image.border.color); }\n/* Comments ———————————————– */ .comments .comments-content .icon.blog-author { background-repeat: no-repeat; background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABIAAAASCAYAAABWzo5XAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEgAACxIB0t1+/AAAAAd0SU1FB9sLFwMeCjjhcOMAAAD+SURBVDjLtZSvTgNBEIe/WRRnm3U8RC1neQdsm1zSBIU9VVF1FkUguQQsD9ITmD7ECZIJSE4OZo9stoVjC/zc7ky+zH9hXwVwDpTAWWLrgS3QAe8AZgaAJI5zYAmc8r0G4AHYHQKVwII8PZrZFsBFkeRCABYiMh9BRUhnSkPTNCtVXYXURi1FpBDgArj8QU1eVXUzfnjv7yP7kwu1mYrkWlU33vs1QNu2qU8pwN0UpKoqokjWwCztrMuBhEhmh8bD5UDqur75asbcX0BGUB9/HAMB+r32hznJgXy2v0sGLBcyAJ1EK3LFcbo1s91JeLwAbwGYu7TP/3ZGfnXYPgAVNngtqatUNgAAAABJRU5ErkJggg==); }\n.comments .comments-content .loadmore a { border-top: 1px solid $(widget.alternate.text.color); border-bottom: 1px solid $(widget.alternate.text.color); }\n.comments .comment-thread.inline-thread { background-color: $(post.footer.background.color); }\n.comments .continue { border-top: 2px solid $(widget.alternate.text.color); }\n/* Accents ———————————————- */ .section-columns td.columns-cell { border-$startSide: 1px solid $(body.rule.color); }\n.blog-pager { background: $(paging.background); }\n.blog-pager-older-link, .home-link, .blog-pager-newer-link { background-color: $(content.background.color); padding: 5px; }\n.footer-outer { border-top: $(footer.bevel) dashed #bbbbbb; }\n/* Mobile ———————————————– */ body.mobile { background-size: $(mobile.background.size); }\n.mobile .body-fauxcolumn-outer { background: $(mobile.background.overlay); }\n.mobile .body-fauxcolumn-outer .cap-top { background-size: 100% auto; }\n.mobile .content-outer { -webkit-box-shadow: 0 0 3px rgba(0, 0, 0, .15); box-shadow: 0 0 3px rgba(0, 0, 0, .15); }\n.mobile .tabs-inner .widget ul { margin-left: 0; margin-right: 0; }\n.mobile .post { margin: 0; }\n.mobile .main-inner .column-center-inner .section { margin: 0; }\n.mobile .date-header span { padding: 0.1em 10px; margin: 0 -10px; }\n.mobile h3.post-title { margin: 0; }\n.mobile .blog-pager { background: transparent none no-repeat scroll top center; }\n.mobile .footer-outer { border-top: none; }\n.mobile .main-inner, .mobile .footer-inner { background-color: $(content.background.color); }\n.mobile-index-contents { color: $(body.text.color); }\n.mobile-link-button { background-color: $(link.color); }\n.mobile-link-button a:link, .mobile-link-button a:visited { color: $(mobile.button.color); }\n.mobile .tabs-inner .section:first-child { border-top: none; }\n.mobile .tabs-inner .PageList .widget-content { background-color: $(tabs.selected.background.color); color: $(tabs.selected.text.color); border-top: $(tabs.border.width) solid $(tabs.border.color); border-bottom: $(tabs.border.width) solid $(tabs.border.color); }\n.mobile .tabs-inner .PageList .widget-content .pagelist-arrow { border-$startSide: 1px solid $(tabs.border.color); }\n  body {\n    min-width: $(content.width);\n  }\n\n  .content-outer, .content-fauxcolumn-outer, .region-inner {\n    min-width: $(content.width);\n    max-width: $(content.width);\n    _width: $(content.width);\n  }\n\n  .main-inner .columns {\n    padding-left: $(main.column.left.width);\n    padding-right: $(main.column.right.width);\n  }\n\n  .main-inner .fauxcolumn-center-outer {\n    left: $(main.column.left.width);\n    right: $(main.column.right.width);\n    /* IE6 does not respect left and right together */\n    _width: expression(this.parentNode.offsetWidth -\n        parseInt(\"$(main.column.left.width)\") -\n        parseInt(\"$(main.column.right.width)\") + 'px');\n  }\n\n  .main-inner .fauxcolumn-left-outer {\n    width: $(main.column.left.width);\n  }\n\n  .main-inner .fauxcolumn-right-outer {\n    width: $(main.column.right.width);\n  }\n\n  .main-inner .column-left-outer {\n    width: $(main.column.left.width);\n    right: 100%;\n    margin-left: -$(main.column.left.width);\n  }\n\n  .main-inner .column-right-outer {\n    width: $(main.column.right.width);\n    margin-right: -$(main.column.right.width);\n  }\n\n  #layout {\n    min-width: 0;\n  }\n\n  #layout .content-outer {\n    min-width: 0;\n    width: 800px;\n  }\n\n  #layout .region-inner {\n    min-width: 0;\n    width: auto;\n  }\n\n  body#layout div.add_widget {\n    padding: 8px;\n  }\n\n  body#layout div.add_widget a {\n    margin-left: 32px;\n  }\n  \n\n\n\n\n\n\n\n0 -1 false false BEHIND 0\ntrue #666666 true true #666666 true #2288bb TextAndImage #ffffff\nfalse 1x1 true true true 1 false #ffffff false false\n      &lt;/div&gt;&lt;/div&gt;\n    \n\n      &lt;div class=\"date-outer\"&gt;\n    \n\n\n\n\n      &lt;div class=\"date-posts\"&gt;\n    \n\n\n\n\n\n\n\n\n\n\n\n\n    &lt;/div&gt;&lt;/div&gt;\n  \n:\n:\n           \n        \n           \n      \n\n\n         \n        \n         \n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n       \n      \n       \n      \n()\n›\n‹\n›\n,\n:\nI’m Russ Poldrack, a Professor of Psychology at Stanford University. This is my personal blog and any statements made only represent my personal opinion.\nNONE @russpoldrack on Twitter https://twitter.com/russpoldrack Poldrack lab web site http://www.poldracklab.org/\ntrue true #999999 #000000 #666666 #666666 #2288bb #2288bb #2288bb\nHIERARCHY yyyy true MMMM MMM dd MM/dd false true MONTHLY\n()\n()\n()\n    ▼ \n  \n    ◄ \n  \n    ► \n  \n#ffffff #666666 160x600 #ffffff #2288bb #000000 TextAndImage\ntrue false"
  },
  {
    "objectID": "posts/switching-to-linux-update/index.html",
    "href": "posts/switching-to-linux-update/index.html",
    "title": "An update on moving from MacOS to Linux on the desktop",
    "section": "",
    "text": "It’s been about eight months since I switched to Linux on the desktop - here’s an update. The TL;DR is that I am overall very happy that I made the switch.\nMy favorite thing about moving is that I feel much more in control of my system than I did on the Mac, due to all of the recent security “improvements” on MacOS. No hidden filesystems, no app restarts due to system permissions, no fighting with the system to install non-certified software packages. Most software updates can be done without a reboot, and all of them can be done from the command line. When a reboot is required, it’s quick; no more sitting with the machine unavailable for long periods of time waiting for a system update. I would liken the difference to driving a car with a manual transmission versus an automatic; it takes a bit more practice to drive with a stick, but the experience is much more direct and (for many people) much more fun than driving with an automatic.\nThere are very few things that I could do on the Mac that I can’t do on Linux. Fortunately, in those cases there are generally cloud services that I can use. The one I use most commonly is Adobe Acrobat, which I sometimes require to work with PDF portfolios and signing of PDFs. The cloud version of Acrobat (paid subscription) has worked well for this. I’ve also occasionally needed to use the Office365 version of Word.\nI still keep my Mac around, for one main reason: music. I like to play guitar directly from the computer (so I can mix it with music to play along) and control my HX Stomp via USB using the HX Edit app. There is simply no comparison in the level of music support in Linux compared to MacOS.\nOne thing I am still not loving is my presentation workflow. I started out using Quarto to build my talks, and I loved it for a while, but at some point I got tired of spending way too much time on manual positioning of graphic elements. So, in the last few months I have largely switched to using Google Slides. I don’t love the interface (in particular, I find the template workflow annoying) but overall it works fine. The clincher for me has been the ease with which I can convert my old keynote slides using @cloudconvert, which somehow seens to do a better job of converting than Keynote’s own pptx exporter!\nHardware-wise I am fairly happy with the Thinkpad X1. I’ve also gotten good feedback from others about the laptops from Framework, which are modular and highly customizable. It turns out that having a Linux laptop can often be a conversation-starter!\nOverall I would highly recommend trying out the switch if you are curious, but I would also add the following nota bene: My experience may not be universal, given that I have significant experience with Linux system administration. I don’t feel like it has come up very often, but on occasion it’s been really useful to have enough sysadmin skills to feel comfortable messing with system files. The most notable example is that at one point my /boot partition became full because of a number of older kernels that were hanging around and were not being removed by the autoremove function, which prevented updating of the kernel. I had to delete some of these files by hand to make room for the new kernel, which would be a pretty bad idea unless you really know what you are doing. Another place it has come up is in security configuration, where I wanted to configure my machine with more stringent settings than default, such as changing the SSH port from the default setting. Less experienced users would probably be stuck blindly copying and pasting commands from Stack Overflow, which usually works but can on occasion be a recipe for disaster.\nOne could almost certainly use Linux on the desktop without knowing any sysadmin skills and probably be fine 99% of the time, but I would be nervous about recommending it for one’s primary system, especially on the road. On the other hand, how better to get those skills than trial by fire! I learned them first by playing with my own systems as side projects, and then when I ran a cluster for our lab at UCLA (which I would definitely not recommend - there is a reason that sysadmin is a separate job description from faculty member!). If you are nervous about it, you can always try out Linux in a virtual machine before going all in."
  },
  {
    "objectID": "posts/computing-models-for-neuroimaging-lab/index.html",
    "href": "posts/computing-models-for-neuroimaging-lab/index.html",
    "title": "Computing models for a neuroimaging lab",
    "section": "",
    "text": "This was originally posted on blogger here.\nI had a conversation with a colleague recently about how to set up computing for a new neuroimaging lab. I thought that it might be useful for other new investigators to hear the various models that we discussed and my view of their pros and cons. My guess is that many of the same issues are relevant for other types of labs outside of neuroimaging as well - let me know in the comments below if you have further thoughts or suggestions!The simplest model: Personal computers on premiseThe simplest model is for each researcher in the lab to have their own workstation (or laptop) on which all of their data live and all of their computing is performed.Pros: Easy to implement Freedom: Each researcher can do whatever they want (within the bounds of the institution’s IT policies) as they have complete control over their machine. NOTE: I have heard of institutions that do not allow anyone on campus to have administrative rights over their own personal computer. Before one ever agrees to take a position, I would suggest inquiring about the IT policies and make sure that they don’t prevent this; if they do, then ask the chair to add language to your offer letter than explicitly provides you with an exception to that policy. Otherwise you will be completely at the mercy of the IT staff — and this kind of power breeds the worst behavior in those staff. More generally, you should discuss IT issues with people at an institution before accepting any job offer, preferably with current students and postdocs, since they will be more likely to be honest about the challenges. Cons: Lack of scalability: Once they need to run more jobs than there are cores on the machine, the researcher could end up waiting a very long time for those jobs to complete, and/or crash the machine due to resource insufficiency. These systems also generally have limited disk space.Underuse: One can of course buy workstations with lots of cores/RAM/storage, which can help address the previous point to some degree. However, then one is paying a lot of money for resources that will sit underutilized most of the time. Admin issues: Individual researchers are responsible for managing their own systems. This means that each researcher in the lab will likely be using different versions of each software package, unless some kind of standardized container system is implemented. This also means that each researcher needs to spend their precious time dealing with software installation issues, etc, unless there is a dedicated system admin, which costs $$$$. Risk: The systems used for these kinds of operations are generally commodity-level systems, which are more likely to fail compared to enterprise-level systems (discussed below). Unless the lab has a strict policy for backup or duplication (e.g. on Dropbox or Box) then it’s almost certain that at some point data will be lost. There is also a non-zero risk of personal computers being stolen or lost.Verdict: I don’t think this is generally a good model for any serious lab. The only strong reason that I could see for having local workstations for data analysis is if one’s analysis requires a substantial amount of graphics-intensive manual interaction.Virtual machines in the cloudUnder this model, researchers in the lab house their data on a commercial cloud service, and spin up virtual machines on that service as needed for data analysis purposes.Pros: Flexibility: This model allows the researcher to allocate just enough resources for the job at hand. For the smallest jobs, one can sometimes get by with the free resources available from these providers (I will use Amazon Web Services[AWS] as an example here since it’s the one I’m most familiar with). On AWS, one can obtain a free t2.micro instance (with 1 GB RAM and 1 virtual CPU); this will not be enough to do any real analysis, but could be sufficient for many other functions such as working with files. At the other end, one can also allocate a c5.24xlarge instance with 96 virtual CPUs and 192 GiB of RAM for about $4/hour. This range of resources should encompass the needs of many labs. Similarly, on the space side, you can scale your storage space in an effectively unlimited way. Resource-efficiency: You only use what you pay for. Energy-efficiency: Cloud services are thought to be much more energy-efficient compared to on-premise computers, due to their higher degree of utilization (i.e. they are not sitting idle most of the time) and the fact that they often obtain their power from renewable resources. AWS estimates that cloud computing can reduce carbon emissions by up to 88% compared to on-premise computers.Resilience: Occasionally the hardware on a cloud VM goes out. When this happens, you simply spin up a new one — no hardware replacement cost.Cons: Administration and training: Since most scientists will not have experience spinning up and administering cloud systems, there will be some necessary training to make this work well; preferably, one would have access to a system administrator with cloud experience. Researchers need to be taught, for example, to shut down expensive instances after using them, lest the costs begin to skyrocket. Costs: Whereas the cost of a physical computer is one-time, cloud computing has ongoing costs. If one is going to be a serious user of cloud computing, then they will need to deeply understand the cost structure of their cloud computing services. For example, there are often substantial costs to upload and download data from the cloud, in addition to the costs of the resources themselves. Cloud users should also implement billing alarms, particularly to catch any cases where credentials are compromised. In one instance in my lab, criminals obtained our credentials (which were accidentally checked into Github) and spent more than $20,000 within about a day; this was subsequently refunded by AWS, but it caused substantial anxiety and extra work. Scalability: There will be many cases in which an analysis cannot be feasibly run on a single cloud instance in reasonable time (e.g., running fMRIprep on a large dataset). One can scale beyond single instances, but this requires a substantial amount of work, and is really only feasible if one has a serious cloud engineer involved. It is simply not a good use of a scientist’s time to figure out how to spin up and manage a larger cluster on a cloud service; I know this because I’ve done it, and those are many hours that I will never get back that could have been used to do something more productive (like play guitar, do yoga, or go for a nice long walk). One could of course spin up many individual instances and manually run jobs across them, but this requires a lot of human effort, and there are better solutions available, as I outline below. Verdict: For a relatively small lab with limited analysis needs and reasonably strong system administration skills or support, I think this is a good solution. Be very careful with your credentials!Server under a desk (SUAD)Another approach for many labs is a single powerful on-premise server shared by multiple researchers in the lab, usually located in some out-of-the-way location so that no one (hopefully) spills coffee on it or walks away with it. It will often have a commodity-grade disk array attached to it for storage.Pros:Flexibilty: As with the on-premise PC model, the administrator has full control.Cons:Basically all the same cons as the on-premise PC model, with the added con that it’s a single point of failure for the entire lab.Same scaling issues as cloud VMsAdministration: I know that there are numerous labs where either faculty or graduate students are responsible for server administration. This is a terrible idea! Mostly because it’s time they could better spend reading, writing, exercising, or simply having a fun conversation over coffee.Verdict: Don’t do it unless you or your grad students really enjoy spending your time diagnosing file system errors and tuning firewall rules.Cluster in a closet (CIIC)This is a common model for researchers who have outgrown the single-computer-per-researcher or SUAD model. It’s the model that we followed when I was a faculty member at UCLA, and that I initially planned to follow when I moved from UCLA to UT Austin in 2009. The CIIC model generally involves a rack-mounted system with some number of compute nodes and a disk array for storage. Usually shoved in a closet that is really too small to accommodate it.Pros:Scalability: CIIC generally allows for much better scalability. With current systems, one can pack more than 1000 compute cores alongside substantial storage within a single full-height rack. Another big difference that allows much greater scalability is the use of a scheduling (or queueing) system, which allows jobs to be submitted and then run as resources are available. Thus, one can submit many more jobs than the cluster can handle at any one time, and the scheduler will deal with this gracefully. It also prevents problems that happen often under the SUAD model when multiple users log in and start jobs on the server and overrun its resources. Flexibility: One can configure one’s cluster however they want, because they will have administrative control over the system.Cons: Administration:Administering a cluster well is a complex job that needs a professional system administrator, not a scientist moonlighting as an sysadmin; again, I know this because I lived it. In particular, as a cluster gets bigger, the temptation for criminals to compromise it grows as well, and only a professional sysadmin is going to be able to keep up with cybercriminals who break into systems for a living.Infrastructure: Even a reasonably sized cluster requires substantial infrastructure that is unlikely to be met by a random closet in the lab. The first is power: A substantial cluster will likely need a dedicated power line to supply it. The second is cooling: Computers generate lots of heat, to a degree that most regular rooms will not be able to handle. On more than one occasion we had to shut down the cluster at UCLA because of overheating, and this can also impact the life of the computer’s components. The third is fire suppression: If a fire starts in the closet, you don’t want regular sprinklers dumping a bunch of water on your precious cluster. It is for all of these reasons that many campuses are no longer allowing clusters in campus buildings, instead moving them to custom-built data centers that can address all of these needs.Cost: The cost of purchasing and running a cluster can be high. Commercial-level hardware is expensive, and when things break you have to find money to replace them, because your team and colleagues will have come to rely on them. Training: Once you move to a cluster with more than a single node, you will need to use a scheduler to submit and run jobs. This requires a change in mindset about how to do computing, and some researchers find it annoying at first. It definitely requires letting go of a certain level of control, which is aversive for many people. Interactivity: It can be more challenging to do interactive work on a remote cluster than on a local workstation, particularly if it is highly graphics-intensive work. One usually interacts with these systems using a remote window system (like VNC), and these often don’t perform very well.Verdict: Unless you have the resources and a good sysadmin, I’d shy way from running your own cluster. If you are going to do so, locate it in a campus data center rather than in a closet.High-performance computing centersWhen I moved from UCLA to UT Austin in 2009, I had initially planned to set up my own CIIC. However, once I arrived I realized that I had another alternative, which was to instead take advantage of the resources at the Texas Advanced Computing Center, which is the local high-performance computing (HPC) center (that also happens to be world-class). My lab did all of its fMRI analyses using the TACC systems, and I have never looked back. Since moving to Stanford, we now also take advantage of the cluster at the Stanford Research Computing Facility, while also continuing to use the TACC resources as well.Pros: Scalability: Depending on the resources available at one’s HPC center, one can often scale well beyond the resources of any individual lab. For example, on the Frontera cluster at TACC (its newest, currently the 5th most powerful supercomputer on Earth), a user can request up to 512 nodes (28,672 cores) for up to 48 hrs. That’s a lot of Freesurfer runs. The use of scheduling systems also makes the management of large jobs much easier. These centers also usually make large-scale storage available for a reasonable cost. Professional management: HPC centers employ professional system administrators whose expertise lies in making these systems work well and fixing them when they break. And the best part is that you generally don’t have to pay their salary! (At least not directly). Cons: Training: The efficient usage of HPC resources requires that researchers learn a new model for computing, and a new set of tools required for job submission and management. For individuals with solid UNIX skills this is rarely a problem, but for researchers without those skills it can be a substantial lift. Control: Individual users will not have administrative control (“root”) on HPC systems, which limits the kinds of changes one can make to the system. Conversely, the administrators may decide to make changes that impact one’s research (e.g. software upgrades). Sharing: Using HPC systems requires good citizenship, since the system is being shared by many users. Most importantly: Users must never run jobs on the login node, as tempting as that might sometimes be. Waiting: Sometimes the queues will become filled up and one may have to wait a day for one’s jobs to run (especially just before the annual Supercomputing conference). Access: If one’s institution has an HPC center, then one may have access to those resources. However, not all such centers are built alike. I’ve been lucky to work with centers at Texas and Stanford that really want researchers to succeed. However, I have heard horror stories at other institutions, particularly regarding HPC administrators who see users as an annoyance rather than as customers, or who have a very inflexible approach to system usage that doesn’t accomodate user needs. For researchers without local HPC access, there may be national resources that one can gain access to, such as the XSEDE network in the US. Verdict: For a lab like mine with significant computing needs, I think that HPC is the only way to go, assuming that one has access to a good HPC center. Once you live through the growing pains, it will free you up to do much larger things and stop worrying about your cluster overheating because an intruder is using it to mine Bitcoin.These are of course just my opinions, and I’m sure others will disagree. Please leave your thoughts in the comment section below!"
  },
  {
    "objectID": "posts/computing-models-for-neuroimaging-lab/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/computing-models-for-neuroimaging-lab/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Computing models for a neuroimaging lab",
    "section": "3 comments captured from original post on Blogger",
    "text": "3 comments captured from original post on Blogger\nv said on 2019-12-12\nDon’t forget to mention research software engineers and/or web developers!\nRuss Poldrack said on 2019-12-12\nThey are necessary regardless of what model one chooses!\nJoke said on 2019-12-17\nUsing HPC also teaches valuable skills for a future outside of academia !"
  },
  {
    "objectID": "posts/a-quick-and-dirty-workflow-for-creating/index.html",
    "href": "posts/a-quick-and-dirty-workflow-for-creating/index.html",
    "title": "A quick and dirty workflow for creating lecture videos",
    "section": "",
    "text": "This was originally posted on blogger here.\nI’m currently in the midst of fully reworking my undergraduate statistics course for online learning, which includes creating about 40 short (5-10 minute) mini-lectures for the students to view asynchronously. As much as I would love to put many hours into generating high-touch videos, my time is limited so I needed a workflow that would allow me to generate these videos with as little overhead as possible. Here is what I came up with.Platform: I’m using a Macbook Pro as part of the setup described in my previous post on my home office setup. Software: I use Keynote to create the slides, Zoom to record the presentation, and QuickTime Player for cleaning up the video.Slide Prep: First, it’s important to make sure that your slides don’t use any builds, because (at least for Keynote) the Zoom “Slides as virtual background” feature doesn’t support builds. So just separate your builds out into separate slides. Second, because your head will appear in the bottom right of the screen, you should make sure that there is no essential material that appears in that location. In the worst case, you can always just move your head out of the way, which I imagine is someone amusing for the viewer.Recording workflow:1. Start a Zoom meeting, and start Screen Sharing. Under the Advanced tab, choose “Slides as virtual background”, share the screen and the choose your presentation file. The slides will load, with a small image of your head in the bottom right. One exception to this workflow is if you need to present video as part of your presentation, which doesn’t work with the “Slides as virtual background” option. In this case you’ll need to use a regular screen share, which will lose the talking head in the corner.2. Start recording, being sure to select “Record on this computer”.3. After you start recording, give yourself a few seconds to settle and get any fidgets out of the way. Then start talking. I try to give the entire lecture without stopping, realizing that I will probably make a few mistakes, and that’s ok. Occasionally I find myself totally flummoxed part way through, or realizing that I need to make a big change, in which case I simply quit and start over. Since each of the videos is relatively short, I don’t lose that much time if I have to bail partway through.One tip that I still find somewhat difficult to follow: Try to finish your comments about a particular slide before you flip to the next slide. I find that I have a habit of flipping forward to the next slide as I am finishing my comments about a slide. This is usually fine for talk, but for these lectures I am using Panopto within Canvas to embed quiz questions in the video, which I usually want to place at a transition between slides. However, if I am still talking about the previous slide after I have transitioned to the next slide, the quiz placement becomes awkward.4. When you get to the end of the lecture, give yourself a few seconds of stillness on the last slide or on a blank slide inserted after the last slide.5. End the Zoom session using the End button (no need to stop sharing). This will cause Zoom to save the video to a file, which will pop up in the Finder once it’s done. Post-processing workflow:1. Open the mp4 file from the Zoom recording folder in QuickTime Player. 2. Find the point where you want to start the video, just before you start talking. With the player paused at that location, choose “Split Clip” from the Edit menu. Click on the leftmost section in the timeline, and press Delete to remove that leading section, then click Done to save the change. Now do the same for the end of the video, finding the point where you want to end and removing the trailing section.There is a “Trim clip” feature that one can use to do this in a single step rather than two, but I find that it’s easier to be precise about where the trimming happens using the Split Clip method.3. Close and save the video to a new .mp4 file.I find that this method takes me only a minute or so to post-process each video once it’s recorded. Of course, you could do much fancier stuff if you wanted; in that case I would check out DaVinci Resolve, which is one of the most amazing pieces of free software ever created but has a pretty steep learning curve for serious video editingUploading the video:If you are using Canvas and your instance supports Panopto, then I would recommend using that method to upload the videos, since it provides viewing statistics (e.g. for recording which students have watched the video) and also allows embedding quiz questions within the video. As always, suggestions are welcome in the comments below!"
  },
  {
    "objectID": "posts/my-productivity-toolbox/index.html",
    "href": "posts/my-productivity-toolbox/index.html",
    "title": "My Productivity Toolbox",
    "section": "",
    "text": "This was originally posted on blogger here.\nToday I’m going to say a bit about some of the tools and strategies that I use for productivity. I’ve been a fan of GTD since Jen introduced me to David Allen’s book several years ago, though I use a fairly lightweight version of it. When an email comes, I try to process it as quickly as possible, either responding to it or putting it into my to-do list (more on that below). Although I don’t always live up to it (especially when I’m traveling), I am a strong proponent of the Inbox Zero philosophy. When I see people who have thousands of emails in their inbox it makes me cringe (and it usually correlates with an inability to get things done on schedule!).Platform: My desktop platform is the 15” MacBook Pro, which I carry between home and my two offices (where I have external monitors and extra power adapters). It generally has enough juice to do anything I need, and it’s nice to have everything in one place. I previously kept a desktop machine at work and laptop for the road, but I find it more straightforward to keep everything on a single machine.Calendar: I use the mac calendar app + mobile me, which keeps everything synchronized across my computers, iPhone, and iPad. It’s well worth the yearly fee in my opinion; there are probably ways that one could achieve similar synchronization using free means, but mobile me just works so I’m happy to pay for it.To-do/GTD: I use OmniFocus to manage my to-do lists and tasks. I really don’t even scratch the surface of its features, but it still works very well for me. What I particularly like is the ability to trigger new to-do items from Mail messages, which are then directly accessible within OmniFocus (including attachments). This makes life SO much easier for me that I can’t imagine living without it.Mail: I use the Mac Mail.app as my primary mail program, using IMAP to access gmail as my primary mail account; mail to all of my other addresses is forwarded to my gmail account. My main reason for using Mail.app is its nice integration with OmniFocus, as well as the ability to work offline (I like to catch up on emails in flight). Backup: I don’t back up my machine per se. However, all of my important documents are in a folder that is attached to a 100GB Dropbox account, which means that they are automatically synchronized across all of my machines, as well as being accessible from the web. Again, there are probably free alternatives, but I doubt any of them can offer the really solid usability of Dropbox. My only compaint is that if I add large numbers of files (e.g., digital photos), my machine gets fairly overloaded while they are being uploaded to the Dropbox server. Document sharing: Here too, Dropbox is the killer app. We have a lab dropbox that is separate from my personal dropbox, which serves as a central location for files that I can share with lab members. We recently wrote a grant using Dropbox, with authors strewn between Texas and California, and it was amazing how well it worked.PDF management: I have been using Papers for a while now, though I still am not as religious about it as some others, and I don’t actually use it for bibliographies (more on my writing workflow another time). I do find its ability to obtain papers through the UT proxy server very useful when I am off campus. Note Taking: I use Evernote for note taking. Its greatest feature for me is the seamless integration and synchronization across my Mac, iPhone, and iPad. As with most of the other software mentioned above, I am not exactly a power user; I just use the simple features that meet my basic needs.Another tool that I have found very handy is my Fujitsu ScanSnapdesktop document scanner, which allows me to scan in a document and attach it to an email with 1 button press and two mouse clicks. Very handy for those pesky travel reimbursement forms that always require written signatures.I’d love to hear your suggestions and comments."
  },
  {
    "objectID": "posts/productivity-stack-for-2019/index.html",
    "href": "posts/productivity-stack-for-2019/index.html",
    "title": "Productivity stack for 2019",
    "section": "",
    "text": "This was originally posted on blogger here.\nApparently some people seem to think my level of productivity is simply not humanly possible: For the record, there is no cloning lab in my basement. I attribute my productivity largely to a combination of mild obsessive/compulsive tendencies and a solid set of tools that help me keep from feeling overwhelmed when the to-do list gets too long. I can’t tell you how to become optimally obsessive, but I can tell you about my productivity stack, which I hope will be helpful for some of you who are feeling increasingly overwhelmed as you gain responsibilities. Platform: MacBook Pro 13” + Mac OS X I have flirted with leaving the Mac as the OS has gotten increasingly annoying and the hardware increasingly crappy, but my month-long trial period with a Windows machine left me running back to the Mac (mostly because the trackpad behavior on the Dell XPS13 was so bad). Despite the terrible keyboard (I’ve had two of them replaced so far) and the lack of a physical escape key, the 13” Macbook Pro is a very good machine for working on the road - it’s really light and the battery life is good enough that I rarely have to plug in, even on a long flight from SFO to the east coast. In the old days I would invert my screen colors to reduce power usage, but now I just use Dark Mode in the latest Mac OSX. I keep a hot spare laptop (a previous-generation Macbook Pro) synced to all of my file sharing platforms (Dropbox, Box, and Google Drive) in case my primary machine were to die suddenly. Which has happened. And will happen again. If you can afford to have a second laptop I would strongly suggest keeping a hot spare in the wings. I don’t have a separate desktop system in my office - when I’m there I just plug into a larger monitor and work from my laptop. In the past I had separate desktop and laptop systems but just found it too easy for things to get desynchronized. Pro Tip: About once a month I run the Onyx maintenance scripts, run the DiskUtility file system repair, and clone my entire system to a lightweight 1TB external drive (encrypted, of course) using CarbonCopyCloner. Having a full disk backup in my backpack has saved me on a few occasions when things went wrong while traveling. Mobile: Pixel 2 + Google Fi I left the iPhone more than a year ago now and have not looked back. The Pixel 2 is great and Google Fi wireless service is awesome, particularly if you travel a lot internationally, since data costs the same almost everywhere on earth. If you want to sign up, use my referral link and you’ll get a $20 credit (full disclosure - I will get a $100 credit).Email: Gmail For many years I used the Mac Mail.app client, but as it became increasingly crappy I finally gave up and moved to the GMail web client, which has been great. The segregation of promotion and social emails, and new features like nudges, make it a really awesome mail client. My email workflow is a lazy adaptation of the GTD system: I try not to leave anything in my inbox for more than a day or so (unless I’m traveling). I either act on it immediately, decide to ignore/delete it, or put it straight into my todo list (and archive the message so it’s no longer in my inbox). I’m rarely at inbox zero, but I usually manage to keep it at 25 or fewer messages, so I can see it all in a single screen. To do list: Todoist I moved to Todoist a couple of years ago and have been very happy with it. It’s as simple as it needs to be, and no simpler. The integration with GMail is particularly nice. Calendar: Google CalendarThe integration between my Android device and Gmail across platforms makes this a no-brainer.Notes: Evernote Evernote is my go-to for note-taking during meetings, talks, and whenever I just want to write something down. Lab messaging: Slack I really don’t love Slack (because I feel that it’s too easy for things to get lost when a channel is busy), but it has become our lab’s main platform for messaging. We’ve tried alternatives but they have never really stuck.Safe Surfing: Private Internet Access VPN + UBlock Origin/Privacy Badger Whenever I’m on a public network I stay safe by using the Private Internet Access VPN, which works really well across every platform I have tested it. (and you can pay for it with Bitcoin!)When surfing in Chrome I use UBlock Origin and Privacy Badgerextensions to prevent trackers. Writing: Google Docs/TexShop For collaborative writing we generally stick to Google Docs, which just works. Paperpile is a very effective reference management system. For my own longer projects (like books) I write in LaTeX using TexShop, with BibDesk for bibliography management, via the MacTex distribution. If I were writing a dissertation today I would definitely use LaTeX, as I have seen too many students scramble as Microsoft Word screwed up their huge dissertation file. Some folks in the lab use Overleaf, which I like, but I also do a lot of writing while offline so a web-based solution isn’t optimal for me. Presentations: Keynote I have tried at various points to leave Keynote, but always came crawling back. It’s just so easy to create great-looking presentations, and as cool as it would be to build them in LaTeX, I would have nightmares involving the inability to compile my presentation 3 minutes before my talk. Art: Affinity Designer I gave up on Adobe products when they moved to a subscription model. For vector art, I really like Affinity Designer, though it does have a pretty substantial learning curve. I’ve tried various freeware alternatives but none of them work very well.Coding in R: Rstudio If you’ve read my statistics book you know that I have a love/hate relationship with R, and most of the love comes from RStudio, which is an excellent IDE. Except for code meant to run on the supercomputer, I write nearly all of my R code in RMarkdown Notebooks, which are the best solution for literate programming that I have seen. Coding in Python: Anaconda + Jupyter Lab/Atom Python is my language of choice for most coding problems, and Anaconda has pretty much everything I need for scientific Python. For interactive coding (e.g. for teaching or exploration) I use Jupyter Lab, which has matured nicely. For non-interactive coding (e.g. for code that will run on the supercomputer) I generally use Atom which is nice and simple but gets the job done.Hopefully these tips are helpful - now back to getting some real work done!"
  },
  {
    "objectID": "posts/productivity-stack-for-2019/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/productivity-stack-for-2019/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Productivity stack for 2019",
    "section": "4 comments captured from original post on Blogger",
    "text": "4 comments captured from original post on Blogger\nDoctorMagnet said on 2018-12-03\nI actually have considerable overlap with your schema, but have not been able to give up the clone lab yet. There’s no way I could match your productivity without it.\nUnknown said on 2018-12-03\nAdvice on productivity stacking one’s brain? ;)\nUnknown said on 2018-12-03\nAhhh, the mistake when abandoning the Mac was going to Windows. Try a Linux distribution like Ubuntu.\n∂å˜ˆ´¬ said on 2018-12-04\nThis is great - wish more people would lay out there cohesive \"productivity stacks\". As a new grad student I really felt like I was trying to reinvent the wheel and lost a lot of time adopting and discarding inefficient workflows. However, what I would also be really interested to know is how you organize your daily/weekly/monthly/yearly schedule? How do you divide your work/teaching/personal life? And what tips might you have for graduate students in that regard? Thanks for the blog!"
  },
  {
    "objectID": "posts/are-good-science-and-great-storytelling/index.html",
    "href": "posts/are-good-science-and-great-storytelling/index.html",
    "title": "Are good science and great storytelling compatible?",
    "section": "",
    "text": "This was originally posted on blogger here.\nChris Chambers has a piece in the Guardian (“Are we finally getting serious about fixing science?”) discussing a recent report about reproducibility from the UK Academy of Medical Sciences, based on a meeting held earlier this year in London. A main theme of the piece is that scientists need to focus more on going good science and less on “storytelling”:Some time in 1999, as a 22 year-old fresh into an Australian PhD programme, I had my first academic paper rejected. “The results are only moderately interesting”, chided an anonymous reviewer. “The methods are solid but the findings are not very important”, said another. “We can only publish the most novel studies”, declared the editor as he frogmarched me and my boring paper to the door.I immediately asked my supervisor where I’d gone wrong. Experiment conducted carefully? Tick. No major flaws? Tick. Filled a gap in the specialist literature? Tick. Surely it should be published even if the results were a bit dull? His answer taught me a lesson that is (sadly) important for all life scientists. “You have to build a narrative out of your results”, he said. “You’ve got to give them a story”. It was a bombshell. “But the results are the results!” I shouted over my coffee. “Shouldn’t we just let the data tell their own story?” A patient smile. “That’s just not how science works, Chris.”He was right, of course, but perhaps it’s the way science should work. None of us in the reproducibility community would dispute that the overselling of results in service of high-profile publications is problematic, and I doubt that Chambers really believes that our papers should just be data dumps presented without context or explanation. But by likening the creation of a compelling narrative about one’s results to “selling cheap cars”, this piece goes too far. Great science is not just about generating reproducible results and “letting the data tell their own story”; it should also give us deeper insights into how the world works, and those insights are fundamentally built around and expressed through narratives, because humans are story-telling animals. We have all had the experience of sitting through a research talk that involved lots of data and no story, and it’s a painful experience; this speaks to the importance of solid narrative in our communication of scientific ideas.Narrative becomes even more important when we think about conveying our science to the public. Non-scientists are not in a position to “let the data speak to them” because most of them don’t speak the language of data; instead, they speak the language of human narrative. It is only by abstracting away from the data to come up with narratives such as “memory is not like a videotape recorder” or “self-control relies on the prefrontal cortex” that we can bring science to the public in a way that can actually have impact on behavior and policy.I think it would be useful to stop conflating scientific storytelling with “embellishing and cherry-picking”. Great storytelling (be it spoken or written) is just as important to the scientific enterprise as great methods, and we shouldn’t let our zeal for the latter eclipse the importance of the former."
  },
  {
    "objectID": "posts/are-good-science-and-great-storytelling/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/are-good-science-and-great-storytelling/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Are good science and great storytelling compatible?",
    "section": "7 comments captured from original post on Blogger",
    "text": "7 comments captured from original post on Blogger\nUnknown said on 2015-11-02\nI think there is a fine line between ‘selling’ and ‘telling’. To discern this line requires expertise, time and as few conflicts of interest as possible. If one of these is lacking, the task quickly becomes too difficult. At the top journals who decide careers, it often enough seems like none of them are present as editors too few pick time-constrained big-shots who stand to benefit from high-profile papers in their field.So, while hopefully most of the time science is being told, in more and more cases, science is sold. Since such publications make careers, these marketers teach their students about how to get a job in science - by selling it.Given the time-constraints we’re all in, telling the difference between a good and a bad story becomes easy and that between good and bad methodology difficult - so we all too often give the story-teller a free pass, while the less eloquent colleague faces an uphill battle. Over time and with enough such decisions, we will have weeded out all bad marketers - only hoping they might also be good scientists.\nUnknown said on 2015-11-02\nThat previous comment was by me, Björn Brembs\nRuss Poldrack said on 2015-11-02\nThanks for your comment Björn. My main goal here was to push back against the implication that I saw in Chris’s piece that good storytelling is necessarily associated with bad science. I completely agree that good storytelling can sometimes obscure bad science lurking behind the story.\nRodolphe said on 2015-11-02\nGreat post. I totally agree that Scientists should not be novelists. A paper should be easy to understand. Although, correct me if i’m wrong, but you’re not talking about the root of the problem. Why do writers try to embellish the story even when their methods and results are solid but, like you said, a little bit dull? Why do people, in extreme cases do p-hacking? (Which is a form of embellishing a paper). None of this would happen if journals were accepting more negative/replication results instead of going for sensational positive result.\nRuss Poldrack said on 2015-11-02\nThanks Rodolphe - I am not sure that there is a single root of this problem. I agree that there are problems that arise due to the perceived need for positive results, but I am trying to make a different point: As scientists, our ultimate goal is to come up with effective stories about how the world works, which must necessarily abstract away from the details of the data. If we are not testing and ultimately telling stories then we are not doing science, we are just collecting data. The stories need to reflect the data accurately, both where the data fit the story and where they do not. To me, this is the essence of good scientific expression.\nsatra said on 2015-11-03\n@russ: much as investigative reporters stitch together scant pieces of information to produce a news article, scientists stitch together pieces of information to create a narrative around their results. as such it is colored through the lens of the author (and perhaps some small set of reviewers). scientific storytelling relies on expert knowledge of domain and access to/awareness of information, and as such it carries with it many biases. further, the human brain that comprehends these stories is not capable of thinking beyond a few dimensions. therefore, by reducing the mechanistic complexity of how the world works through a narrow low dimensional projection is only satisfying our inherent need for simplicity, but abstracts away from reality. i don’t believe that everything reduces to E=mc2 (and even that equation has a more complex form). fundamentally what needs to be asked is which pillars of our knowledge foundation are shaky, and by reducing to storytelling we communicate the excitement but perhaps not our limitations in understanding. coming back to the original article, i know that you agree that only by making the many pieces of data, whether significant or not, available, do you provide access to the richness of the story and complexity of the characters.\nDick_Retired said on 2016-07-24\nI think that ‘telling a story’ is important. Before the trolls are at me, I don’t mean ‘overhype the data’; I mean ‘explain what the data mean’.A major advance in animal lesion work was the ‘cross-lesion technique’. It enables us to study interactions between areas, not just the function of particular areas. Both my thesis supervisor, George Ettlinger, and Mort Mishkin at NIH claimed to have been the first to devise the technique. Mort assures me that it was him, and I believe him. But that is not the point.The point is that George, who published the first full paper using the method (in Brain in 1959), didn’t know how to write. The paper is bald, dull, full of tables, with no diagrams and no proper explanation of why the method is helpful and why the results mattered.Mort wrote up a paper much later in which he produced diagrams of the various stages, showed the data in histogram form and really explained why the method was so powerful.‘Telling a story?’ Yes, but not overhyping things. Just taking the trouble to think how best to help readers to understand why the method and results are significant."
  },
  {
    "objectID": "posts/reproducible-analysis-in-myconnectome/index.html",
    "href": "posts/reproducible-analysis-in-myconnectome/index.html",
    "title": "Reproducible analysis in the MyConnectome project",
    "section": "",
    "text": "This was originally posted on blogger here.\nToday our paper describing the MyConnectome project was published in Nature Communications. This paper is unlike any that I have ever worked on before (and probably ever will again), as it reflects analyses of data collected on myself over the course of 18 months from 2012-2014. A lot has been said already about what the results might or might not mean. What I want to discuss here is the journey that ultimately led me to develop a reproducible shared analysis platform for the study.Data collection was completed in April 2014, shortly before I moved to the Bay Area, and much of that summer was spent analyzing the data. As I got deeper into the analyses, it became clear that we needed a way to efficiently and automatically reproduce the entire set of analyses. For example, there were a couple of times during the data analysis process when my colleagues at Wash U updated their preprocessing strategy, which meant that I had to rerun all of the statistical analyses that relied upon those preprocessed data. This ultimately led me to develop a python package (https://github.com/poldrack/myconnectome) that implements all of the statistical analyses (which use a mixture of python, R, and cough MATLAB) and provides a set of wrapper scripts to run them. This package made it fairly easy for me to rerun the entire set of statistical analyses on my machine by executing a single script, and provided me with confidence that I could reproduce any of the results that went into the paper. The next question was: Can anyone else (including myself at some later date) reproduce the results? I had performed the analyses on my Mac laptop using a fairly complex software stack involving many different R and python packages, using a fairly complex set of imaging, genomic, metabolomic, and behavioral data. (The imaging and -omics data had been preprocessed on large clusters at the Texas Advanced Computing Center (TACC) and Washington University; I didn’t attempt to generalize this part of the workflow). I started by trying to replicate the analyses on a Linux system; identifying all of the necessary dependencies was an exercise in patience, as the workflow would break at increasingly later points in the process. Once I had the workflow running, the first analyses showed very different results between the platforms; after the panic subsided (fortunately this happened before the paper was submitted!), I tracked the problem down to the R forecast package on Linux versus Mac (code to replicate issue available here). It turned out that the auto.arima() function (which is the workhorse of our time series analyses) returned substantially different results on Linux and Mac platforms if the Y variable was not scaled (due apparently to a bug on the Linux side), but very close results when the Y variable was scaled. Fortunately, the latest version of the forecast package (6.2) gives identical results across Linux and Mac regardless of scaling, but the experience showed just how fragile our results can be when we rely upon complex black-box analysis software, and how we shouldn’t take cross-platform reproducibility for granted (see here for more on this issue in the context of MRI analysis).Having generalized the analyses to a second platform, the next logical step was to generalize it to any machine. After discussing the options with a number of people in the open science community, the two most popular candidates were provisioning of a virtual machine (VM) using Vagrant, or creating a Docker container. I ultimately chose to go with the Vagrant solution, primarily because it was substantially easier; in principle you simply set up a Vagrantfile that describes all of the dependencies, and type “vagrant up”. Of course, this “easy” solution took many hours to actually implement successfully because it required reconstruction of all of the dependencies that I had taken for granted on the other systems, but once it was done we had a system that allows anyone to recreate the full set of statistical analyses exactly on their system, which is available at [https://github.com/poldrack/myconnectome-vm](https://github.com/poldrack/myconnectome-vm)https://github.com/poldrack/myconnectome-vm. A final step was to provide a straightforward way for people to view the complex set of results. Our visualization guru, Vanessa Sochat, developed a flask application (https://github.com/vsoch/myconnectome-explore) that provides a front end to all of the HTML reports generated by the various analyses, as well as a results browser that allows one to browse the 38,363 statistical tests that were computed for project. This browser is available locally if one installs and runs the VM, and is also accessible publicly from http://results.myconnectome.orgDashboard for analysesBrowser for timeseries analysis resultsWe have released code and data with papers in the past, but this is the first paper I have ever published that attempts to include a fully reproducible snapshot of the statistical analyses. I learned a number of lessons in the process of doing this:The development of a reproducible workflow saved me from publishing a paper with demonstrably irreproducible results, due to the OS-specific software bug mentioned above. This in itself makes the entire process worthwhile from my standpoint.Converting a standard workflow to a fully reproducible workflow is difficult. It took many hours of work beyond the standard analyses in order to develop a working VM with all of the analyses automatically run; that doesn’t even count the time that went into developing the browser. Had I started the work within a virtual machine from the beginning, it would have been much easier, but still would require extra work beyond that needed for the basic analyses.Ensuring longevity of a working pipeline is even harder. The week before the paper was set to published I tried a fresh install of the VM to make sure it was still working. It wasn’t. The problem was simple (miniconda had changed the name of its installation directory), and highlighted a significant flaw in our strategy, which was that we had not specified software versions in our VM provisioning. I hope that we can add that in the future, but for now, we have to keep our eyes out for the disruptive effects of software updates.I look forward to your comments and suggestions about how to better implement reproducible workflows in the future, as this is one of the major interests of our Center for Reproducible Neuroscience."
  },
  {
    "objectID": "posts/my-personal-computing-history/index.html",
    "href": "posts/my-personal-computing-history/index.html",
    "title": "My personal computing history",
    "section": "",
    "text": "This was originally posted on blogger here.\nMy generation may be the last to ever have known a world without ubiquitous computing. It’s now almost impossible to imagine a computer being a thing that sits in the corner at the school library and gets used only by a few geeky kids, but in my high school that’s exactly what it was. I’ve recently been trying to reconstruct the history of different computers that I played with and owned back in those days; here is my first pass at a personal computing history, helped in large part by the awesome amount of computer history information that’s available online.My first exposure to computers came during 1981-1983 when I was in high school, though several different paths (the order of which is slightly hazy me at this point). The first system that I remember programming on was the TI 99/4A, which was a “home computer” released in 1981 (we probably got it around 1982-1983). It came with relatively little software, but I do remember playing a number of games including “TI Invaders” which was their clone of the Space Invaders game. The programs were released on cartridges, more like a video game system than a personal computer. The machine came installed with “TI BASIC” which was a pretty rudimentary version of BASIC. We didn’t have a hard drive, but it was possible to save programs to cassette tape. I don’t remember much about what kind of programming I did on this system, though I’m pretty sure it was all text-based. Northstar AdvantageAround the same time, my father purchased a computer for his accounting office, back when this was just becoming common. It was a Northstar Advantage, which was resold with a GBC logo. This machine was released in 1982, right after the release of the first IBM PC. It ran an operating system called CP/M, which was similar to but predated Microsoft’s DOS. The apps that I remember using included WordStar (word processing) and dBASE (a database program). I don’t think I actually programmed on this system, but it was my first experience with command line interfaces (PIP, anyone?). Apple IIeIn our high school there was an Apple IIe in the library that was almost always free. I was ostensibly on the tennis team when I was a senior in high school, but given that I wasn’t very good at tennis, the coach was happy to let me go to the library and play with the computer instead. This system had a great advantage over the TI at home, which was that I could save programs to 5 ¼” floppy disk. This is the first machine that I remember doing any graphics programming on, though I’m sure that it was incredibly rudimentary. In the summers of 1984 and 1985, I worked at a local bank helping them to automate some of their records. The bank had just recently purchased a new IBM PC/AT which was the second generation of IBM PC’s. It had such amazing features as 16 MB of RAM and a 20 MB hard drive. I primarily used Lotus 1-2-3(an early spreadsheet program). However, in the moments between real work, I would also take advantage of the machine’s modem to dial into a number of BBS sites. It’s hard for me to remember exactly what I found on these sites or how I even found out about them, but it was basically like a very earlier version of the web that one could call into for free. I think I mostly used them to download software. Macintosh SE/30During most of my college career I did not have my own computer; very few students did. If we needed one, we would go to the computer lab on campus, which is where I was introduced to Macintosh (Plus or SE) computers. In my senior year of college the Mac SE/30 was released, and my parents were nice enough to plop down several thousand dollars to buy me one before I headed off to graduate school, along with an ImageWriter II dot-matrix printer. This machine was my workhorse throughout graduate school. I addition to writing papers, I was my platform for learning other languages including PASCAL and C. It was the system on which I wrote my first simulations, which would be published here (though the actual simulations for that paper were run on a Sun UNIX system at the Beckman Institute because it was so much faster than my SE/30). PowerTower ProWhen I arrived at Stanford for my postdoc in 1995, it was definitely time to upgrade. I was lucky enough to be in the market during the short period in which Mac clones were available, and I bought a Power Computing Power Tower Pro, which was one of my favorite computers ever. It was this machine on which I first started playing serious with Linux (using mkLinux IIRC). The PowerTower Pro is the last machine that I’ve owned that I think could be legitimately called “historical”, so that’s where my story ends for now."
  },
  {
    "objectID": "posts/my-personal-computing-history/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/my-personal-computing-history/index.html#comments-captured-from-original-post-on-blogger",
    "title": "My personal computing history",
    "section": "3 comments captured from original post on Blogger",
    "text": "3 comments captured from original post on Blogger\nMexHistorian said on 2010-11-21\nI remember the SE/30–I came over and spent the day at the Nutty Pecan putting my history paper together. (I got an A on it with the comment \"Pretty good\" from the prof.) Good times.\nUnknown said on 2010-11-21\nIn 1993 you asked me if you should buy a computer or go to Europe. I told you computers were like sex – you shouldn’t have to pay for them. You came to Europe…\nRuss Poldrack said on 2010-11-30\nThat was great advice Gordon!"
  },
  {
    "objectID": "posts/is-reverse-inference-fallacy-comment-on/index.html",
    "href": "posts/is-reverse-inference-fallacy-comment-on/index.html",
    "title": "Is reverse inference a fallacy? A comment on Hutzler",
    "section": "",
    "text": "This was originally posted on blogger here.\nA new paper by Florian Hutzler has been published online at Neuroimage which claims to show that reverse inference is not as problematic as has been claimed in my previous publications (TICS, 2006; Neuron, 2010). I had previously reviewed this paper for another journal (I signed my review so this is not a surprise), and I’m happy to see that some of my concerns about the paper were addressed in the version that was published at Neuroimage. However, I still have one major concern about the general framing of the paper.I would first like to be clear about what I said about reverse inference in my 2006 paper:“It is crucial to note that this kind of ‘reverse inference’ is not deductively valid, but rather reﬂects the logical fallacy of afﬁrming the consequent…However, cognitive neuroscience is generally interested in a mechanistic understanding of the neural processes that support cognition rather than the formulation of deductive laws. To this end, reverse inference might be useful in the discovery of interesting new facts about the underlying mechanisms. Indeed, philosophers have argued that this kind of reasoning (termed ‘abductive inference’ by Pierce [8]), is an essential tool for scientiﬁc discovery [9].”Thus, while I did point out the degree to which reverse inference reflects a fallacy under deductive logic, I also pointed out that it could be potentially useful under other forms of reasoning; it’s a bit of a stretch to go from this statement to using the term “reverse inference fallacy” which has started to pervade peer reviews. This is unfortunate in my view, if only because authors must often think that I am the culprit! (I assure you all that I would never use this phrase in a review.) The potential utility of reverse inference has been further cashed out in the Neurosynth project (Yarkoni et al, 2011). I say all of this just to highlight the fact that I have never painted reverse inference as wholly fallacious, but rather have tried to highlight ways in which its limited utility can be quantified (e.g. through meta-analysis) or its power improved (e.g., through the use of machine learning methods).The Hutzler paper applies reverse inference in a much more restrictive sense than it has usually been discussed, which he calls “task-specific functional specificity.” The idea is that given some task, one can compute (e.g., using meta-analysis) the reverse inference conditional on that task (which I had noted but not further explored in my 2006 paper). I have no quibbles with the paper’s analysis, and I think it nicely shows how reverse inference can be useful within a limited domain (in fact, Anthony Wagner and I made this pointin 2004 in regard to left prefrontal function). My general concern is that the situation described in the Hutzler paper is fairly different from the one in which most reverse inference is performed. Here is what I said in my initial review of his paper, which still holds for the published version:If it is true that reverse inference is helpful within the context of a specific task, then that’s perfectly fine, except that in the wild reverse inference is rarely used within the same task. In fact, it’s almost always used in task domains where one doesn’t know what to expect! See my recent Neuron paper for examples of these kinds of reverse inferences; rarely does one see a reverse inference based on prior data from very similar tasks. Thus, the paper basically makes my point for me by showing that the procedure is only effective in very specific cases which are outside of the standard way it is used.In summary, while I agree with the analysis presented by Hutzler, I hope that readers will go beyond the title (which I think oversells the result) to see that it really shows the success of reverse inference in a very limited domain."
  },
  {
    "objectID": "posts/on-carbon-footprints-and-biased/index.html",
    "href": "posts/on-carbon-footprints-and-biased/index.html",
    "title": "on carbon footprints and biased judgments",
    "section": "",
    "text": "This was originally posted on blogger here.\nI recently bought a new SUV. I was in the market for a new car, and really liked the styling of the Toyota FJ Cruiser, not to mention its utility for carrying all the various stuff that I need to carry. When I announced my purchase on Facebook, it occasioned a number of humorous jabs from my friends, which I was sure that I deserved; after all, I had just bought a monstrous gas-guzzler (17 MPG in town, 21 on the highway)! These comments also inspired me to think more about how we judge the impact of our choices on the environment. Everyone knows that driving a fuel-efficient, low-emission vehicle is better for the environment than a gas-guzzler, because this issue is prominent in the news as well as car advertisements. However, a large body of psychological research has shown that our judgments about things like this are often biased. Rather than basing our judgments on statistics (as we should if we want them to be accurate), we often use rules of thumb, or “heuristics” as they are known in psychology. For example, when asked to judge how common something is, we often base our judgments on how readily an example of it comes to mind. This “availability heuristic” is the reason why people’s judgments about the prevalence of crime often don’t match crime statistics; even when crime is going down, news reports of criminal acts easily come to mind and bias us to think that crime is actually increasing. When it comes to the impacts of our behavior on the environment, the recent popularity of hybrid cars has made the environmental consequences of driving highly available in our mind, and thus we think of driving as a major component of our carbon footprint. However, some analysis using the various carbon footprint calculators on the web shows that we may be overestimating the relative benefit of buying a fuel-efficient car in comparison to other lifestyle choices that we make. According to the carbonfootprint.com calculator, my FJ Cruiser will produce about 3.25 tons of CO2 over a year of driving my average of 6000 miles. If I had purchased a Toyota Prius, I would produce 1.17 tons over the same year, a savings a just over 2 tons of CO2. That sounds like a big reduction in CO2 emissions, but how does it compare to some other choices that we might make? A round trip flight from LAX to JFK produces 2.41 tons of CO2 per passenger when the effects of water vapor are taken into account. Thus, a single airline flight obliterates the benefits of driving a hybrid for a year. Another choice that has a big environmental impact is eating meat. According to the Nature Conservancy’s calculator, eliminating meat from one’s diet reduces one’s yearly carbon footprint by 3.2 tons; again, more effective than buying a hybrid. The choice that has the largest environmental impact by far is having children. According to a study published last year, the lifetime carbon footprint of an American child is estimated to be more than 9,000 tons of CO2. Thus, the environmental impact of having just one child in the United States is equivalent to 1,450 people each driving a Cadillac Escalade 10,000 miles over one year. Another way to look at it is that the yearly CO2 savings due to all of the hybrid vehicles sold in the US in 2009 (290,272 according to HybridCars.com) is enough to offset the lifetime carbon impact of about 64 American children. How can we improve our ability to make accurate judgments in situations like this? First we need to be aware of our biases, but this is difficult: Research by Emily Pronin from Princeton University has shown that humans often have a “blind spot” regarding their own biases. Research subjects in one of her studies (who were randomly chosen passengers at the San Francisco airport) rated themselves as substantially less biased than the “average passenger.” Once we realize that we are all biased, we can start to try to overcome those biases. One way to overcome biases is to slow down; when we take the time to take in more evidence regarding a judgment or decision, we are more likely to rely upon information that is less biased than the “gut feeling” that our heuristics provide for us."
  },
  {
    "objectID": "posts/a-discussion-of-causal-inference-on/index.html",
    "href": "posts/a-discussion-of-causal-inference-on/index.html",
    "title": "A discussion of causal inference on fMRI data",
    "section": "",
    "text": "This was originally posted on blogger here.\nMy facebook page was recently home to an interesting discussion about causal inference that was spurred by a recent article that I retweeted:Gary Williams ‏@orestesmantra16 DecEffective connectivity or just plumbing? Granger Causality estimates highly reliable maps of venous drainage.  Retweeted by Russ PoldrackWhat follows is the discussion that ensued. I have moved it to my blog so that additional people can participate, and so that the discussion will be openly accessible (after removing a few less relevant comments). Please feel free to comment with additional thoughts about the discussion.Peter Bandettini Agree! Latencies due to veins 1 to 4 sec. Latencies due to neurons &lt; 300 ms. Vein latency variation over space dominates.December 16 at 8:15pm via mobile · Like · [Russ Poldrack](https://www.facebook.com/poldrack) it’s amazing how we have known for a good while now that Granger causality is useless for fMRI, yes people continue to use it[December 16 at 8:15pm](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39362615&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152197229548646)Russ Poldrack it’s amazing how we have known for a good while now that Granger causality is useless for fMRI, yes people continue to use itDecember 16 at 8:15pm · Like · [Peter Bandettini](https://www.facebook.com/peter.bandettini) There are ways to look through the plumbing.[December 16 at 8:15pm](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39362616&offset=0&total_comments=44) via [mobile](https://www.facebook.com/mobile/) · [Unlike](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152197229638646)Peter Bandettini There are ways to look through the plumbing.December 16 at 8:15pm via mobile · Unlike · [Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Tal Yarkoni My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.December 17 at 9:39am · Like · [Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Micah Allen In terms of hemodynamics, I’m not aware of any assumptions different from those in all canonical HRF models. DCM uses a slightly updated version of the balloon model. Further, unlike GCM the parameters estimated by DCM have been repeatedly validated using multimodal imaging. Although I will bring this up at the next Methods meeting.December 17 at 9:45am via mobile · Edited · Like[Russ Poldrack](https://www.facebook.com/poldrack) it’s amazing how we have known for a good while now that Granger causality is useless for fMRI, yes people continue to use it[December 16 at 8:15pm](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39362615&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152197229548646)Russ Poldrack Tal - I don’t agree regarding blanket skepticism about causal inference from observational data. It is definitely limited by some strong assumptions, but under those assumptions it works well, at least on simulated data (as shown e.g. by the recent work from Ramsey and Glymour). But I do agree regarding general skepticism about DCM - not so much about the theory, which I think is mostly reasonable, but about the application to fMRI (especially by people who just punch the button in SPM without really understanding what they are doing). I thought that the Lohmann paper did a good job of laying out many of the problems, and that Karl et al.’s response was pretty much gobbledygook.December 17 at 9:47am · Like · [Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Vince Calhoun I do not think even GCM can be considered useless for fMRI…however it is critical to keep the assumptions (what are the parameters capturing) and the data (what causes hemodynamic delay, etc) in mind in evaluating its utility and especially in interpreting the results (e.g. does this approach enable us to ‘see through’ the delay or just to characterize it). The post analysis (single or group) is critical as well and will often float or sink the boat. Every method out there has lots of holes in it, but in the right hands most of them can be informative.December 17 at 10:03am · Like · [Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Tal Yarkoni Russ, I think the question is whether you think those strong assumptions are ever realistic… E.g., looking back at the Ramsey & Glymour sims, my impression is that it’s kind of an artificial situation in that (a) the relevant ROIs have already been selected (typically one wouldn’t have much basis for knowing that only these 11 ROIs are relevant to the causal model!), (b) the focus is on ROI-level dynamics when it’s not at all clear that connectivity between ROIs is a good proxy for neuronal dynamics (I understand this is a practical assumption made of necessity, but we don’t actually know if it’s a reasonable one!), and (c) I think the way error is quantified is misleading–if you tell me that you only have 3 mislabeled edges out of 60, that doesn’t sound like much, but consider the implication, which could be, e.g., that now we think ACC directs activity in IPC rather than vice versa–which from a theoretical standpoint could completely change our understanding, and might very well falsify one’s initial hypothesis!And this still ignores all of the standard criticisms of causal modeling of observational data, which still apply to neuroimaging data–e.g., the missing variable problem, the labeling problem (if you define the boundaries of the nodes differently, you will get very different results), the likelihood that there’s heterogeneity in causal relationships across subjects, and so on. So personally I’m very skeptical that once you get beyond toy examples with a very small number of nodes (e.g., visual cortex responds to a stimulus and causes prefrontal changes at some later time), it’s possible to say much of anything positive about causal dynamics in a complex, brain-wide network. That said, I do agree with Vince that any method can have utility when handled carefully. E.g., a relatively benign use of causal methods may be to determine whether a hypothesized model is completely incompatible with the data–which is a safer kind of inference. But that’s almost never how these techniques are used in practice…December 17 at 10:21am · Like · [Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Tyler Davis Pragmatically, if you take all of the nonsense that happens with SEM in the behavioral literature, and then multiply be a factor of two to account for the nonsense due to mismodeling/misinterpreting hemodynamics, it makes it tough to believe an fMRI study with causal modeling unless you know the authors are good at math/know what they are doing or you were inclined to believe the results already.December 17 at 10:39am · Like · [Russ Poldrack](https://www.facebook.com/poldrack) it’s amazing how we have known for a good while now that Granger causality is useless for fMRI, yes people continue to use it[December 16 at 8:15pm](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39362615&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152197229548646)Russ Poldrack We are using a measurement method that intrinsically is at least an order of magnitude from the real casual mechanisms, so It would be folly to have too much faith in the resulting causal inferences. But to the degree that we can validate them externally, the results could still be useful. For me the proof is always in the pudding via mobile · Like · [Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Anastasia Christakou What is the pudding? If you have a biological hypothesis and your model-driven analysis is in line with it, what more can you do to “prove” it? Are you talking about independent external validation?December 17 at 11:38am · Like · [Russ Poldrack](https://www.facebook.com/poldrack) it’s amazing how we have known for a good while now that Granger causality is useless for fMRI, yes people continue to use it[December 16 at 8:15pm](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39362615&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152197229548646)Russ Poldrack the pudding in this case is some kind of external validation - for example, if you can show me that graphical model/DCM/GCM parameters are believably heritable, or that model parameters from fMRI are predictive of results in another domain (e.g., EEG/MEG, behavior).December 17 at 11:40am · Like · [Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Micah Allen Tal Yarkoni I don’t really find the ROI/model-selection problem with DCM overly troubling, although I do see where you are coming from. As Karl is fond of saying, it’s your job as the researcher to motivate the model space as it’s the hypothesis you are testing. The DCM is only valid in light of that constraint. I find it a bit unfortunate that there is this divergence between the two schools of thought; obviously in cases where you have no clue what the relevant hypothesis should be, tools like graph theory and functional correlations can be an excellent starting point. By definition, the validity of any DCM depends upon the validity of interpreting the experimentally-induced mass-univariate activations. DCM is built to assess specific directional hypothesis between regions activated by some experimental manipulation - it is unsurprising that if the selected task activates brain regions non-specifically or the ROIs are extracted haphazardly then the DCMs are equally invalid. But this isn’t an indictment of DCM, it’s a rather a failure to motivate a relevant model space. If working in unclear or murky territory, Karl is the first to say that a graph theoretical or connectomic approach can be a first step towards motivating a DCM. This is all circumstantial to the basic issue that GCM just plain gets the direction of connections wrong. I’ve read quite a bit of the DCM and GCM literature, and I actually do agree with you that many of the early papers are plagued by extremely toy examples. The end result was also always the same, that everything connected to everything. This is part of why the best DCM papers are those based on computational models, such as Hanneke den Ouden’s papers on motor learning, where the estimated parameters and winning model families are themselves of interest. DCM is extremely well equipped to assess the evidence that say, motor cortex updates premotor with the probability of a stimulus being a face or a house. This can’t be said for any other method - DCM is fundamentally built to assess brain-behavior relationships in this way. All the connectomics in the world won’t tell you much beyond “these brain regions are really important hubs and you probably shouldn’t knock them out”. To go the rest of the way you need a hypothesis driven methodology. That being said, a previous limitation has simply been that it’s incredibly frustrating and time consuming to estimate interesting models with more than 2 or 3 nodes. This has resulted in many small sample, ‘toy’ DCM papers that are generally not very interesting. The new post-hoc model optimization routines largely ameriolate these practical concerns - in my upcoming paper I easily estimate models with all 6-7 nodes activated by my mass-univariate analysis. I have a colleague estimating similar models in 600 scans from the HCP database. This means we will begin to see more intuitive brain-behavior correlations. As for the debate between Lohmann and Friston - Lohmann’s critique is just plain factually wrong on the details of the post-hoc procedure, and on several other details. Further Lohmann seems to fundamentally misunderstand the goal of model development and model selection. So I’m not really convinced by that. DCM requires strong hypothesis, which is both it’s strength and weakness, and fits extremely well with exploratory data-driven methods that actually work as advertised (unlike GCM). We’ve not even gotten into DCM for MEG/EEG (on which I am not an expert). The neural mass models there are extremely fascinating, going far beyond modelling mere data features to actually encapsulating a model of the underlying neurophysiological brain dynamics underlying observed responses. DCM for fMRI is itself at best just a starting point for actual neural modelling in M/EEG. Finally, the greatest strength of DCM is the Bayesian model selection procedure. Don’t like the canonical HRF? Substitute your own HRF and compare model evidences. Want to combine DTI, MEG, and LFP? DCM can do that.December 17 at 11:46am · Edited · Like[Russ Poldrack](https://www.facebook.com/poldrack) it’s amazing how we have known for a good while now that Granger causality is useless for fMRI, yes people continue to use it[December 16 at 8:15pm](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39362615&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152197229548646)Russ Poldrack Micah - what about Lohmann’s comments regarding model fit? I read Karl’s response as saying that it doesn’t matter if your model only accounts for 0.1% of the variance, as long as it beats another model in the model selection procedure. That seems mathematically correct but scientifically wrongheaded. FWIW We gave up on DCM soon after starting when we found that the models generally fit the data really poorly.December 17 at 11:53am · Like · [Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Micah Allen Were you doing DCM for fMRI or MEG? In general I’ve been told that the MEG variance explained tend to be far higher than in the fMRI DCM - close to 90% in some cases, compared to 20-60% in the best case for fMRI. An important caveat with DCM for fMRI is that it is not ideal for fast purely event related designs. If you have a fast stimulus driving factor and a slow (e.g. attention) factor you should see variance explained in the 20-60% range. In my paradigm I only get about 6-12% likely because I have two fast alternating events (stimulus intensity and probability). On the model fit question, I think this comes down to what the variational free energy parameter is actually optimizing. I am not an expert on the calculus behind VFE, but i’ve been to the DCM workshops and tried to understand the theory as much as my limited calc background allows. Essential VFE is a measure that weights the fit of the model (how well the model predicts the data, in a bayesian posterior sense) by the model complexity. I found this bit of Karl’s response helpful:“Generally speaking, a model with a poor fit can have moreevidence than a model with a good fit. For example, if you give amodel pure measurement noise, a good model should properly identifythat there is noise and no signal. This model will be better than amodel that tries to (over) fit noisy fluctuations.”So the model evidence accounts for fit, but goes beyond it. As Karl points out, a model with perfect fit can be a very poor model, so the VFE is an attempt to balance these things. Beyond that all I can say is that I was taught to always look at the variance explained as a diagnostic step - generally the rule of thumb here is that if your variance explained is really low, you probably made a mistake somewhere or the paradigm is very poorly optimized for DCM. I think in a practical sense, Lohmann have a point here, as there are diagnostic tools (like spm_dcm_explore) but no real guidelines for using them (when to decide you’ve mucked up the DCM). I think strictly speaking the VFE does the job, but only in the (perhaps too ideal) world where the models make sense.I found this blog really useful for understanding why DCM sometimes fails to give a good variance explained for event related paradigms:http://spm.martinpyka.de/?p=81SPM for programmers » The neural model of DCMspm.martinpyka.deWhat follows is a rough but hopefully didactically useful introduction into the …See More3 hours ago · Edited · Like · Remove Preview[Russ Poldrack](https://www.facebook.com/poldrack) it’s amazing how we have known for a good while now that Granger causality is useless for fMRI, yes people continue to use it[December 16 at 8:15pm](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39362615&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152197229548646)Russ Poldrack this was with fMRI data (with Marta Isabel Garrido) - but it was with fast ER designs, which are pretty much all we do in my lab, so that might explain part of the poor fit. Karl’s point about penalizing model complexity is totally reasonable, but I’m not sure that it really has anything to do with VFE per se -VFE is just one of many ways to penalize for model complexity. (of course, it’s Bayesian, so it must be better December 17 at 12:12pm · Like · [Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Daniel Handwerker The other trouble with the current DCM approaches is that they rely heavily on assumptions of what hemodynamic responses should be - including using the balloon model in a way it was never intended to be used. As part of a commentary on hemodynamic variation (Neuroimage 2012), I ran a small side analysis that showed how a modest, but very believable, difference in the size of the post-peak undershoot can flip the estimated direction of causality using DCM. This was a bit of a toy analysis, but it highlights a real concern how assumptions in building low-level parts of SPM’s DCM model can really affect results.December 17 at 12:27pm · Unlike · [Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Micah Allen Daniel, i’m not sure i’d agree that is a limitation. The nice thing about DCM is since it is built around a Bayesian hypothesis testing architecture, any competing HRF can be substituted for another and the resulting model estimates compared. So you could easily run a DCM with your HRF vs the canonical - if yours wins it would be a good argument for updating the stock model. The HRF part of DCM is totally modular, so a power user should find it easy to substitute a competing model (or multiple competing models). This point was made repeatedly at the DCM workshop in Paris last year.December 17 at 12:32pm · Edited · Like[Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Tal Yarkoni Micah, I think you’re grossly underestimating the complexities involved. Take the problem of selecting ROIs: you say it’s incumbent on the researcher to correctly interpret the mass univariate results. But what does that mean in practice? Estimation uncertainty at any given voxel is high; if you choose an ROI based on passing some p value threshold, you will often have regions in which the confidence interval’s lower bound is very close to zero. With small samples, you will routinely miss regions that are actually quite important at the population level, and routinely include other regions that are non-zero but probably should not be included. If you use very large samples, on the other hand, then everything is statistically significant, so now you have the problem of deciding which parts of the brain should be part of your causal model and which shouldn’t based on some other set of criteria. If you take what you just said to its conclusion, people probably shouldn’t fit DCM models to small datasets period, since the univariate associations are known to be shaky.Even if you settle on a set of ROIs, the problem of definition is not just one of avoiding “haphazard” selection; e.g., Steve Smith’s work in NeuroImage nicely showed that even relatively little mixing between different nodes’ timeseries will seriously muck up estimation–and that’s with better behaved network modeling techniques, in a case where we know what the discrete nodes are a priori (because it’s a simulation), and using network structures that are much more orderly than real data are likely to be (Smith et al’s networks are very sparse). In the real world you typically don’t know any of this; for example, in task-related activation maps, the entire dorsal medial surface of the brain is often active to some degree, and there is no good reason to, say, split it into 4 different nodes based on one threshold versus 2 nodes at another–even though this is the kind of choice that can produce wildly different results in a structural model.As for the hemodynamic issue: the problem is not so much that the canonical HRF is wrong (though of course we know it often is–and systematically so, in that it differs reliably across brain regions)–it’s that you compound all of its problems when your modeling depends on deconvolved estimates. It’s one thing to say that there is X amount of activation at a given brain region when you know that your model is likely to be wrong to some degree, and quite another to say that you can estimate complex causal interactions between 12 different nodes when the stability of that model depends on the same HRF fitting the data well in all cases.As to the general idea that DCM depends on strong hypotheses; this sounds great in principle, but the problem is that there are so many degrees of freedom available to the user that it is rarely clear what constitutes a disconfirmation of the hypothesis versus a “mistake” in one’s initial assumptions. Of course to some degree this is a problem when doing research of any kind, but it’s grossly compounded when the space of models is effectively infinite and the relationship between the model space and the hypothesis space is quite loose (in the sense that there are many, many, many network models that would appear consistent with just about any theoretical story one can tell).Mind you, this is an empirical question, at least in the sense that one could presumably quantify the effect of various “trivial” choices on DCM results. Take the model you mention with 6-7 nodes: I would love to know what happens if you systematically: (a) fit models that subsample from those nodes; (b) add nodes from other ROIs that didn’t meet the same level of stringency; (c) define ROIs based on different statistical criteria (remember that surviving correction is a totally arbitrary criterion as it’s sample size dependent); (d) randomly vary the edges (for all of the above). The prediction here is that there should be a marked difference in terms of model fit between the models you favor and the models generated by randomly permuting some of these factors–or, that when other models fit better, they should be theoretically consistent in terms of interpretation with the favored model. Is this generally the case? Has anyone demonstrated this?December 17 at 12:58pm · Like · [Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Rajeev Raizada I read the above discussion just now, and found it very interesting indeed. I have never tried playing with DCM, but I have worked in neural modeling in the computational neuroscience / neural circuits sense. In general, I am skeptical of complex and sophisticated models, especially models which congratulate themselves on their complexity and sophistication, or, in the case of some of the models mentioned immediately above, models which congratulate themselves on the sophistication of their self-quantification of complexity. A question for the DCM/GCM-aficionados out there: is there a single instance of such approaches generating a novel insight into brain function which was later independently validated by a different method? It looks to me as though there are a lot of instances of such approaches producing interesting-looking outputs which seem reasonable given what we think we know about the brain. But the road to hell in science is paved with interesting and reasonable-sounding stories.This last line is partly troll-bait, but I’ll throw it out there anyway as I think it might be a valid comparison: are DCM/GCM approaches a bit like the evolutionary psychology of fMRI? Sources of interesting just-so stories?December 17 at 1:03pm · Edited · Unlike · [Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Micah Allen Out of the office at dinner now so I’m afraid I must leave this debate for now. You make dinner excellent points Tal though I’m not sure I agree with the picture you paint of fMRI effects bring quite so arbitrary. At my former center we stuck to an N = 30 guideline and found this to be an excellent compromise, neither under nor over powered. In my oddball paradigm I get extremely anatomically constrained activations that fit well with the literature. I extract 6mm spherical VOIs from each peak. This seems like a pretty reasonable way to characterize the overall effect, but I do think the analysis you suggest would interesting in any context DCM or otherwise. Sorry so brief- on my phone at dinner via mobile · Edited · Like[Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Micah Allen Rajeev Raizada a good question but there is a lot of ongoing intracortical and multimodal validation work being done with DCM. Check out Rosalyn Moran’s recent papers December 17 at 1:27pm via mobile · Like[Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Rajeev Raizada Sounds interesting. Are you talking about this paper, which finds that boosting acetylcholine increases the size of the mismatch negativity?http://www.ncbi.nlm.nih.gov/pubmed/23658161 An interesting result, but it’s not entirely clear to me that the theoretical overlay of predictive-coding and DCM adds a great deal, or that the empirical result adds much support to the theory. After all, this paper from 2001 already showed the converse result (reduced ACh gives reduced MMN): http://www.ncbi.nlm.nih.gov/pubmed/11467911 , and there is a ton of evidence showing that ACh increases cortical signal-to-noise. However, I may be focussing on a different paper than you were referring to.Free energy, precision and learning: the role of … [J Neurosci. 2013] - PubMed - NCBIwww.ncbi.nlm.nih.govPubMed comprises more than 23 million citations for biomedical literature from M…See MoreDecember 17 at 2:06pm · Like · Remove Preview[Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Daniel Handwerker As Tal notes, the issue with hemodynamic variation is that it varies quite a bit around the brain and there is no existing model in a Baysean or any other framework that can solve this degree of variation. This isn’t a problem if one’s analysis is robust to much of the variation, but the deconvolution step in DCM amplifies its sensitivity to the selected hemodynamic model. If two brain regions having slightly different relative undershoots is enough to make the model fail, then when can we trust that it works?Another way of saying that a power user can make their own hemodynamic model is to say that the model that is default with the SPM DCM software doesn’t work in real-world situations, but that doesn’t preclude someone in the future from making a model that does work. This might be true, but it does little to increase confidence in the accuracy of current DCM studies. As others have noted, simpler causality measures often have their assumptions more out in the open and someone can design a study that is robust to those assumptions. If a study was designed well enough that it could work within the complex assumptions of DCM, I can’t think of a situation where it wouldn’t also meet the assumptions of simpler approaches.December 17 at 8:28pm · Like · [Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Micah Allen No, the option is there for those who want to show that there is a better model. The canonical balloon HRF enjoys a great deal of robustness, which is why its the function of choice in nearly all neuroimaging packages. Further DCM has been shown to be very robust to lag differences of up to 2 seconds. I hope you guys at not out there rejecting DCM papers on these basic principles without at least reading the actual validation papers behind it. Your toy example shows that DCM is sensitive to the HRF of choice, not that the canonical is an inappropriate model. If there is that big of a smoking gun problem with DCM, I’d probably suggest you publish to that effect to prevent a lot of people from wasting time via mobile · Like[Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Micah Allen Just to try to close on a positive note, I find this review by Stephen Smith to be exceptionally balanced, cutting through much of the hyperbole surrounding different connectivity methods. I think he really does a great job pointing out the natural strengths and weaknesses of each of the available approaches, advocating for a balanced viewpoint. Also don’t miss the excellent foot note where he describes views from ‘scientist A and scientist B’ - pretty obvious who he’s referring to there http://www.sciencedirect.com/…/pii/S1053811912000390#The future of FMRI connectivitywww.sciencedirect.comYesterday at 8:29am · Edited · Unlike ·  · Remove Preview[Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Marta Isabel Garrido This is one of the best validation papers showing robustness of DCM for fMRI (and also how GC fails terribly) http://www.ncbi.nlm.nih.gov/pubmed/19108604Identifying neural drivers with functional MRI: an… [PLoS Biol. 2008] - PubMed - NCBIwww.ncbi.nlm.nih.govPubMed comprises more than 23 million citations for biomedical literature from M…See MoreYesterday at 5:05am · Like ·  · Remove Preview[Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Daniel Handwerker Micah, when I say that a fundamental assumption of the current implementation of DCM fails under very realistic conditions to such an extent that it will flip the result, improving DCM is not my only option. The other option is to not personally use DCM and treat DCM findings which rest on these fundamental assumptions with a good bit of skepticism. If I had a specific project that would clearly benefit from the DCM approach, I might reassess my stance, but until then, it’s the job of DCM users to make the method/interpretations more robust.I’ll also note that the robustness of the canonical HRF is analysis dependent. My 2004 Neuroimage paper showed that it’s robust for finding significance maps using a GLM, but it is less robust if you’re taking magnitude estimates into a group analysis. Still, the robustness of the canonical HRF in linear regression based studies has been shown to be pretty good. That doesn’t mean the same model is robust when used as part of a completely different analysis method. Any causality measure is probably going to be more sensitive to GLM shape compared to intravoxel statistical approaches. My review that I mentioned earlier ( http://www.sciencedirect.com/…/pii/S1053811912001929 ) is a published example showing how DCM can fail. It’s an example rather than a full examination of the method’s limitations, but it’s enough to cause me some concern.The continuing challenge of understanding and modeling hemodynamic variation in fMRIwww.sciencedirect.com20 hours ago · Like · Remove Preview[Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Nikolaus Kriegeskorte this is a fascinating thread. i’m left with three thoughts.(1) the paper by webb et al. does not seem to invalidate granger causality inferences based on comparisons between different experimental conditions. it would be good to hear Alard Roebroeck’stake on these issues. (2) it would be good to have an fMRI causality modelling competition in which several simulated data sets (with known but unrevealed ground truth and realistic complexities) are analysed by multiple groups with multiple techniques. (3) the only thing that puts the proof in the pudding for me is prediction. in decoding analyses, for example, the use of independent test sets (crossvalidation) ensures that incorrect assumptions make us more likely to accept the null hypothesis. what is the equivalent of this in the realm of causal analyses?16 hours ago · Edited · Unlike · [Russ Poldrack](https://www.facebook.com/poldrack) it’s amazing how we have known for a good while now that Granger causality is useless for fMRI, yes people continue to use it[December 16 at 8:15pm](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39362615&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152197229548646)Russ Poldrack Niko - agreed, the only reasonable use of Granger causality analysis with fMRI that I have seen is the work by Roebroek that showed differences between conditions in G-causality within the same region, which mostly obviates the issues regarding HRFs (unless you think the latency is changing with amplitude). if only I was FB friends with Alard! And I second both your call for a competition (though the devil is in the details of the simulated data) and the ultimate utility of crossvalidation.16 hours ago · Like[Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Jack Van Horn Niko: something like your item #2 as a challenge for the OHBM Hackathon.#15 hours ago · Like · [Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Nikolaus Kriegeskorte jack, that would be great. for the next meeting, we have an organising team with a plan already. could consider this for hawaii. it would be good to hear opinions from Klaas and Karl (whom i’m missing on facebook), Alard Roebroeck, and Stephen Smith – as well as the commentators above.15 hours ago · Like · [Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Nikolaus Kriegeskorte Russ Poldrack is right that the devil is in the details of the simulated data. each method may prevail when its model matches the one that generated the data. it will therefore be key to include realistic complications of the type described by tal, which none of the methods address. the goal could be to decide about each of a set of causal claims that either do or do not hold in the data-generating ground-truth model.15 hours ago · Like[Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Nikolaus Kriegeskorte I’d like to hear Rik Henson’s take on this entire thread.14 hours ago · Like[Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Micah Allen This has been a very informative thread for me - wish Facebook posts could be storified as this is an important debate. I would love to see it formally continued at OHBM. I will try to get Karl’s opinion on these issues at the next methods meeting. Over beers last night several members of that group expressed that some of this validation was underway, and that DCM is undergoing significant revision to address many of these issues. Still it’s clear that there has not been enough discussion. Also as a side note I agree that the GCM paper in my blog post does not necessarily implicate between condition differences. I think the answer to some of these issues is inevitably going to come down to experimental design, as probably both GCM and DCM can be more or less robust to vascular confounds depending on the timing and nature of your paradigm. It would be nice to know exactly what those constraints are.5 hours ago via mobile · Edited · Like[Tal Yarkoni](https://www.facebook.com/tal.yarkoni) My impression is that as bad as the assumptions you have to make for GCM may be, the assumptions for DCM are still worse. The emerging trend seems to be towards a general skepticism of causal modeling of observational imaging data, not a shift from one causal method to another.[December 17 at 9:39am](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39371921&offset=0&total_comments=44) · [Like](https://www.facebook.com/poldrack/posts/10152196838323646?comment_id=39407394&offset=0&total_comments=44&notif_t=feed_comment#) · [](https://www.facebook.com/browse/likes?id=10152198399063646)Jack Van Horn This brings to mind early 1990’s concerns about the effects that vasculature and draining veins, in particular, had on measured regional BOLD activation. This was considered a real issue by some and a potential deal breaker for the future of fMRI over PET. But others felt that the HRF actually saved the day since it would be modulated by neural activity under cognitive task conditions. The issue kind of got swept under the rug for the last 15 years or so. Interesting that this is now re-emerging in the realm of functional connectivity, resting-state, and notably Granger causality. Could it be that auto-correlative Granger modeling is doing exactly what it is supposed to do? And perhaps it has simply been our poor understanding of its implications relative to actual hemodynamics that is finally catching up to us? I look forward to seeing the further discussions."
  },
  {
    "objectID": "posts/a-discussion-of-causal-inference-on/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/a-discussion-of-causal-inference-on/index.html#comments-captured-from-original-post-on-blogger",
    "title": "A discussion of causal inference on fMRI data",
    "section": "10 comments captured from original post on Blogger",
    "text": "10 comments captured from original post on Blogger\ndaniele said on 2013-12-19\nThanks for the fascinating discussion.As shown by Lionel Barnett and Anil Seth in a recent paper, the problem is not HRV per se, but HRF together with downsampling.Furthermore, the problem of generating a good ground truth is a though one.We have developed a method for retrieving and deconvolving the HRF for each voxel even at resthttp://www.ncbi.nlm.nih.gov/pubmed/23422254and this seems to improve the performance of GC.We used it also to get whole brain GC maps from deconvolved data, which can be compared to the paper by Webb et alhttp://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0073670I wrote Olivier David asking him his data from his PLOS Biology 2008 paper to see whether I can get GC to perform better.\ntal said on 2013-12-20\nRuss, you really need to raise the character limit on comments; 4,096 ain’t enough–this isn’t Twitter! ;)Micah, I don’t really think it’s fair to summarize people’s concerns here by saying that \"you can find examples where DCM for fMRI may perform poorly\". A better way to summarize them is that the space of possible models is so unimaginably huge, our understanding of what the brain is doing is so poor, and there are so many choices a researcher needs to make that are essentially arbitrary, that it’s almost inconceivable that there could be a method for reliably evaluating the (observational) evidence for causal models. This wouldn’t be such a terrible thing if there was a serious movement to try and validate the methods before putting them into use, but in practice there doesn’t be. The kind of simulations that Lohmann et al did, for instance, strike me as absolutely critical, and is the kind of thing one should do before a method becomes widely applied, not several years later.To respond to your specific comments, I’m certainly not saying that the definition of nodes in the model is totally arbitrary; what I’m saying is that much of it is arbitrary, and that there is essentially no reason to think that just by blind luck we will ever get it right (not perfectly right; I mean even close to right). Let’s take again your data as an example. You say your model has 6 nodes because those 6 survived group-level correction. But surely you agree that if your sample size were twice as large, many more regions would survive correction. And if your sample size were halved, fewer would. This is not up for debate; it’s a necessary consequence of the null hypothesis significance testing framework. If we take your argument seriously that DCM should only be applied to models constrained by the results of a mass univariate analysis where regions activate in response to experimental manipulations, what are we now to make of this? Is it really reasonable to say that the 6-node model you selected happens to be the right one, and is a decent approximation to the truth, even thought we know full well that a doubling of sample size would have given you perhaps a 15-node model, and a halving might have perhaps given you a 4-node model? And that every time you re-run the experiment, you’re likely to get somewhat different nodes? (1/4; continued below.)\ntal said on 2013-12-20\nNote that this is exactly the same omitted variable issue that presents such trenchant problems for structural models in any domain; imaging is not special. The key point, which is almost invariably ignored, is that a given causal interpretation of a model is only sensible if one is willing to assume that all and only the relevant nodes are included in the network. In behavioral research this is already almost always an untenable assumption because it isn’t hard to think of confounding variables for almost any measured variable that is supposed to carry causal influence. But it gets much worse when you’re talking about neuroimaging, where the brain is a very dense causal system and we know for a fact that most of the nodes selected for DCM analysis correlate quite strongly with many other voxels or ROIs that were left out (typically for arbitrary reasons). To put it in perspective, this is kind of like constructing a simple causal model positing that the positive correlation between food intake on body mass is mediated by fat consumption, and then when someone pops up and politely interjects that fat consumption is heavily confounded with carb consumption, protein consumption, and any number of other things, simply saying \"well sure, but we didn’t model those things, so oh well\" and continuing along. When you build a model that includes, say, ACC and DLPFC, but not inferior parietal cortex, this is only an okay approximation if you have strong reason to think that IPC is not actually doing a lot of the work you’re attributing to ACC or DLPFC (with which it will be strongly correlated) AND you also know that including IPC wouldn’t fundamentally change the causal dynamics between the other regions (and frankly, I don’t know how you’d know that).Even if you set the omitted variable issue aside–which, again, is not a small one, and for many people is a deal-breaker even in the behavioral literature–you still haven’t really addressed the issue of node definition: if you haven’t quantified the impact of, say, including or dropping an extra 10% of the voxels in each ROI based on a slight juggling of the threshold (which, again, you can’t argue is anything other than arbitrary!), what reason do I have to believe your results? Again, it’s not that this is some far-fetched hypothetical concern that could, under just the right circumstances, present some smalll problem. We have both theoretical reasons and empirical reasons to suspect that very small perturbations of the nodes will have very large effects on the model. It’s not an acceptable defense to say that, well, it’s the researcher’s job to get things right. Surely it’s also the researcher’s job to show the people evaluating the research that the results don’t completely fall apart when very minor, and essentially arbitrary, parameters are tweaked. And again, it’s not really up for debate that there is a large amount of arbitrariness, because as I understand it you’re defining the nodes using spheres centered around the mass univariate activation peaks, and I’m quite sure that if you look at the confidence intervals of all your voxels, you will find that the peak voxels would have been quite different with a different sample (plus the diameter of the sphere will make a big difference too). (2/4)\ntal said on 2013-12-20\nMind you, all this still depends on yet another very strong, and in my view largely indefensible, assumption, which is that a brain region that plays an important causal role in the network has to show a univariate difference between experimental conditions. Frankly this strikes me as wishful thinking. For one thing, many absolutely crucial causal effects may be driven by very transient changes in psychological state space that don’t translate into robust BOLD signals (i.e., they would be invisible to fMRI). We have no reason to think that fMRI is in a position to capture all, or even most, of the causal dynamics crucial to understanding what a network is doing, do we? Moreover, each voxel contains hundreds of thousands or millions of neurons, and a typical ROI may contain hundreds of millions. We know from electrophysiological studies that it is entirely typical to find interdigitated assemblies of neurons with quite different response properties within roughly the same chunk of tissue; it is not at all a stretch to think that a particular region might show no net change in BOLD signal even though vitally important causal interactions are taking place (e.g., two different populations of prefrontal neurons are involved in recurrent activation of different kinds of sensory representations in posterior cortex). Note that I’m not arguing that all of the signals must sum to a net of exactly zero when comparing two experimental conditions. I’m saying that as far as I’m aware, we have no good reason at all to expect anything like a strong linear relationship between the causal centrality of a region and its activation strength in a mass univariate analysis. And we have even less reason to expect that such a relationship would manifest in a readily interpretable way–which is what you require when you say that it is the researcher’s job to look at their experimental results and pick out the nodes that matter. I think this is giving researchers entirely too much credit–not to mention some very extraordinary powers of apprehension.This of course all entirely leaves aside the Lohmann et al critique, which in my view carries exactly the force Lohmann et al suggested, and still hasn’t been rebutted in a convincing way. It’s no good to say that it’s the researcher’s job to select the right models to test (which was the gist of Friston’s response) if there’s no evidence that the method is capable of discriminating between models reliably to begin with. What Lohmann et al showed was that DCM cannot reliably distinguish families of models that are neuroscientifically plausible from those that are wildly implausible; in view of that result, why should anyone expect that DCM can discriminate between two models that are both neuroscientifically plausible–a discrimination that can only be more difficult? When you say that \"With good experimental design I think DCM is largely robust to the small differences\", what are you basing that on? The assertion flies in the face of the Lohmann simulations, all of the considerations I raised above, and of many people’s experience tweaking DCM models for a very long time to try to get a stable result. It isn’t the kind of statement the purveyors and consumers of a sophisticated causal modeling technique that rests on numerous strong assumptions should just take for granted–especially when it’s so easy to empirically test. Or, to put it this way: how do you (and I mean you personally) actually know that your results are robust, if you haven’t done the work to test their resilience to small changes, or compared them to what you would get by generating models at random? Isn’t this a giant leap of faith? Isn’t it your responsibility to first verify for yourself that your results don’t disappear when you tweak key parameters before publishing them? (3/4)\ntal said on 2013-12-20\nLastly, I think it’s kind of unfair to characterize this as a debate between people on the machine learning side who favor exploratory data analysis but don’t really care about hypotheses, and people who think deeply about hypothesis teseting but are willing to take a couple of steps beyond the data. It’s a nice rhetorical move, because it allows you to write off half of the field as people with philosophical differences, but I don’t think this is about philosophical differences at all. One can think deeply about theoretical models and still recognize that just because a model makes sense in your head, and produces results you like when explicitly compared with several other models, does not mean that it approximates the truth in any meaningful sense. This is not a philosophical claim, it’s an empirical one that can be directly tested (and has been tested). A major problem with much of the DCM literature (and the causal modeling literature more generally) is that there is very little emphasis on validation beyond what makes theoretical sense to the authors–which isn’t really validation at all. That is the substantive criticism you see from many people in the above thread. I imagine that when you look at your model–which DCM reports has better evidence than at least a few other alternatives–you see an elegant theoretical story supported by strong evidence, and you probably view concerns about the quality of the underlying data as minor annoyances that aren’t your responsibility, and that future studies can address. The problem is that many other people who look at your results will immediately think of all the extremely strong assumptions you have to make in order to believe the results but that are almost certain to be false, and which you could have easily tested but didn’t.The key point is that this really isn’t some deep philosophical divide between two incommensurable camps; it’s something that causal modeling proponents have the power to address. All it would take to bridge whatever divide exists is for people in the causal modeling camp to get more serious about external validation. This means two things, as far as I’m concerned: (a) testing the assumptions underlying DCM in cases where it’s straightforward to do so (e.g., by simply repeating your analyses several times with slightly different parameter choices); and (b) as Niko said, actually predicting novel observations in an unbiased way (i.e., showing that the DCM model with the highest evidence also consistently predicts new observations we care about better than models with lower evidence). If you can show me in your papers that your results aren’t entirely dependent on parameter choices that were clearly not under your control in any meaningful sense, and that your method is robust against very simple validity threats (e.g., random models having better evidence, as in Lohmann et al), then I will be happy to take them seriously. But if you can’t or aren’t willing to provide that kind of support, then I don’t see why I (or anyone else) should put any stock in the findings given all of the enormous assumptions involved. (4/4)\nUnknown said on 2013-12-23\nHi this is Anil Seth. What an excellent debate and I hope I can add few quick thoughts of my own since this is an issue close to my heart (no pub intended re vascular confounds).First, back to the Webb et al paper. They indeed show that a vascular confound may affect GC-FMRI but only in the resting state and given suboptimal TR and averaging over diverse datasets. Indeed I suspect that their autoregressive models may be poorly fit so that the results rather reflect a sort-of mental chronometry a la Menon, rather than GC per se.In any case the more successful applications of GC-fMRI are those that compare experimental conditions or correlate GC with some behavioural variable (see e.g. Wen et al. http://www.ncbi.nlm.nih.gov/pubmed/22279213). In these cases hemodynamic and vascular confounds may subtract out. Interpreting findings like these means remembering that GC is a description of the data (i.e. DIRECTED FUNCTIONAL connectivity) and is not a direct claim about the underlying causal mechanism (e.g. like DCM, which is a measure of EFFECTIVE connectivity). Therefore (model light) GC and (model heavy) DCM are to a large extent asking and answering different questions, and to set them in direct opposition is to misunderstand this basic point. Karl, Ros Moran, and I make these points in a recent review (http://www.ncbi.nlm.nih.gov/pubmed/23265964).Of course both methods are complex and ‘garbage in garbage out’ applies: naive application of either is likely to be misleading or worse. Indeed the indirect nature of fMRI BOLD means that causal inference will be very hard. But this doesn’t mean we shouldn’t try. We need to move to network descriptions in order to get beyond the neo-phrenology of functional localization. And so I am pleased to see recent developments in both DCM and GC for fMRI. For the latter, with Barnett and Chorley I have shown that GC-FMRI is INVARIANT to hemodynamic convolution given fast sampling and low noise (http://www.ncbi.nlm.nih.gov/pubmed/23036449). This counterintuitive finding defuses a major objection to GC-fMRI and has been established both in theory, and in a range of simulations of increasing biophysical detail. With the development of low-TR multiband sequences, this means there is renewed hope for GC-fMRI in practice, especially when executed in an appropriate experimental design. Barnett and I have also just released a major new GC software which avoids separate estimation of full and reduced AR models, avoiding a serious source of bias afflicting previous approaches (http://www.ncbi.nlm.nih.gov/pubmed/24200508).Overall I am hopeful that we can move beyond premature rejection of promising methods on the grounds they fail when applied without appropriate data or sufficient care. This applies to both GC and fMRI. These are hard problems but we will get there.\ndaniele said on 2013-12-25\nI am a bit concerned about how GC was computed in that paper:-they computed pairwise GC, that especially for datasets composed of many short time series will result in a high number of false positives.-why did they consider only regions with a high asymmetry in GC? zero GC balance could result from no interaction or by high outflow plus high inflow, which are quite different scenarios.Using our approach to retrieve and deconvolve HRF from resting state, I plotted the HRF shape on the brain:http://figshare.com/articles/HRF_parameter/886139looks like that regions with higher, slower and wider HRF are those that you identify as GC sinks (venous overlap), and this would explain that GC maps obtained after deconvolution (and with a conditioned approach) are different from the ones they find (Wu et al. in the references of the paper).\nUnknown said on 2013-12-27\nExcellent discussion, with some great arguments, a number of insights for me, but also a deja vu or two.Concerning the initial post: I wonder about the argumentation… Showing that a method has high sensitivity for A (here: large vasculature) does not in any way establish it lacks the specificity to pick up B (here: neuronal interactions). You could also use almost any ICA variant on the same data to find the same (or even better?) vasculature maps. Does that establish that ICA cannot be used for rs-fMRI analysis to find neuronal resting state networks? Obviously (to me, at least): no, it doesn’t establish that.From a historical perspective, when we first worked on Granger Causality Mapping (GCM) around 2004 (http://www.ncbi.nlm.nih.gov/pubmed/15734358; thanks for your appreciation for the work, Russ!) we wanted to answer pretty much the following questionsi) What could GCM add, why do it at all? Our best answer: It adds structural model exploration that can (should?) precede confirmatory connectivity analysis, such as DCM. Confirmatory analysis selects between hypotheses or models containing a few areas (and for fMRI each hypothesis contains the same areas). It runs the risk of selecting one that compares poorly with many non-tested models that contain 1 or (many) more crucial missing regions (the missing region problem). Moreover: a causal mapping approach can correctly identify the boundaries between differently interacting nodes, which might have been lumped together based on a thresholded GLM, and can identify nodes that did not show up in a GLM at all. We have re-iterated this point in http://www.ncbi.nlm.nih.gov/pubmed/19786106, and after that (and Karl’s kind agreeing reply: http://www.ncbi.nlm.nih.gov/pubmed/19770049), methods to allow DCM to test models with more than 3-4 regions have started to appear. I am happy to now see FIL lab affiliates (read: Micah) say things like ‘a previous limitation has simply been that it’s incredibly frustrating and time consuming to estimate interesting models with more than 2 or 3 nodes’ and ‘DCM requires strong hypothesis, which is both it’s strength and weakness, and fits extremely well with exploratory data-driven methods’ agreeing with our original point almost 10 years ago. Likewise it is interesting to see in this discussion (e.g.: Tal’s posts) that the missing region problem is still one of the chief concerns scientists have in using confirmatory approaches. ii) Can we pick up 10-100ms neuronal (conduction and processing) delays through 1000’s ms hemodynamics delays and in the presence of remote vasculature with 100’s ms delays? Our best answer came from first testing this in simulation and was, perhaps surprisingly: yes. However, anything with more delay is picked up with more statistical power (more sensitivity, see Figure 3 in http://www.ncbi.nlm.nih.gov/pubmed/15734358). Shorter TRs (as in the HCP data) increase power altogether and separate long and short delays more effectively. From that fact and from seeing early ‘raw’ GCMs abundant in large vasculature, as expected, we devised a strategy to filter out vasculature: ‘instantaneously- filtered’ GCMs. In short: short delays components (such neuronal interactions) show up both in instantaneous correlations and G-causal (delayed) terms; large delay components (such as vasculature) show up almost exclusively in the G-causal terms. So, filter out delayed components that do not have an instantaneous correlation term. That proved very effective. Conversely: if you really want to look at vascular delays, unfiltered maps are a very sensitive instrument, which is what the cited PLoS ONE study seems to have done. No surprise here. And, if it is the study target, mapping out large vasculature can be very useful, e.g. as a danger-zone map for other methods.(1/2)\nUnknown said on 2014-01-06\n\nHow do we deal with the varying HRF confounds?Our best answer: Always (at the very least) look for experimental modulation of G-causality. Period. We have made and re-made this point many times, but it is still often missed. This also lets experimental manipulations drive causal understanding, as they should, rather than just assumptions added to statistics that come from the data. Model based deconvolution can be an additional tool here but, as we discussed in http://www.ncbi.nlm.nih.gov/pubmed/19786106, this is yet another model-based endeavor and inaccurate HRF models can lead to new errors rather than remove old ones (see Daniel’s posts above). Incidentally, we wrote this work mainly as a reply to Olivier David’s study (cited by Marta above) to argue there are a few gross oversights in that study: first: they did not use experimental modulation in applying G-causality  and, second, when they added the deconvolution model of DCM to G-causality it performed perfectly against the gold-standard, unlike DCM which underperformed, even on 3 regions (Marta: Why cite this study to say GC does not perform well!?). Interestingly, even in deconvolution modeling, one can pit more or less parametric models against each other. As Pedro-valdes Sosa, Anil Seth and myself summarized in http://jmlr.org/proceedings/papers/v12/roebroeck11/roebroeck11.pdf, various AR and ARMA models (used for G-causality) have invariances that theoretically render them unaffected by things like varying HRF convolution kernels. Anil’s nice work subsequently showed this can indeed be the case for simple AR models if one samples fast enough (see Anil’s post above).I second (third, fourth?) Niko’s call for a causal analysis competition. And, yes, the generating model must be realistic in the detail that we know and agree on, whether we a priori believe it will be relevant detail or not. Not being close to the estimating model, as Niko posed, is one important aspect. Actually containing the aspects we know to be present in the brain (such as 10ms conduction delays) rather than assuming these will not matter anyway, is another important aspect. To illustrate: when Steve Smith showed me the manuscript of the network modeling approaches paper, I told him I thought the conclusion that the sensitivity of G-causality methods is low is misleading. Steve used the DCM model for fMRI to generate the data, which does not have any neuronal delays. It can’t, it doesn’t use delays (in fMRI). The model necessarily puts any observed delays in the hemodynamics in the inversion. It has been designed to do that. So, if you generate data with it, you are generating data with only instantaneous dependencies. It is obvious connectivity models tuned to detect instantaneous dependencies or that use them in their causal modeling will do well on these simulations. And that those tuned to delayed interactions will (correctly) show nothing. I told Steve I expected the results to be different if, instead, he would use the neuronal DCM model for EEG/MEG with suitable neuronal delays (then chained with the DCM hemodynamic model, of course). Steve said he did not think it would make a difference. I still think it will, and I think Anil’s paper has shown as much with an even more complex neuronal simulation model with delays.I, for one, am still simultaneously critical and enthusiastic about both GCM and DCM. The methods currently seem to be converging as each is taking up the strengths of the other. I agree external validation is important and should be a new norm: test the same (set of) model(s) with different modalities over several studies. But still both will be potentially very useful if used carefully and quite dangerous if used indiscriminately. Just like any other even moderately complex method. The thing is, I think that if we chose not to use them at all we would lose some of our best current instruments to help understand the wildly complex multiscale non-linearly coupled network that is the brain…(2/2)\n\nUnknown said on 2014-01-06\nBtw: Niko, to your comment on crossvalidation of connectivity model fits on independent test sets. This has been done by Jason Smith and Barry Hortwitz, see e.g.: http://www.ncbi.nlm.nih.gov/pubmed/19969092 . The switching linear dynamic system essentially learns a different AR connectivity model for each experimental condition from the training data. Then it predicts experimental conditions at each time point in the unseen test data. This has two more important side implications for the discussion. 1) experimental design and modulation have it again. 2) The prediction level on the test set indeed works as a global measure of fit immune to overfitting. We could accept, say, 80% correct as a minimum to accept any model if we were so inclined."
  },
  {
    "objectID": "posts/skeletons-in-closet/index.html",
    "href": "posts/skeletons-in-closet/index.html",
    "title": "Skeletons in the closet",
    "section": "",
    "text": "This was originally posted on blogger here.\nAs someone who has thrown lots of stones in recent years, it’s easy to forget that anyone who publishes enough will end up with some skeletons in their closet. I was reminded of that fact today, whenDorothy Bishop posted a detailed analysis of a paper that was published in 2003 on which I am a coauthor.This paper studied a set of children diagnosed with dyslexia who were scanned before and after treatment with the Fast ForWord training program. The results showed improved language and reading function, which were associated with changes in brain activation. Dorothy notes four major problems with the study:There was no dyslexic control group; thus, we don’t know whether any improvements over time were specific to the treatment, or would have occurred with a control treatment or even without any treatment.The brain imaging data were thresholded using an uncorrected threshold.One of the main conclusions (the “normalization” of activation following training”) is not supported by the necessary interaction statistic, but rather by a visual comparison of maps. The correlation between changes in language scores and activation was reported for only one of the many measures, and it appeared to have been driven by outliers.Looking back at the paper, I see that Dorothy is absolutely right on each of these points. In defense of my coauthors, I would note that points 2-4 were basically standard practice in fMRI analysis 10 years ago (and still crop up fairly often today). Ironically, I raised two of of these issues in my recent paper for the special issue of Neuroimage celebrating the 20th anniversary of fMRI, in talking about the need for increased methodological rigor:Foremost, I hope that in the next 20 years the field of cognitive neuroscience will increase the rigor with which it applies neuroimaging methods. The recent debates about circularity and “voodoo correlations” ([Kriegeskorte et al., 2009] and [Vul et al., 2009]) have highlighted the need for increased care regarding analytic methods. Consideration of similar debates in genetics and clinical trials led (Ioannidis, 2005) to outline a number of factors that may contribute to increased levels of spurious results in any scientific field, and the degree to which many of these apply to fMRI research is rather sobering:•small sample sizes•small effect sizes•large number of tested effects•flexibilty in designs, definitions, outcomes, and analysis methods•being a “hot” scientific fieldSome simple methodological improvements could make a big difference. First, the field needs to agree that inference based on uncorrected statistical results is not acceptable (cf. Bennett et al., 2009). Many researchers have digested this important fact, but it is still common to see results presented at thresholds such as uncorrected p &lt; .005. Because such uncorrected thresholds do not adapt to the data (e.g., the number of voxels tests or their spatial smoothness), they are certain to be invalid in almost every situation (potentially being either overly liberal or overly conservative). As an example, I took the fMRI data from Tom et al. (2007), and created a random “individual difference” variable. Thus, there should be no correlations observed other than Type I errors. However, thresholding at uncorrected p &lt; .001 and a minimum cluster size of 25 voxels (a common heuristic threshold) showed a significant region near the amygdala; Fig. 1 shows this region along with a plot of the “beautiful” (but artifactual) correlation between activation and the random behavioral variable. This activation was not present when using a corrected statistic. A similar point was made in a more humorous way by Bennett et al. (2010), who scanned a dead salmon being presented with a social cognition task and found activation when using an uncorrected threshold. There are now a number of well-established methods for multiple comparisons correction (Poldrack et al., 2011), such that there is absolutely no excuse to present results at uncorrected thresholds. The most common reason for failing to use rigorous corrections for multiple tests is that with smaller samples these methods are highly conservative, and thus result in a high rate of false negatives. This is certainly a problem, but I don’t think that the answer is to present uncorrected results; rather, the answer is to ensure that one’s sample is large enough to provide sufficient statistical power to find the effects of interest.Second, I have become increasingly concerned about the use of “small volume corrections” to address the multiple testing problem. The use of a priori masks to constrain statistical testing is perfectly legitimate, but one often gets the feeling that the masks used for small volume correction were chosen after seeing the initial results (perhaps after a whole-brain corrected analysis was not significant). In such a case, any inferences based on these corrections are circular and the statistics are useless. Researchers who plan to use small volume corrections in their analysis should formulate a specific analysis plan prior to any analyses, and only use small volume corrections that were explicitly planned a priori. This sounds like a remedial lesson in basic statistics, but unfortunately it seems to be regularly forgotten by researchers in the field. Third, the field needs to move toward the use of more robust methods for statistical inference (e.g., Huber, 2004). In particular, analyses of correlations between activation and behavior across subjects are highly susceptible to the influence of outlier subjects, especially with small sample sizes. Robust statistical methods can ensure that the results are not overly influenced by these outliers, either by reducing the effect of outlier datapoints (e.g., robust regression using iteratively reweighted least squares) or by separately modeling data points that fall too far outside of the rest of the sample (e.g., mixture modeling). Robust tools for fMRI group analysis are increasingly available, both as part of standard software packages (such as the “outlier detection” technique implemented in FSL: Woolrich, 2008) and as add-on toolboxes (Wager et al., 2005). Given the frequency with which outliers are observed in group fMRI data, these methods should become standard in the field. However, it’s also important to remember that they are not a panacea, and that it remains important to apply sufficient quality control to statistical results, in order to understand the degree to which one’s results reflect generalizeable patterns versus statistical figments.It should be clear from these comments that my faith in the results of any study that uses such problematic methods (as the Temple et al. study did) is relatively weak. I personally have learned my lesson and our lab now does its best to adhere to these more rigorous standards, even when they mean that a study sometimes ends up being unpublishable. I can only hope that others will join me."
  },
  {
    "objectID": "posts/skeletons-in-closet/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/skeletons-in-closet/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Skeletons in the closet",
    "section": "11 comments captured from original post on Blogger",
    "text": "11 comments captured from original post on Blogger\ndrbrocktagon said on 2012-03-06\nIf only all scientists were as willing to admit previous mistakes.I think, however, your second-to-last sentence reveals the real problem. If you perform a study badly and get \"interesting\" results, somebody somewhere will publish it (PNAS if you know the right people). Doing the study properly can make it unpublishable.\nAnders Eklund said on 2012-04-11\nEven if activity maps are corrected for multiple testing the results can be invalidhttp://www.sciencedirect.com/science/article/pii/S1053811912003825?v=s5\nUnknown said on 2013-05-17\nAs a reviewer, what is your practice in detecting wrong usage of small volume correction?\nRuss Poldrack said on 2013-05-18\nIt’s really tough to give a general rule. I feel better about it if it follows directly from previous work by the same group, such that the same area is corrected across multiple papers and there is a stronger reason to think that they set out with that specific hypothesis in mind.\nUnknown said on 2013-05-18\nI agree. Cases, like FFA to face processing, amygdala to emotion and hippocampals to memory, are widely accepted. But there are some other omnipotent regions in fMRI studies, such as the insula cortex. In addition, as we know, a researcher can always find several published papers that support what he finds in his result, when trying to do some interpretation. That is, they can always collect some papers to support their choices of regions for small volume correction.So, I wonder whether our neuroscientists can provide a list of cognition-region pairs (e.g., face recognition-FFA) that are proper to use with small volume correction. \nRuss Poldrack said on 2013-05-18\nI think a better solution would be to give people a chance to pre-register their hypotheses and ROIs prior to data collection and analysis. That would allow more flexibility for new hypotheses but would still help prevent post-hoc SVCs.\nUnknown said on 2013-05-18\nThat’s to say, researchers should publish their papers mostly based on the self-discipline. :)\nStudent said on 2016-07-21\nIn another opinion paper regarding fMRI analyses and reporting (http://www.ncbi.nlm.nih.gov/pubmed/21856431), Dr. Poldrack states “Some simple methodological improvements could make a big difference. First, the field needs to agree that inference based on uncorrected statistical results is not acceptable (cf. Bennett et al., 2009). Many researchers have digested this important fact, but it is still common to see results presented at thresholds such as uncorrected p&lt;.005. Because such uncorrected thresholds do not adapt to the data (e.g., the number of voxels tests or their spatial smoothness), they are certain to be invalid in almost every situation (potentially being either overly liberal or overly conservative).” This is a good point, but given the fact that Dr. Poldrack has published papers in high impact journals that rely heavily on inferences from data using uncorrected thresholds (e.g. http://www.ncbi.nlm.nih.gov/pubmed/16157284), and does not appear to have issued any statements to the journals regarding their validity, one wonders whether Dr. Poldrack wants to have his cake and eat it too, so to say. A similar point can be made regarding Dr. Poldrack’s attitude regarding the use of small volume correction. In this paper, he states “Second, I have become increasingly concerned about the use of “small volume corrections” to address the multiple testing problem. The use of a priori masks to constrain statistical testing is perfectly legitimate, but one often gets the feeling that the masks used for small volume correction were chosen after seeing the initial results (perhaps after a whole-brain corrected analysis was not significant). In such a case, any inferences based on these corrections are circular and the statistics are useless”. While this is also true, one wonders whether Dr. Poldrack only trusts his group to use this tool correctly, since it is frequently employed in his papers. In a third opinion paper (http://www.ncbi.nlm.nih.gov/pubmed/20571517), Dr. Poldrack discusses the problem of circularity in fMRI analyses. While this is also an important topic, Dr. Poldrack’s group has also published papers using circular analyses (e.g. http://www.jneurosci.org/content/27/14/3743.full.pdf, http://www.jneurosci.org/content/26/9/2424, http://www.ncbi.nlm.nih.gov/pubmed/17255512). \nStudent said on 2016-07-21\n(Final) I would like to note that the reason for this comment is not to malign Dr. Poldrack or his research, but rather to attempt to clarify Dr. Poldrack’s opinion of how others should view his previous research when it fails to meet the rigorous standards that he persistently endorses. I am very much in agreement with Dr. Poldrack that rigorous methodology and transparency are important foundations for building a strong science. As a graduate student, it is frustrating to see high-profile scientists such as Dr. Poldrack call for increased methodological rigor by new researchers (typically while, rightfully, labeling work that does not meet methodological standards as being unreliable) when they (1) have benefited (and arguably continue to benefit) from the relatively lower barriers to entry that come from having entered a research field before the emergence of a rigid methodological framework (i.e. in having Neuron/PNAS/Science papers on their CV that would not be published in a low-tier journal today due to their methodological problems) , and (2) not applying the same level of criticism or skepticism to their own previous work as they do to emerging work when it does not meet current standards of rigor or transparency. I would like to know what Dr. Poldrack’s opinions are on these issues. I greatly appreciate any time and/or effort spent reading and/or replying to this comment. \nRuss Poldrack said on 2016-07-21\nthanks for your comments. I will address them in an upcoming blog post.\nRuss Poldrack said on 2016-07-22\nhere is my response, thanks again for your comments: http://www.russpoldrack.org/2016/07/having-my-cake-and-eating-it-too.html"
  },
  {
    "objectID": "posts/why-i-will-be-flying-less/index.html",
    "href": "posts/why-i-will-be-flying-less/index.html",
    "title": "Why I will be flying less",
    "section": "",
    "text": "This was originally posted on blogger here.\nSince reading David Wallace Wells’“The Uninhabitable Earth: Life After Warming” earlier this year, followed by some deep discussions on the issue of climate change with my friend and colleague Adam Aron from UCSD, I no longer feel we can just sit back and hope someone else will fix the problem. And it’s becoming increasingly clear that if we as individuals want to do something about climate change, changing our travel habits is probably the single most effective action we can take. Jack Miles made this case in his recent Washington Post article, “For the love of Earth, stop traveling”: According to former U.N. climate chief Christiana Figueres, we have only three years left in which to “bend the emissions curve downward” and forestall a terrifying cascade of climate-related catastrophes, much worse than what we’re already experiencing. Realistically, is there anything that you or I can do as individuals to make a significant difference in the short time remaining?The answer is yes, and the good news is it won’t cost us a penny. It will actually save us money, and we won’t have to leave home to do it. Staying home, in fact, is the essence of making a big difference in a big hurry. That’s because nothing that we do pumps carbon dioxide into the atmosphere faster than air travel. Cancel a couple long flights, and you can halve your carbon footprint. Schedule a couple, and you can double or triple it.I travel a lot - I have almost 1.3 million lifetime miles on United Airlines, and in the last few years have regularly flown over 100,000 miles per year. This travel has definitely helped advance my scientific career, and has been in many ways deeply fulfilling and enlightening. However, the toll has been weighing on me and Miles’ article really pushed me over the edge towards action. I used the Myclimate.org carbon footprint calculator to compute the environmental impact of my flights just for the first half of 2019, and it was mind-boggling: more than 23 tons of CO2. For comparison, my entire household’s yearly carbon footprint (estimated using https://www3.epa.gov/carbon-footprint-calculator/) is just over 10 tons! For these reasons, I am committing to eliminate (to the greatest degree possible) academic air travel for the foreseeable future. That means no air travel for talks, conferences, or meetings – instead participating by telepresence whenever possible. I am in a fortunate position, as a tenured faculty member who is already well connected within my field. By taking this action, I hope to help offset the travel impact of early career researchers and researchers from the developing world for whom air travel remains essential in order to get their research known and meet fellow researchers in their field. I wish that there was a better way to help early career researchers network without air travel, but I just haven’t seen anything that works well without in-person contact. Hopefully the growing concern about conference travel will also help spur the development of more effective tools for virtual meetings. Other senior researchers who agree should join me in taking the No Fly pledge at https://noflyclimatesci.org/. You can also learn more here: https://academicflyingblog.wordpress.com/"
  },
  {
    "objectID": "posts/why-i-will-be-flying-less/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/why-i-will-be-flying-less/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Why I will be flying less",
    "section": "4 comments captured from original post on Blogger",
    "text": "4 comments captured from original post on Blogger\nusfoodpolicy said on 2019-06-28\nYou are a hero. Wishing much joy, adventure, and accomplishment to you in this time of change.\nUnknown said on 2019-06-29\nI couldn’t agree more. This will make a big difference.\nKaelaB said on 2019-07-01\nThanks for this post Russ. If it helps, there is research that shows that number of flights does not correlate with academic influence or career success https://www.sciencedirect.com/science/article/pii/S0959652619311862?dgcid=author\nCx289 said on 2020-02-06\nI have broken my record of largest carbon footprint when flying in three of the last four years. In 2016 I got 30.9t, breaking my 2012 record of 22.6t, in 2017 it was only 12.1t, but in 2018 it increased yet again to 38.7t and it reached 43t in 2019 thanks to three flights in Business Class with my family.However, 2020 may be my first no-flight year since 2009, as my only possible trip is a school trip to Japan, which is not sure that whether it may go ahead or not. If the trip does not go ahead, and we do not fly out in summer (I will not fly out in Christmas for mock revision), 2020 will be my first no-flight year in a decade."
  },
  {
    "objectID": "posts/signers-of-letter-to-editor-of-new-york/index.html",
    "href": "posts/signers-of-letter-to-editor-of-new-york/index.html",
    "title": "Signers of letter to the editor of the New York Times",
    "section": "",
    "text": "This was originally posted on blogger here.\nA letter has been submitted to the editor of the NY Times regarding the outrageous Op-Ed piece by Martin Lindstrom. (Once it is published I will add a link here.) Because the NY Times will not allow a long list of signers on a letter, I am attaching here a list of all of the individuals who contributed to and signed the letter. If you would like to add your name in support of the letter, please do so in the comments section. Russell Poldrack, Ph.D., University of Texas at AustinGeoffrey K Aguirre, M.D., Ph.D., University of PennsylvaniaAdam Aron, Ph.D., University of California at San DiegoLisa Feldman Barrett, Ph.D., Northeastern UniversityMark G. Baxter, Ph.D., Mount Sinai School of MedicineSusan Bookheimer, Ph.D., University of California at Los AngelesColin Camerer, Ph.D., California Institute of TechnologyMcKell Carter, Ph.D., Duke UniversityChristopher Chabris, Ph.D., Union CollegeMolly Crockett, Ph.D., University of Zurich, SwitzerlandNathaniel Daw, Ph.D., New York UniversityPaul Downing, Ph.D., University of Bangor, Wales, UKRussell Epstein, Ph.D., University of PennsylvaniaMichael Frank, Ph.D., Brown UniversityJanet Frick, Ph.D., University of GeorgiaPaul Glimcher, Ph.D., New York UniversityTom Hartley, Ph.D., University of York, UKBenjamin Hayden, Ph.D., University of RochesterHauke R. Heekeren, M.D., Freie Universität Berlin, GermanySimon Hjerrild, M.D., University of Aarhus, DenmarkScott Huettel, Ph.D., Duke UniversityNancy Kanwisher, Ph.D., Massachusetts Institute of TechnologyBrian Knutson, Ph.D., Stanford UniversityJohn Kubie, Ph.D., SUNY Downstate Medical CenterMichael V. Lombardo, Ph.D., University of Cambridge, UKKen Norman, Ph.D., Princeton UniversityOlivier Oullier, Ph.D., Aix-Marseille University, FranceSteven Petersen, Ph.D., Washington UniversityElizabeth Phelps, Ph.D., New York UniversityRajeev Raizada, Ph.D., Cornell UniversityAntonio Rangel, Ph.D., California Institute of TechnologyPeter B. Reiner, Ph.D., University of British Columbia, CanadaGregory Samanez-Larkin, Ph.D., Vanderbilt UniversityGeoff Schoenbaum, M.D., Ph.D., University of MarylandDaphna Shohamy, Ph.D., Columbia UniversityJon Simons, Ph.D., University of Cambridge, UKPeter Sokol-Hessner, Ph.D., California Institute of TechnologyDavid Somers, Ph.D., Boston UniversityDamian Stanley, Ph.D., California Institute of TechnologyJohn Van Horn, Ph.D., University of California at Los AngelesBradley Voytek, Ph.D., University of California, San FranciscoAnthony Wagner, PhD, Stanford University.Daniel Willingham, Ph.D., University of VirginiaTal Yarkoni, Ph.D., University of Colorado BoulderJeff Zacks, Ph.D., Washington UniversityJamil Zaki, Ph.D., Harvard University"
  },
  {
    "objectID": "posts/signers-of-letter-to-editor-of-new-york/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/signers-of-letter-to-editor-of-new-york/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Signers of letter to the editor of the New York Times",
    "section": "12 comments captured from original post on Blogger",
    "text": "12 comments captured from original post on Blogger\nChristopher Chabris said on 2011-10-03\nDoes this mean they accepted the letter, and if so, were you able to give them the revised version?\nPrefrontal said on 2011-10-03\nCompletely support the letter - Craig Bennett, Ph.D., University of California at Santa Barbara\nMartin Lindstrom said on 2011-10-04\nMy first foray into neuro-marketing research was for my New York Times bestseller Buyology: Truth and Lies about Why We Buy. For that book I teamed up with Neurosense, a leading independent neuro-marketing company that specializes in consumer research using functional magnetic resonance imaging (fMRI) headed by Oxford University trained Gemma Calvert, BSc DPhil CPsychol FRSA and Neuro-Insight, a market research company that uses unique brain-imaging technology, called Steady-State Topography (SST), to measure how the brain responds to communications which is lead by Dr. Richard Silberstein, PhD. This was the single largest neuro-marketing study ever conducted—25x larger than any such study to date and cost more than seven million dollars to run.In the three-year effort scientists scanned the brains of over 2,000 people from all over the world as they were exposed to various marketing and advertising strategies including clever product placements, sneaky subliminal messages, iconic brand logos, shocking health and safety warnings, and provocative product packages. The purpose of all of this was to understand, quite successfully I may add, the key drivers behind why we make the purchasing decisions that we do.For the research that my recent Op-Ed column in the New York Times was based on I turned to Dr. David Hubbard, a board-certified neurologist and his company MindSign Neuro Marketing, an independently owned fMRI neuro-marketing company. I asked Dr. Hubbard and his team a simple question, “Are we addicted to our iPhones?” After analyzing the brains of 8 men and 8 women between the ages of 18-25 using fMRI technology, MindSign answered my question using standardized answering methods and completely reproducible results. The conclusion was that we are not addicted to our iPhones, we are in love with them.The thought provoking dialogue that has been generated from the article has been overwhelmingly positive and I look forward to the continued comments from professionals in the field, readers and fans.Respectfully,Martin Lindstrom\nSJB said on 2011-10-04\nRuss, you should use this as a perfect example in your next paper on reverse inference. SJ Blakemore.\njvs said on 2011-10-04\nDear Mr. Lindstrom,When you make a claim about the emotional response people have to a product on the basis of fMRI data, you move beyond the purview of neuro-marketing and into the realm of scientific psychology and neuroscience. As a result, professionals in the field will evaluate your claim in the same manner that they would any other result in the field. And in this case, despite the overwhelmingly positive response you say you have received, the response of scientists who work in this field has been overwhelmingly negative, and for good reason. In addition to this post, see these other responses to your op-ed piece:http://www.talyarkoni.org/blog/2011/10/01/the-new-york-times-blows-it-big-time-on-brain-imaging/http://www.cultureofscience.com/2011/10/02/attention-new-york-times-readers-you-probably-do-not-love-your-iphone-literally/http://neurocritic.blogspot.com/It is simply not sufficient to say, as you do here, that MindSign used \"standardized answering methods and completely reproducible results\" and concluded that \"we are not addicted to our iPhones, we are in love with them.\" As scientists, we want to know how you and MindSign came to that conclusion. At present, there is no standardized, reproducible method for determining from fMRI data when people are or are not in love with a stimulus.In your op-ed piece, the only explanation you give for the conclusion that people love their iPhones is that you observed activation of insular cortex while people viewed images of an iPhone. You explain that activation of the insula is associated with feelings of love. While this is true, it is also true that activation of the insula is associated with responses to pain, feelings of disgust, and attempting to remember things over a brief delay, amongst many other things. Presumably, you would not claim that people love these things as well.In short, the reasoning that says that, \"insula activation occurs during feelings of love, therefore if the insula is activated people are experiencing love\" is deeply flawed. To illustrate why with an example, let us imagine that I presented people with pictures of their mother during fMRI imaging, and I observed activation in the fusiform gyrus (which I certainly would). I then present people with pictures of President Obama, and lo and behold I observe activation in the fusiform gyrus again. Should I then conclude that people think President Obama is their mother? Clearly not, and yet the same \"reverse inference,\" as it is called, was used to draw this conclusion and to conclude that people love their iPhones.The problem in both cases is the same: the cognitive process subserved by the activated brain region is not necessarily the one you think it is. In my example, I assumed that the cognitive process is \"mother recognition,\" when in fact research suggests that it is face recognition more generally. In your case, you assumed that the cognitive process that activated the insula was love. Unfortunately at present there is no consensus as to precisely what cognitive processes are subserved by the insula, but we can say that it is almost certainly not love, because of the wide range of situations in which the insula is activated.Respectfully,Jared Van Snellenberg\nnic said on 2011-10-04\nI support Russ’ letter, Tal’s letter, and Jared’s response to Lindstrom’s comment. Please add my name to the list! Nicole Giuliani, Ph.D., University of Oregon\natlas said on 2011-10-04\nI too support the letters to the NYTimes and agree with Russ, Tal, and Jared.Lauren Atlas, Ph.D.New York University\nleahsom said on 2011-10-04\nI too support the rebuttal to Lindstrom’s Op-Ed piece. Thank you to Russ and Tal for mobilizing the neuroscientist troops,Leah Somerville PhD, Weill Cornell Medical College\nPascal Wallisch said on 2011-10-04\nStrong agree. Didn’t see this initiative until just now.Pascal Wallisch, PhD, New York University\nJochen Weber said on 2011-10-05\nJared, I completely agree with your scientific analysis on the interpretation of the results ;) Making a claim (the 16 human subjects in the study, of which I don’t know how they were selected, love iPhones) based on reverse inference (the insular cortex shows an increase in BOLD contrast upon exposure to representative stimuli) is, well, not science, but…I think the point of the article (let’s say the two last paragraphs) was/is to raise the alarm. And while the evidence (insular cortex activation after seeing an iphone) cannot and should not be linked to the conclusion (\"we\" love iPhones), I think that it’s an interesting phenomenon that might merit some research (although using more refined protocols, if possible).Since the behavioral aspect (e.g. people reporting a feeling of \"being cut off\" after leaving their smartphone at home) was at (or closer to) the beginning of the article (introduction?), I would rather start there… I think that there is (at least anecdotal) evidence for technologies becoming more and more intricately linked with human experience (see Betsy Sparrow’s article about externalizing memories via search engines), and I would rather try to study the effect of distress upon withdrawal than that of \"gratification\" (attachment, approach, etc.) upon exposure, since that is a more salient indicator for people’s report (at least as mentioned in the article…).Just my $0.02 though :)\nMarthaG said on 2011-10-08\nI am a non-scientist who is very fond of her technology (Android-based, for the record), and I find this conversation fascinating. As a writer and editor, I had to question Mr. Lindstrom’s claim: \"It appears that a whole new generation is being primed to navigate the world of electronics in a ritualized, Apple-approved way.\" My anecdotal observation–as an aunt–is that those babies who \"swiped\" the screens of Blackberries in their ritualized way probably did some other things to them–drooling, biting,hitting, and throwing–actions that don’t so easily fit Mr. Lindstrom’s thesis. I welcome your thoughts.\nShackman said on 2011-10-11\nI support the letter and related commentary by Poldrack, Van Snellenberg & Yarkoni.Alex Shackman, Ph.D.UW-Madison"
  },
  {
    "objectID": "posts/my-analysis-of-ohbm-2011-abstracts/index.html",
    "href": "posts/my-analysis-of-ohbm-2011-abstracts/index.html",
    "title": "My analysis of OHBM 2011 abstracts",
    "section": "",
    "text": "This was originally posted on blogger here.\nAs the past chair of the Organization for Human Brain Mapping (which just ended its 2011 meeting in Quebec City), I was tasked with giving the “Meeting Highlights” talk which traditionally closes the meeting. It’s a pretty daunting challenge to summarize an entire meeting with such little time for preparation, so I took the tack of doing lots of mining on the full text of the abstracts prior to the meeting. My entire wrap-up talk is available here; below I present the main results of my text mining, along with some additional analyses that didn’t make it into the talk.Overall meeting stats: Number of Abstracts 2230 Number of Unique Authors 7622 Mean # of abstracts/author 1.64 (max=33) Mean # of authors/abstract 5.6 (max=41) Authorship distribution:The OHBM is known for being an international organization, and the authorship data confirm this. In order to visualize the authorship data, I used the Google Maps API to identify the latitude/longitude for each affiliation in the authorship list. This was successful for more than 90% of the abstracts. These latitude/longitude values were uploaded into Google Fusion Tables, from which I exported a KML file (available here) which I then opened in Google Earth. (That’s a lot of Google!)Using Google Earth I then created a tour that circled the globe, showing all of the author locations on a path from Quebec City to Beijing (location of the 2012 meeting). Here is the video:Each red pin represents the location of an author at the meeting. Authorship networks:Using the abstracts I created a coauthorship network and did some basic analyses on this network (using the Networkx toolbox in Python and the Network Workbench). The code and an anonymized version of the graph (in graphml format) are available via github. Here is an overall view of the network:This shows one giant connected component with 4600 authors (60.3%), along with a large number of much smaller components (the second largest component had 103 authors). Focusing in on the giant component, here is the spring-embedded visualization:Here are the network statistics: Clustering coefficient 0.88 Average degree 8.10 Average shortest path length (giant component only) 6.96 Maximum shortest path length 18 Modularity (giant component only) 0.92 Here is the degree distribution plotted in log space, with a degree distribution for a matched random graph for comparison:The degree distribution has a long tail compared to the random network, which is what one would expect from this kind of network (for background on this kind of analysis, see Mark Newman’s paper The structure of scientific collaboration networks). Using PageRank centrality, I identified the 10 most central authors in this network (listed with number of abstracts and centrality value):Paul Thompson (33 abstracts: 0.002020)Vince Calhoun (21 abstracts: 0.001816)Arno Villringer (23 abstracts: 0.001756)Arthur Toga (30 abstracts: 0.001625)Yong He (19 abstracts: 0.001416)Peter Fox (26 abstracts: 0.001381)Michael Milham (24 abstracts: 0.001340)Alan Evans (16 abstracts: 0.001318)Robert Turner (23 abstracts: 0.001292)Daniel Margulies (13 abstracts: 0.001194)Content analysis:Using the full text from the articles, I created several tag clouds (using Wordle) to show different aspects of the content. The first was created from the entire abstract text after filtering out standard stop words along with anatomical regions and author names. The second was created using a count of all anatomical terms (from the PubBrain anatomical lexicon):The third was created using a count of all of the terms in the Cognitive Atlaslexicon of mental concepts:These tag clouds give a good overview of the major topics at the meeting.If you have other ideas for mining of these data, let me know and I’ll give it a try. I have also done topic modeling using latent Dirichlet allocation, and may get around to writing about that in the future."
  },
  {
    "objectID": "posts/reproducibility-and-quantitative/index.html",
    "href": "posts/reproducibility-and-quantitative/index.html",
    "title": "Reproducibility and quantitative training in psychology",
    "section": "",
    "text": "This was originally posted on blogger here.\nWe had a great Town Hall Meeting of our department earlier this week, which was focused on issues around reproducibility, which Mike Frank has already discussed in his blog. A number of the questions that were raised by both faculty and graduate students centered around training, and this has gotten many of us thinking about how we should update our quantitive training to address these concerns. Currently the graduate statistics course is fairly standard, covering basic topics in probability and statistics including basic probability theory, sampling distributions, null hypothesis testing, general(ized) linear models (regression, ANOVA), and mixed models, with exercises done primarily using R. While many of these topics remain essential for psychologists and neuroscientists, it’s equally clear that there are a number of other topics that we might want to cover that are highly relevant to issues of reproducibility:the statistics of reproducibility (e.g., implications of power for predictive validity; Ioannidis, 2005)Bayesian estimation and inferencebias/variance tradeoffs and regularizationgeneralization and cross-validationmodel-fitting and model comparisonThere are also a number of topics that are clearly related to reproducibility but fall more squarely under the topic of “software hygiene”:data managementcode validation and testingversion controlreproducible workflows (e.g., virtualization/containerization)literate programmingI would love to hear your thoughts about what a 21st century graduate statistics course in psychology/neuroscience should cover- please leave comments below!"
  },
  {
    "objectID": "posts/reproducibility-and-quantitative/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/reproducibility-and-quantitative/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Reproducibility and quantitative training in psychology",
    "section": "15 comments captured from original post on Blogger",
    "text": "15 comments captured from original post on Blogger\nDr Cyril Pernet said on 2016-02-26\nLiterate programming is an interesting thing - it doesn’t need to be taught, it can be demonstrated. For instance, make a R script to demonstrate the logic of linear modeling (http://www.sbirc.ed.ac.uk/cyril/glm/GLM_lectures.html). If we start teaching such that the lectures are literate programming (rather than boring ppt), students get use to- express/convert a though in a bit of code- relates code to maths (seriously, density functions, integrals, kernels etc are easier to understand using a computer - http://www.ted.com/talks/conrad_wolfram_teaching_kids_real_math_with_computers?language=en)- become accustom to literate programmingMaybe instead of an essay, we should ask to express and solved a problem that way. I don’t need to convince you that sharing data and code is the only way .. and teaching student using literate programming make them use to do that (I think)\nSong said on 2016-02-26\nAm planning course specifically in open science/reproducible research right now at Karolinska Institutet. Have proposed as learning objectives that students should be able to: • Account for reproducibility problems in research because of issues such as small sample sizes, bias, and incomprehensive methods reporting• Recognize questionable research practices, discuss their prevalence, identify methods to discourage their use• Find and apply field-specific guidelines for reporting research with different designs• Preregister research protocols and assess others’ preregistered research protocols• Identify suitable repositories and publish scientific works using self-archiving• Annotate, describe, anonymize, and openly publish scientific data and analysis code• Find, acquire, and analyse data published openly in different formats• Integrate and accumulate data across studies (meta-analysis)*Sorry for delete and re-post, trying to sign with my name. /G Nilsonne\nChris Gorgolewski said on 2016-02-26\nWhile in grad school I tough (together with +Mike Hull) a 12 week class called \"Introduction to Research Computing\". We spoke about practical software skills such as python programming, version control, automatic figure generation, MATLAB code optimization and others.If I were to teach it now I would try to add something about reproducibility, notebooks, and pitfals of p-hacking.https://github.com/chrisfilo/IntroductionToResearchComputing\nMichael Frank said on 2016-02-26\nThis is great, Russ. I think it might be helpful to have students begin grad school with an orientation to some of the computing concepts, and then move towards methodological and statistical concepts along the way. Imagine a first-year sequence that walked students through how to set up a project and the appropriate environment for managing data/analysis/writeup, taught them how to code up the experiment and run it, and then introduced the statistical techniques necessary for analysis. This sequence would proceeding in parallel with their development of a first-year project, so they’d presumably get both course projects and research experience and have an opportunity to apply the same toolset to both…\nUnknown said on 2016-02-26\nI agree with Cyril. I see the topics listed as \"software hygiene\" as a vehicle to teaching the other topics; however, that might be a product of having to learn it on-the-fly during/post graduate training. I think putting students in practical situations is like immersion learning for these issues. \"Your regression model is highly significant? Show me it can generalize to an individual that isn’t included in the model. How? Use cross-validation. It doesn’t work? Why?\" In my view, researchers should all be knowledgeable of these techniques and issues, but many choose to collaborate with \"data scientists\" or statisticians, who have all the technical and math expertise but often know little about how to appropriately use the data. It seems clear that powerpoint-laden lectures divulging theory are not enough, but how do you determine how much programming should be taught concurrent to stats? I think the 21st century shift in data practices and communication demand these skills, but they may not belong in a graduate stats-specific course. I found that there were highly variable level of programming skill in my graduate cohort, so it would likely slow progress down quite a bit. Specifically, the statistics of reproducibility, generalization/cross-validation, model-fitting, & and model comparison would be my top votes for inclusion in a stats focused course. That being said, I think it would be a really good idea to include the programming aspects in a pre-requisite, complimentary, or supplementary type course. \nUnknown said on 2016-02-26\nI work a few of those topics into my course on regression. There are a few points in general regression topics where you can talk about bias/variance tradeoffs naturally. We also deal with differences between structural models and predictive models, and bring up how you might make different choices (and have different tests) depending on your goal. At the end, we have a whole day devoted to not making mistakes, why coding is better than spreadsheets, etc. So, I think it’s possible to fold those topics into your current courses for the most part. It just takes an effort to re-tailor those topics to practices now (vs ones our ancestors were obsessed with like sequential F tests).Updating this further based on Cyril’s point: I also have R code for every lecture that goes over multiple ways of calculating key statistics (not using lm’s output) such as using algebraic formulas, calculating solely from the variance-covariance matrix, matrix algebra, etc. This serves two goals of having them (1) learn some computational skills and (2) understand the formulas and principles by working with them. So it is definitely possible to teach almost everything off that list be folding into the current classes.Fox’s book \"Applied Regression Analysis and Generalized Linear Models\" makes the incorporation of these concepts somewhat seamless, in my opinion. His chapters on Robust Regression, Bootstrapping, and Model Selection, Averaging, and Validation are a gold mine for these basic concepts that other books don’t have (or don’t do as well)\ndavid said on 2016-02-26\nGood paper on \"software hygiene.\" Something to consider for assigned readings:http://web.stanford.edu/~gentzkow/research/CodeAndData.pdf\ntal said on 2016-02-26\nWhen I was at Colorado, I taught a graduate project-building course that amounted to something like \"scientific computing for psychologists\". I think the students liked it, and some very nice projects came out of it… but the main limitation from my perspective was the huge variability in technical ability coming into the course. Some students were basically software developers, and others had never seen for-loops before. I agree with Michael Frank that the optimal approach would require multiple courses.My preferred sequence would probably be: (i) Programming for psychologists (which students can test out of), which, in addition to standard stuff (data types, control structures, etc.), would cover programming concepts and applications relevant to psychologists (e.g., a lot of text processing, numerical computing, and stimulus presentation, not much about algorithms, systems, etc.). (ii) Scientific computing for psychologists, which would assume basic programming ability, and focus on all the stuff you’re talking about here (version control, interactive notebooks, reproducibility, etc.), and get students to a point where they’re managing all their data/analysis/code properly. Then (iii) Project-building, where students carry out technically-oriented projects that focus on their own particular area of research. The last course wouldn’t be mandatory, but perhaps strongly encouraged.Of course this leave aside the question of what to do with conventional stats and/or research methods. Having 4 required methods courses seems like a bit much, but I’m not sure how much one could really compress things down. The reality is that there’s just more to learn these days, if you don’t already have a strong technical background. (So maybe a more realistic solution is to only accept students who come in with many of these skills.)\npractiCal fMRI said on 2016-02-26\nHi Russ, where do the issues of experimental design and systematic errors come in? Power is important but won’t fix a bad study design or confounding variables.\nRuss Poldrack said on 2016-02-26\nGood point - some of the issues around study design would presumably be discussed in courses that are more area-specific, since they will often vary across different research domains, though certainly there are many issues (e.g., confounding) that are general and should probably be discussed here.\nMD said on 2016-02-26\nIn my own stats course, I’ve been covering issues related to reproducibility and questionable research practices for about 5 years. Each year, I give our incoming graduate students a questionnaire about scientific practices and the crisis of confidence. What I find most striking, is that (a) most students (~60%-80% in any given year) report that they’ve worked in labs where researchers engaged in optional stopping, flexibility of analysis, and HARKing, and (b) almost none of them have heard of the crisis of confidence or issues surrounding reproducibility prior to coming to graduate school. Graduate school shouldn’t be their first introduction. In case anyone is wondering, our students come from a variety of institutions ranging from small liberal arts colleges to Ivy league and equivalent research institutions.\nUnknown said on 2016-02-26\nNot mentioned above, but very much needed, is training in tools for building theories. Relatedly, psychology would benefit from a priori theory building (and analysis). And places to publish this sort of work. – Pat Shafto\nUnknown said on 2016-02-26\nHi Russ, interesting ideas. My own take is that the most important thing is to help students think more critically about data analysis. In a way, the goal would be to teach statistics not as a set of disparate techniques (ANOVA, regression, etc.) but as \"statistical modeling\", a way to model data so that we can understand patterns and structure in data. This is all very natural if one embraces a Bayesian mindset, and different analyses map to different ways of modeling the data. I’m reading a book right now that might be the best book ever in terms of basic exposition and didactical approach to this: http://xcelab.net/rm/statistical-rethinking/ The book is so well written that I’ve enjoyed reading even Chapter 1! It goes without saying that students need to learn to program; if not in R (it’s rather cryptic), some version of Python, or even a probabilistic language like Stan.\nBrad Wyble said on 2016-02-26\nI think it’s a shame that custom permutation tests aren’t taught routinely in grad stats. They can be written easily in any language, and are so flexible that they can sometimes find a way to test for differences that traditional approaches aren’t well suited for.\nv said on 2016-02-26\nThere should be a required entry level course for all new graduate students that covers the basics, and then a higher department (eg, research computing) should offer regular advising and office hours for all students, faculty, and staff."
  },
  {
    "objectID": "posts/to-code-or-not-to-code-in-intro/index.html",
    "href": "posts/to-code-or-not-to-code-in-intro/index.html",
    "title": "To Code or Not to Code (in intro statistics)?",
    "section": "",
    "text": "This was originally posted on blogger here.\nLast week we wrapped Stats 60/Psych 10, which was the first time I have ever taught such a course. One of the goals of the course was for the students to develop enough data analysis skill in R to be able to go off and do their own analyses, and it seems that we were fairly successful in this. To quantify our performance I used data from an entrance survey (which asked about previous programming experience) and an exit survey (which asked about self-rated R skill on a 1-7 scale). Here are the data from the exit survey, separated by whether the students had any previous programming experience:This shows us that there are now about fifty Stanford undergrads who had never programmed before and who now feel that they have at least moderate R ability (3 or above). Some comments on the survey question “What were your favorite aspects of the course?” also reflected this (these are all from people who had never programmed before):The emphasis on learning R was valuable because I feel that I’ve gained an important skill that will be useful for the rest of my college career.I feel like I learned a valuable skill on how to use RGradually learning and understanding coding syntax in RFinally getting code right in R is a very rewarding feelingSense of accomplishment I got from understanding the R material on my ownAt the same time, there was a substantial contingent of the class that did not like the coding component. This was evident to some comments on the survey question “What were your least favorite aspects of the course?”:R coding. It is super difficult to learn as a person with very little coding background, and made this class feel like it was mostly about figuring out code rather than about absorbing and learning to apply statistics.My feelings are torn on R. I understand that it’s a useful skill & plan to continue learning it after the course (yay DataCamp), but I also found it extremely frustrating & wouldn’t have sought it out to learn on my own.I had never coded before, nor have I ever taken a statistics course. For me, trying to learn these concepts together was difficult. I felt like I went into office hours for help on coding, rather than statistical concepts.One of the major challenges of the quarter system is that we only have 10 weeks to cover a substantial amount of material, which has left me asking myself whether it is worth it to teach students to analyze data in R, or whether I should instead use one of the newer open-source graphical statsitics packages, such as JASP or Jamovi. The main pro that I see of moving to a graphical package are that the students could spend more time focusing on statistical concepts, and less time trying to understand R programming constructs like pipes and ggplot aesthetics that have little to do with statistics per se. On the other hand, there are the several reasons that I decided to teach the course using R in the first place:Many of the students in the class come from humanities departments where they would likely never have a chance to learn coding. I consider computational literacy (including coding) to be essential for any student today (regardless of whether they are from sciences or the humanities), and this course provides those students with a chance to acquire at least a bit of skill and hopefully inspires curiosity to learn more.Analyzing data by pointing and clicking is inherently non-reproducible, and one of the important aspects of the course was to focus the students on the importance of reproducible research practices (e.g. by having them submit RMarkdown notebooks for the problem sets and final project). A big part of working with real data is wrangling the data into a form where the statistics can actually be applied. Without the ability to code, this becomes much more difficult.The course focuses a lot on simulation and randomization, and I’m not sure that the interactive packages will be useful for instilling these concepts.I’m interested to hear your thoughts about this tradeoff: Is it better for the students to walk away with some R skill but less conceptual statistical knowledge, or greater conceptual knowledge without the ability to implement it in code? Please leave your thoughts in the comments below."
  },
  {
    "objectID": "posts/to-code-or-not-to-code-in-intro/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/to-code-or-not-to-code-in-intro/index.html#comments-captured-from-original-post-on-blogger",
    "title": "To Code or Not to Code (in intro statistics)?",
    "section": "14 comments captured from original post on Blogger",
    "text": "14 comments captured from original post on Blogger\ndaniele said on 2018-03-25\nI would say definitely code, making it part of the learning process.Students might be scared when they see code and think they might be evaluated on coding, but unfortunately tend to discard the coding part if it’s not explicit part of the evaluation, and it’s a pity.So I would make it clear from the start that coding is part of the training, and functional to improve the learning, and to express what was learned.\nRuss Poldrack said on 2018-03-25\nthanks Daniele! That is how I tried to do it this time - though it’s clear from some of the comments that we didn’t do a good enough job integrating the coding content and statistical content.\nMr.Rocks said on 2018-03-25\n&gt; trying to learn these concepts together was difficultI don’t think teaching coding and statistics in one bundle is profitable to learn either. I totally agree that both need to be learned, and not coding these days is a kind of illiteracy. But just as I would not teach English and statistics in the same course, I wouldn’t do that to statistics. Either make basic coding skills a prerequisite for the statistics course, or teach statistic concepts in an abstract way without prerequisite and then later get to the applied stuff in a course that does require coding.By the way, I learned to code in SPSS, then R. I only started making sense of R past Stack Overflow copy/paste after I learned Python, and I only made sense of code structure after I learned Haskell. Yes, doing stats provided the motivation to learn to code, but no, R is not a good language to learn coding concepts in.\nRuss Poldrack said on 2018-03-25\nthanks for the thoughts and the interesting analogy. requiring coding as a prereq is a non-starter for this course.\nMr.Rocks said on 2018-03-25\nThen consider teaching statistics first. Learning works best when the novelty is confined to a single dimension. Or break the course into separate blocks, and teach one (either statistics or coding) in isolation first.\nUnknown said on 2018-03-25\nUnfortunately, the most positive evaluations go to easy-A classes that do not push students, down the road you’ll meet lots of students who say: that class was so hard, but I learnt statistics and how to do the  things I needed for this really cool project. You can’t judge a new car when you drive out the dealership, try asking in 3 years. …(SherlockpHolmes)\nRuss Poldrack said on 2018-03-25\nThe following thoughts were emailed by E.-J. Wagenmakers and are being posted with his consent:Dear Russ,My thoughts:1. I love R, and I have no problems whatsoever with teaching studentsR. Every student should learn how to code in R. It does seem to methat it is a separate skill from stats, but then again, if you teachstats through lots of simulations, there is no going around R.2. \"Analyzing data by pointing and clicking is inherentlynon-reproducible\" -&gt; I respectfully disagree. The JASP files store notonly the annotated output, but also store the input options that gaverise to that output. So I can send you a jasp file and you will knowexactly what I’ve done.3. For me, as an R programmer, conducting an analysis in R takesseveral orders of magnitude longer than doing it in JASP.4. With JASP I know I am not making some sort of stupid programming mistake! This means I don’t have to debug my code. This is an clear advantage of GUIs that is missing from your list. Consider advising a student. You either get an analysis output from JASP, or from the student’s own R code. Which result do you trust?5. Two years later, will the students still program in R? Or will theybe sick of having to load the package you need for an ANOVA (\"wait, we also needed to code some stuff as factors, right?\"). Or will they have forgotten most of it?6. I don’t think the choice between R and JASP is an either-or situation.7. Let me stress again how much I like R, and how much I appreciatethe courses that teach R. It is a fabulous tool and, in fact, JASPwould not have been possible without R and its packages (seehttps://jasp-stats.org/r-package-list/). Currently the JASP team isworking to achieve complete synergy with R; this is under development but we are excited about the possibilities for the future.Cheers,E.J.\nSarah Chang said on 2018-03-25\nHi there! (Disclaimer: mostly anec data to follow)As a post-bach RA in a clinical neuroscience lab aiming to pursue a PhD in Psych/Neuroscience, I wish that I had gotten coding experience during undergrad. I majored in Psychobiology (no coding required for my major) at UCLA, and I feel like I’m playing catch up since coding is pretty essential for fMRI paradigms/processing pipelines/analysis. After consulting with my roommates (both previous Econ majors who now work in the business side of tech), they believe that coding/data wrangling is also critical for success in their futures, so regardless of the field, learning R would be valuable to all of us. We’re currently enrolled in online coding classes (Python, SQL, Javascript) through UCLA’s extension program. Personally, I think that integrating stats and coding is parsimonious. I also learned SPSS first in undergrad, then R post grad. There were definitely headbanging moments while learning R, but I wish I could’ve began to learn it in my first psych stats course despite only having 10 weeks to do so.Best wishes,Sarah C.\ndeevybee said on 2018-03-25\nI strongly believe that the best way to give people an intuitive understanding of statistics is by getting them to simulate data and then run analysis. If you do this, you see a light go on as they get, for the first time, what a p-value is and why p-hacking is serious. It’s also good for getting a grip on just how seriously sample size affects power. Therefore i’d go for teaching via R, but it’s all about how you do that. I’d say that if beginners are being exposed to ggplot2 and pipes, then you’re doing it wrong. The R needs to be as simple as is compatible with the statistics you are learning, with exercises that they can work through one line at a time: then they can run the code, look at the output, tweak the code, see what happens If they are natural coders, you can point them to books that will explain how to write elegant scripts, but I think for ease of understanding, clunky scripts can be better. I’m trying to develop teaching scripts myself and always interested in feedback: see https://github.com/oscci/deevybee_repo.\nRuss Poldrack said on 2018-03-26\nThanks Dorothy! I agree regarding ggplot - its API has a seriously steep learning curve, though it is quite powerful once you learn how to use it. regarding pipes, I think there are things that are made so easy by the tidyverse (e.g. grouping and summarizing) that would be a lot more work without it. I was also swayed by the arguments here: http://varianceexplained.org/r/teach-tidyverse/\nDavid said on 2018-03-26\nI would not base any decisions on these kinds of data—even tentatively. Self-efficacy is a terrible predictor of performance (Dunning-Kruger, etc.). Both the quantitative and qualitative data reflect the impact of prior knowledge on confidence, which is a very different hypothetical than coding with R enhances statistics learning outcomes. Also, if we care about diversification of STEM fields for which statistics is an entry point, orienting a course toward coding will give an increased confidence advantage to people with prior computing backgrounds at the expense of those without. Those individuals with these backgrounds are vastly more likely to be white, male, and from upper SES ranges. Those without are the people whom we struggle to bring into computationally intensive fields.Sorry to be a downer.\nRuss Poldrack said on 2018-03-26\nThanks David, great points. Just a couple of comments:- I realize that self-rated skill is not a great predictor of actual performance, but in some ways I am just as interested in self-efficacy here as I am in actual skill. no one is going to become a highly skilled R programmer after just 10 weeks, but if they feel like they have some base skill on which they can build then they are more likely to pursue additional training in the future. - Your points about diversity are well taken, but there is another way to view it: we are offering a course that introduces coding to a set of people who otherwise would likely never have encountered it (since this class is heavily populated with individuals from arts and humanities). If we can get some of them interested in using statistics and computing in their home disciplines (which was the goal of our class project), then I would consider that a win because it would almost certainly be reaching some of those people who had self-selected against computational majors. I have to look at the bright side…\nJonathon said on 2018-03-27\nhi russ,just thought i’d comment on this:&gt; Analyzing data by pointing and clicking is inherently non-reproduciblei don’t think this is true. in jamovi all the options you use to run an analysis, the data used, and the results are all bundled together in the same file. you can click on an earlier analysis, and see the options which were used. better, you can send a file to a colleague, they can open the file, and see what options you used. this makes reproducibility so simple, and people do it without even realising.i’d also mention that jamovi includes an ‘R syntax mode’, where it will output the equivalent R syntax. this lets you copy/paste it into R studio or the like. in this way, jamovi is a great way to ease people into using R.check it outjonathon\nUnknown said on 2018-03-28\nMy first question would be demographics not only majors in Liberal Arts etc but Elective or non-elective: Grad or undergrad. What are students hoping to get out of the course. Statistics and programming (both) are complementary skills for a wide variety jobs. A self-rating might imply a correlation for what students hoped to gain from electing the course. I would also not assume SES or class as a stronger correlation for learning than choice and other motivations on what the students hope to get out of the course so offering basic skills and self-rating indicate an appreciation to both fields they may pursue further. A worthy statistical objective."
  },
  {
    "objectID": "posts/statistical-redistricting-how-to-save/index.html",
    "href": "posts/statistical-redistricting-how-to-save/index.html",
    "title": "statistical redistricting- how to save lots of time and money and get just about the same result",
    "section": "",
    "text": "This was originally posted on blogger here.\nI had promised myself that I wouldn’t blog about politics, but this is really more about statistics so I think it’s ok.David Sparks has posted an interesting piece about using statistical clustering to determine US Congressional districts (h/t R-Bloggers). He uses k-means clustering, and then analyzes the “partisanship” of the resulting districts by assuming that districts with above-median population density are Democratic and those with below-median density are Republican (I’m not sure how good an assumption that is). The result is that you get much more reasonable looking districts than the crazy ones that politicians come up with, but the partisan balance doesn’t seem to change (again, under the assumption that density=party). Here is an example of the map for Texas:This is, of course, way too reasonable to actually be put into practice."
  },
  {
    "objectID": "posts/532-days-of-self-examination/index.html",
    "href": "posts/532-days-of-self-examination/index.html",
    "title": "532 days of self-examination",
    "section": "",
    "text": "This was originally posted on blogger here.\nLast Tuesday we collected the final MRI scan and blood draw for the MyConnectome project.Here I am emerging from the scanner on March 11, after the 104th MRI scan (photo: Mei-Yen Chen)The study started on 9/25/2012 and ended on 3/11/2014, for a total of 532 days. During this time we performed 104 MRI scanning sessions and 48 blood draws. After excluding some of the data for quality control we are left with:88 resting state fMRI scans20 diffusion weighted imaging scans13 anatomical (T1/T2-weighted) scans18 breath-holding fMRI scans15 N-back task scans9 dot-motion/stop signal task scans8 object localizer scans5 language localizer scans4 spatial working memory localizer scansNow comes the fun part, which is analyzing all of these data (which, with imaging and genomics data together, comprise more than 3 TB of data). Fortunately we have the awesome computing resources of the Texas Advanced Computing Center, along with the computing resources at Washington University in St. Louis where our collaborators are also analyzing the data. We will be presenting some of the early results at the Organization for Human Brain Mapping meeting in Hamburg in June and the ICON meetingin Brisbane in July, and hope to have an initial paper submitted later this year. More soon!"
  },
  {
    "objectID": "posts/interested-in-poldrack-lab-for-graduate/index.html",
    "href": "posts/interested-in-poldrack-lab-for-graduate/index.html",
    "title": "Interested in the Poldrack Lab for graduate school?",
    "section": "",
    "text": "This was originally posted on blogger here.\nUpdates: The Poldrack Lab will be accepting new graduate students for 2022.I have instituted a policy that I will no longer meet one-on-one with potential graduate students prior to the application process to discuss potential admission into my lab, as this has the potential to exacerbate existing disparities in graduate school admissions. I am willing to meet with individuals (particularly those from from underrepresented groups) to discuss the graduate admissions process and other academic issues more generally, as time permits.This is the time of year when I start getting lots of emails asking whether I am accepting new grad students for next year. The answer is almost always going to be yes (unless I am moving, and I don’t plan on doing that again for a long time!), because I am always on the lookout for new superstars to join the lab. If you are interested, here are some thoughts and tips that I hope will help make you more informed about the process. These are completely my own opinions, and some of them may be totally inaccurate regarding other PIs or graduate programs, so please take them for what they are worth and no more.Which program should I apply to? I am affiliated with three graduate programs at Stanford: Psychology, Neuroscience, and Biomedical Informatics. In choosing a program, there are several important differences:Research: While most of these programs are fairly flexible, there are generally some expectations regarding the kind of research you will do, depending on the specific program. For example, if you joining the BMI program then your work is expected to have at least some focus on novel data analysis or informatics methods, whereas if you are joining Psychology your work is expected to make some contact with psychological function. Having said that, most of what we do in our lab could be done by a student in any of these programs.Coursework: Perhaps the biggest difference between programs is the kind of courses you are required to take. Each program has a set of core requirements. In psychology, you will take a number of core courses in different areas of psychology (cognitive, neuroscience, social, affective, developmental). In the neuroscience program you will take a set of core modules spanning different areas of neuroscience (including one on cognitive neuroscience that Justin Gardner and I teach), whereas in BMI you take core courses around informatics-related topics. In each program you will also take elective courses (often outside the department) that establish complementary core knowledge that is important for your particular research; for example, you can take courses in our world-class statistics department regardless of which program you enroll in. One way to think about this is: What do I want to learn about that is outside of my specific content area? Take a look at the core courses in each program and see which ones interest you the most.First-year experience: In Psychology, students generally jump straight into a specific lab (or a collaboration between labs), and spend their first year doing a first-year project that they present to their area meeting at the end of the year. In Neuroscience and BMI, students do rotations in multiple labs in their first year, and are expected to pick a lab by the end of their first year. Admissions: All of these programs are highly selective, but each differs in the nature of its admissions process. At one end of the spectrum is the Psychology admissions process, where initial decisions for who to interview are made by the combined faculty within each area of the department. At the other end is the Neuroscience program, where initial decisions are made by an admissions committee. As a generalization, I would say that the Psychology process is better for candidates whose interests and experience fit very closely with a specific PI or set of PIs, whereas the committee process caters towards candidates who may not have settled on a specific topic or PI.Career positioning: I think that the specific department that one graduates from matters a lot less than people think it does. For example, I have been in psychology departments that have hired people with PhDs in physics, applied mathematics, and computer science. I think that the work that you do and the skills that you acquire ultimately matter a lot more than the name of the program that is listed on your diploma. What does it take to get accepted? There are always more qualified applicants than there are spots in our graduate programs, and there is no way to guarantee admission to any particular program. On the flipside, there are also no absolute requirements: A perfect GRE score and a 4.0 GPA are great, but we look at the whole picture, and other factors can sometimes outweigh a weak GRE score or GPA. There are a few factors that are particularly important for admission to my lab:Research experience: It is very rare for someone to be accepted into any of the programs I am affiliated with at Stanford without significant research experience. Sometimes this can be obtained as an undergraduate, but more often successful applicants to our program have spent at least a year working as a research assistant in an active research laboratory. There are a couple of important reasons for this. First, we want you to understand what you are getting into; many people have rosy ideas of what it’s like to be a scientist, which can fall away pretty quickly in light of the actual experience of doing science. Spending some time in a lab helps you make sure that this is how you want to spend your life. In addition, it provides you with someone who can write a recommendation letter that speaks very directly to your potential as a researcher. Letters are a very important part of the admissions process, and the most effective letters are those that go into specific detail about your abilities, aptitude, and motivation.Technical skills: The research that we do in my lab is highly technical, requiring knowledge of computing systems, programming, and math/statistics. I would say that decent programming ability is a pretty firm prerequisite for entering my lab; once you enter the lab I want you to be able to jump directly into doing science, and this just can’t happen if you have to spend a year teaching yourself how to program from scratch. More generally, we expect you to be able to pick up new technical topics easily; I don’t expect students to necessarily show up knowing how a reinforcement learning model works, but I expect them to be able to go and figure it out on their own by reading the relevant papers and then implement it on their own. The best way to demonstrate programming ability is to show a specific project that you have worked on. This could be an open source project that you have contributed to, or a project that you did on the side for fun (for example, mine your own social media feed, or program a cognitive task and measure how your own behavior changes from day to day). If you don’t currently know how to program, see my post on learning to program from scratch, and get going!Risk taking and resilience: If we are doing interesting science then things are going to fail, and we have to learn from those failures and move on. I want to know that you are someone who is willing to go out on a limb to try something risky, and can handle the inevitable failures gracefully. Rather than seeing a statement of purpose that only lists all of your successes, I find it very useful to also know about risks you have taken (be they physical, social, or emotional), challenges you have faced, failures you have experienced, and most importantly what you learned from all of these experiences.What is your lab working on? The ongoing work in my lab is particularly broad, so if you want to be in a lab that is deeply focused on one specific question then my lab is probably not the right place for you. There are few broad questions that encompass much of the work that we are doing:How can neuroimaging inform the structure of the mind? My general approach to this question is outlined in my Annual Review chapter with Tal Yarkoni. Our ongoing work on this topic is using large-scale behavioral studies (both in-lab and online) and imaging studies to characterize the underlying structure of the concept of “self-regulation” as it is used across multiple areas of psychology. This work also ties into the Cognitive Atlas project, which aims to formally characterize the ontology of psychological functions and their relation to cognitive tasks. Much of the work in this domain is discovery-based data-driven, in the sense that we aim to discover structure using multivariate analysis techniques rather than testing specific existing theories. How do brains and behavior change over time? We are examining this at several different timescales. First, we are interested in how experience affects value-based choices, and particularly how the exertion of cognitive control or response inhibition can affect representations of value (Schonberg et al., 2014). Second, we are studying dynamic changes in both resting state and task-related functional connectivity over the seconds/minutes timescale (Shine et al, 2016), in order to relate network-level brain function to cognition. Third, we are mining the MyConnectome data and other large datasets to better understand how brain function changes over the weeks/months timescale (Shine et al, 2016, Poldrack et al., 2015). How can we make science better? Much of our current effort is centered on developing frameworks for improving the reproducibility and transparency of science. We have developed the OpenfMRI and Neurovault projects to help researchers share data, and our Center for Reproducible Neuroscienceis currently developing a next-generation platform for analysis and sharing of neuroimaging data. We have also developed the Experiment Factory infrastructure for performing large-scale online behavioral testing. We are also trying to do our best to make our own science as reproducible as possible; for example, we now pre-register all of our studies, and for discovery studies we try when possible to validate the results using a held-out validation sample.These aren’t the only topics we study, and we are always looking for new and interesting extensions to our ongoing work, so if you are interested in other topics then it’s worth inquiring to see if they would fit with the lab’s interests. At present, roughly half of the lab is engaged in basic cognitive neuroscience questions, and the other half is engaged in questions related to data analysis/sharing and open science. This can make for some interesting lab meetings, to say the least. What kind of adviser am I? Different advisers have different philosophies, and it’s important to be sure that you pick an advisor whose style is right for you. I would say that the most important characteristic of my style is that I am to foster independent thinking in my trainees. Publishing papers is important, but not as important as developing one’s ability to conceive novel and interesting questions and ask them in a rigorous way. This means that beyond the first year project, I don’t generally hand my students problems to work on; rather, I expect them to come up with their own questions, and then we work together to devise the right experiments to test them. Another important thing to know is that I try to motivate by example, rather than by command. I rarely breathe down my trainees necks about getting their work done, because I work on the assumption that they will work at least as hard as I work without prodding. On the other hand, I’m fairly hands-on in the sense that I still love to get deep in the weeds of experimental design and analysis code. I would also add that I am highly amenable to joint mentorship with other faculty.If you have further questions about our lab, please don’t hesitate to contact me by email. As noted above, I have a policy not to meet with potential graduate applicants one-on-one, but I try to do my best to answer specific questions by email about our lab’s current and future research interests."
  },
  {
    "objectID": "posts/interested-in-poldrack-lab-for-graduate/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/interested-in-poldrack-lab-for-graduate/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Interested in the Poldrack Lab for graduate school?",
    "section": "1 comments captured from original post on Blogger",
    "text": "1 comments captured from original post on Blogger\nv said on 2016-08-25\nThis is a great post, and great advice! Here is some to supplement, from the first official BMI graduate from the lab:http://vsoch.github.io/2016/poldracklab/Please reach out to me if you have any questions about BMI, or Poldracklab! Everything Russ said, +1, and Poldracklab ftw! :D"
  },
  {
    "objectID": "posts/the-dimensional-approach-to-studying/index.html",
    "href": "posts/the-dimensional-approach-to-studying/index.html",
    "title": "The dimensional approach to studying mental illness",
    "section": "",
    "text": "This was originally posted on blogger here.\nThe NIHM Director Tom Insel turned a lot of heads recently with his announcement that NIMH-funded research will no longer focus on DSM-based diagnostic categories, and instead will focus on understanding the dimensions of mental function that underlie mental health disorders and often cut across multiple diagnoses. This new focus will be centered around the Research Domain Criteria (RDoC), which comprises a set of psychological constructs that are thought to represent the most important dimensions that are relevant to mental health disorders. This is great news, as it has become clear (particularly from recent genetic research) that diagnostic categories, while being statistically reliable, do not have a great deal of biological reality. Dr. Insel lays out the problem nicely:Unlike our definitions of ischemic heart disease, lymphoma, or AIDS, the DSM diagnoses are based on a consensus about clusters of clinical symptoms, not any objective laboratory measure. In the rest of medicine, this would be equivalent to creating diagnostic systems based on the nature of chest pain or the quality of fever. Indeed, symptom-based diagnosis, once common in other areas of medicine, has been largely replaced in the past half century as we have understood that symptoms alone rarely indicate the best choice of treatment.We have long been interested in this dimensional approach; in particular, it was the basis for the Consortium for Neuropsychiatic Phenomics developed by my colleague Robert Bilder at UCLA several years ago, in which we are examining two specific domains of mental function (memory and executive function) across both healthy individuals and people with several different psychiatric diagnoses (schizophrenia, bipolar disorder, and ADHD). This work is still ongoing but should start to bear fruit in the next year as the neuroimaging and genetic data analyses are completed. It’s great news that more work like this will be funded in the future.Mining for dimensionsGenetic analyses have been able to take advantage of large numbers of subjects to identify the genetic overlap between different psychiatric diagnostic groups. Unfortunately we don’t have those kinds of datasets (yet) for neuroimaging studies. In a paper published in PLOS Computational Biology last year, we asked whether we could use the Neurosynth meta-analytic database as a proxy for such data, in order to see if we can identify how different psychiatric disorders are clustered together in terms of brain function. We first used topic modeling to identify groupings of terms related to psychiatric and neurological disorders in the full text of a set of about 4,400 published papers. In some cases these were specific to single disorders, but in many cases they reflected overlapping disorders (such as alcoholism and antisocial personality disorder, or schizophrenia and bipolar disorder). We then created “topic maps” for each of these by looking at which regions in the brain showed activity that was correlated with the presence of each topic in each paper (using the activation coordinates automatically extracted by Neurosynth). Here are some examples of these topic maps:We then were able to ask the fundamental question that is raised by the dimensional approach: How are different diagnoses related in terms of their neural underpinnings. It’s important to keep in mind that we are not directly examining data from different diagnostic groups; instead, we are examining the neuroimaging data that are associated with the presence of those diagnostic labels across papers. Nonetheless, the data have the potential to give us insight into how different disorders relate with regard to neural activity patterns observed in the literature. Here is what the clustering looked like across disorders:(the abbreviations are: APH: aphasia, DLX:dyslexia, SLI: specific language impairment, DA: drug abuse, AD:Alzheimer’s disease, DEP:depressive disorder, MDD:major depressive disorder, ANX:anxiety disorder, PAN: panic disorder, BPD: bipolar disorder, CD: conduct disorder, GAM: gambling, MD: mood disorder, PD: Parkinson’s disease, OCD: obsessive compulsive disorder, PHO: phobia, EAT: eating disorder, SZ: schizophrenia, OBE: obesity, COC: cocaine related disorder, PSY: psychotic disorder, PAR: paranoid disorder, SZTY: schizotypal personality disorder, TIC: tic disorder, ALC: alcoholism, ALX: alexia, ADD: attention deficit disorder, AMN: amnesia, AUT: autism, ASP: Asperger syndrome.)This clustering is very interesting in that it shows that there are four major branches of mental health disorders that can be identified on the basis of brain activity coordinates from published papers: language disorders (green), mood/anxiety disorders and drug abuse (orange), psychotic and externalizing disorders (yellow), and autism and memory disorders (purple). The fact that this worked with meta-analytic data gives us great hope that once it is applied to subject-level data, it will provide a great deal of power to identify the dimensions along which psychiatric disorders vary.How reliable are the dimensional descriptions?The move towards RDoC as a basis for mental health research also raises the question of reliability. The strength of the DSM was that its diagnoses were reliable, in the sense that two different psychiatrists, shown the same symptoms, are highly likely to make the same diagnosis. How will we ensure that the mappings from behavior to psychological dimensions are equally reliable? This is a problem that we have long struggled with, and in particular was a driver for the development of our Cognitive Atlas project. This project aims to provide a more formal basis for the many psychological constructs that are used by researchers but rarely defined in an explicit way. It makes a clear distinction between mental concepts (i.e. the unobservable psychological constructs that researchers want to measure) and tasks (i.e., the behavioral tests used to measure to those constructs), and most importantly, provides a way to specify the relations between those two levels. For example, here is the page representing the concept of “response inhibition”:This page provides a specific definition of the concept along with links to many different tasks that are thought to measure the concept, such as the stop signal task:Recently we added a new function to the Cognitive Atlas, called Collections. A collection is a set of concepts and relations that together form a larger theoretic framework; it can also be used to identify sets of tasks that are included together in task batteries. We have started to implement the various RDoC frameworks as Collections; for example, the Working Memory Matrix:There is much more work to do, and we hope that researchers interested in RDoC will help by adding their favorite tasks and concepts to the appropriate matrices. In the long run we are hopeful that formal frameworks like the Cognitive Atlas will come to play the same role for dimensionally-driven research that the DSM has played for diagnosis-driven research, providing a basis for reliable specification of the relations between psychological dimensions and behavioral tasks."
  },
  {
    "objectID": "posts/the-dimensional-approach-to-studying/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/the-dimensional-approach-to-studying/index.html#comments-captured-from-original-post-on-blogger",
    "title": "The dimensional approach to studying mental illness",
    "section": "3 comments captured from original post on Blogger",
    "text": "3 comments captured from original post on Blogger\nSucrelege said on 2013-12-01\nHello Dr. Poldrack,I originally came to your blog by way of reading about reverse inference, and was happy to discover many other interesting posts on here. I first heard about your work from when I was working as an RA in Jon Cohen’s lab. Agatha was a graduate student when I worked there and had accepted a job as a post doc in your lab. She had nothing but glowing comments about you. Anyway, thanks for the interesting post. Implementing the RDoC approach to mental illness is an exciting watershed moment in psychopathology research. I am curious about your thoughts on graph theory approaches to mental illness that do away with the conception of mental illness as a latent variable. Below is an article that addresses this in regard to the comorbidity of depression and anxiety:http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0027407–Jessica\nRuss Poldrack said on 2013-12-01\nThanks Jessica! I think that this is an interesting approach, though my general concern about these kinds of graph-theoretic approaches is that they don’t provide much insight about mechanism - however, they can at least give you ideas about where to start looking for mechanisms.\neat it said on 2013-12-05\nI totally agree, descriptives are not very robust without a mechanistic account."
  },
  {
    "objectID": "posts/advice-for-interdisicplinary/index.html",
    "href": "posts/advice-for-interdisicplinary/index.html",
    "title": "Advice for getting a job as an interdisciplinary researcher",
    "section": "",
    "text": "I gave a talk yesterday to the Stanford NeuroTech grad student group about the challenges of getting an academic job as an interdisciplinary researcher. Here are a few takeaways - as always, these are specific to my experience and may or may not be relevant for others, so take it for what it’s worth!\n\nThe struggle is real. There is apparently a large body of work that has examined the penalty for interdisciplinarity in hiring. There are various explanations for this - e.g.: “Extant theory suggests that candidates with an unfocused identity—those spanning different categories—suffer from a valuation penalty because evaluators are confused by their profile and concerned they lack the required skills. We argue that unfocused candidates may be penalized for another reason; they threaten established social boundaries.” https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4102384\nI have seen it in my own group. One former member of the group was unable to find an academic job despite a very strong CV and major prominence in our field. The main problem seems to be that many academic departments are like guilds, and want to hire someone who is “one of us” (aka “a real &lt;insert field here&gt;”)\n\n\n\n\none of us GIF\n\n\n\nI consider myself quite interdisciplinary at this point, but I got there the safe way: Starting as someone with bona fides in a particular field (cognitive neuroscience, which itself is interdisciplinary but was already established as a field, so I could be “one of them”), and then branching out later in my career. But for many people doing truly novel interdisciplinary work, this isn’t a viable alternative.\n\nWhat is an interdisciplinary scientist to do? Here are a few tips that come to mind:\n\nYou need laser focus on your specific topic/question. Don’t let people confuse “interdisciplinary” with “unfocused”.\nFind your people. Seek out and become deeply involved in a specific intellectual community that is most relevant to your work. Get to know others around the world in your specific area.\nYou will have to do double duty. You need to be able to talk to disciplinary specialists in each of your overlapping areas - e.g. a cognitive neuroscientist needs to be able to speak effectively to other kinds of neuroscientists and other kinds of psychologists. Go to disciplinary talks/conferences for each of your component disciplines so you can learn to speak their languages\n\nIf you are lucky enough to get a job interview:\n\nSpeak to the broadest reaches of the department and convince them that your problem is important. Your job talk needs to convey that you can speak their language, especially to the people who are somewhat outside your specific part of the discipline. Eg, if you are neuro/engineering person you need to be able to speak to basic neuroscientists and to engineers who work on very different problems from yours.\nBe ready to show how you would fit in the department.\n\n\nWhat courses could you teach?\nWho might you develop new collaborations with?\nWhat essential training opportunities could you offer their students that don’t already exist?\n\nMany of these tips are true in general, but especially important for candidates at the intersection of different disciplines. These are exactly the people we need to drive science in new and interesting directions, so I hope that we can find better ways in the future to bring them into our academic fold."
  },
  {
    "objectID": "posts/advice-for-interdisicplinary/index.html#advice-for-getting-a-job-as-a-interdisciplinary-researcher",
    "href": "posts/advice-for-interdisicplinary/index.html#advice-for-getting-a-job-as-a-interdisciplinary-researcher",
    "title": "Advice for getting a job as an interdisciplinary researcher",
    "section": "",
    "text": "I gave a talk yesterday to the Stanford NeuroTech grad student group about the challenges of getting an academic job as an interdisciplinary researcher. Here are a few takeaways - as always, these are specific to my experience and may or may not be relevant for others, so take it for what it’s worth!\n\nThe struggle is real. There is apparently a large body of work that has examined the penalty for interdisciplinarity in hiring. There are various explanations for this - e.g.: “Extant theory suggests that candidates with an unfocused identity—those spanning different categories—suffer from a valuation penalty because evaluators are confused by their profile and concerned they lack the required skills. We argue that unfocused candidates may be penalized for another reason; they threaten established social boundaries.” https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4102384\nI have seen it in my own group. One former member of the group was unable to find an academic job despite a very strong CV and major prominence in our field. The main problem seems to be that many academic departments are like guilds, and want to hire someone who is “one of us” (aka “a real &lt;insert field here&gt;”)\n\n\n\n\none of us GIF\n\n\n\nI consider myself quite interdisciplinary at this point, but I got there the safe way: Starting as someone with bona fides in a particular field (cognitive neuroscience, which itself is interdisciplinary but was already established as a field, so I could be “one of them”), and then branching out later in my career. But for many people doing truly novel interdisciplinary work, this isn’t a viable alternative.\n\nWhat is an interdisciplinary scientist to do? Here are a few tips that come to mind:\n\nYou need laser focus on your specific topic/question. Don’t let people confuse “interdisciplinary” with “unfocused”.\nFind your people. Seek out and become deeply involved in a specific intellectual community that is most relevant to your work. Get to know others around the world in your specific area.\nYou will have to do double duty. You need to be able to talk to disciplinary specialists in each of your overlapping areas - e.g. a cognitive neuroscientist needs to be able to speak effectively to other kinds of neuroscientists and other kinds of psychologists. Go to disciplinary talks/conferences for each of your component disciplines so you can learn to speak their languages\n\nIf you are lucky enough to get a job interview:\n\nSpeak to the broadest reaches of the department and convince them that your problem is important. Your job talk needs to convey that you can speak their language, especially to the people who are somewhat outside your specific part of the discipline. Eg, if you are neuro/engineering person you need to be able to speak to basic neuroscientists and to engineers who work on very different problems from yours.\nBe ready to show how you would fit in the department.\n\n\nWhat courses could you teach?\nWho might you develop new collaborations with?\nWhat essential training opportunities could you offer their students that don’t already exist?\n\nMany of these tips are true in general, but especially important for candidates at the intersection of different disciplines. These are exactly the people we need to drive science in new and interesting directions, so I hope that we can find better ways in the future to bring them into our academic fold."
  },
  {
    "objectID": "posts/Organizing-markdown-talks-github/index.html",
    "href": "posts/Organizing-markdown-talks-github/index.html",
    "title": "Organizing Markdown-based talks using Github",
    "section": "",
    "text": "In my previous post I discussed how I had moved from Keynote to a Markdown-based workflow for presentations using Quarto. Because I usually have a number of active talks at any point in time, it was important for me to figure out a good way to organize them, so I started by soliciting input through a post on Mastodon about whether to organize them within a single git repository or a per-talk repository. The guidance was mixed, and I ended up deciding to create a different repository for each talk, primarily because I was worried that a repo with many talks would become bloated and slow."
  },
  {
    "objectID": "posts/Organizing-markdown-talks-github/index.html#creating-a-talk-template",
    "href": "posts/Organizing-markdown-talks-github/index.html#creating-a-talk-template",
    "title": "Organizing Markdown-based talks using Github",
    "section": "Creating a talk template",
    "text": "Creating a talk template\nBased on the work that I did on this first talk, I created a template repository that can be used to create new talks. After marking the repository as a template, now I just select it as the template whenever I create a new talk repository in Github, and I will have all of the required pieces in place to get started.\nOverall I’m very happy with this workflow so far; I’ll continue to post as I learn more."
  },
  {
    "objectID": "posts/editing-lecture-videos-using-davinci/index.html",
    "href": "posts/editing-lecture-videos-using-davinci/index.html",
    "title": "Editing lecture videos using Davinci Resolve",
    "section": "",
    "text": "This was originally posted on blogger here.\nIn my previous post I described a simple workflow for generating lecture videos. One limitation of this workflow is that when slides are shared as a virtual background in Zoom (which I like to do in order to keep my webcam image on the screen next to the slides), the cursor is not captured in the video recording. Since I occasionally need to highlight a portion of the image, this means that I need to edit the video files to add that highlighting. To do this I decided to use DaVinci Resolve 16, which is a powerful video editing tool that is available for free. It has a bit of a learning curve, but the power appears to be well worth it; here I will show my workflow for adding annotations to a lecture video. I’m mostly doing this so that I remember how to do it next time around, but hopefully it might also be useful for others.In this example I am discussing Z-scores, and I want to highlight the location of the Z = 0 (i.e. the mean) and Z =1 (one standard deviation) on the normal distribution. After opening DaVinci Resolve, I start a new project for my video, which will open a browser for the media in my project. I then import my lecture video using File -&gt; Import File -&gt; Import Media; it will ask whether you want to change the video settings to match the current project, which I accept. The video will now appear in the media browser in the top left; drag the video onto the timeline browser in the lower leftmost portion of the screen, which will add it to your timeline. Now go to the “Edit” window by clicking the Edit button at the bottom (third icon from the left). Now you will see your video in the timeline at the bottom, along with a preview window at the top:Now let’s add our annotation. First, find the location in the video where you want to add the annotation. Then, right click in the area just to the left of the timeline, and add a new track:Now open the Effects library using the tab at the top of the screen (if it’s not already open), and navigate to the Effects panel under the Toolbox option. You should see an option for “Adjustment Clip” - grab this and drag it to the location on your video that you had identified.Then, select and right click on the Adjustment Clip that was just created, and choose “Open in Fusion Page”:This will open the Fusion editor, which is a powerful tool for all sorts of video edits. You will see a section in the bottom left showing two nodes for MediaIn1 and MediaOut1; what you need to do is add a Paint node into the line connecting those nodes, which you can do by right-clicking onto the line and Adding a Paint tool node:You will now see details about the Paint tool in the inspector to the top right. Here is perhaps the most important thing to know here: The tool that is opened by default (the “Multistroke” tool) doesn’t do what we want it to do here, which is to create a graphic that remains on the screen for the length of our Adjustment clip. To do that, select the simple “Stroke” tool, which for me is the fourth icon in the panel above the preview:You should then see a set of controls for Stroke1 in the Inspector to the top right:Choose the color for your annotation along with changing any other features of interest; I will use a red painbrush, so I click on the color chooser and pick a red color. Then you simply start painting in the preview window:Now go back to the editing window, and adjust the length of the Adjustment Clip as needed for your video. You can add as many additional annotations as needed using this same method. Occasionally I realize that I have said something incorrect in the video. Rather than re-recording or trying to edit the video itself, I simply add a title to the screen nothing that I misspoke. This is easy using the Title feature from the Effects library in the editing page:Once you are done, simply export the video using the QuickExport feature and you are ready to go!"
  },
  {
    "objectID": "posts/vacation-fun-making-traditional-texas/index.html",
    "href": "posts/vacation-fun-making-traditional-texas/index.html",
    "title": "Vacation fun- Making traditional Texas chili con carne",
    "section": "",
    "text": "This was originally posted on blogger here.\nI’m on vacation at home this week, and one afternoon when it was especially gray (because San Francisco in July) I decided to cook up some chili con carne. The recipe that I use is a modification of one that I can no longer find online, written by Reece Lagnuas back when he was a butcher in Austin. I documented this cook because the recipe is such a great rendition of the traditional Texas chili that I grew up eating that I thought it should be out there for everyone to try. And this batch turned out to be especially good! Your reward at the end of this journeyBe forewarned, this dish requires a pretty substantial time commitment - from start of prep until the dish was cooking it took me about 90 minutes. Once it’s cooking you just need to check it occasionally to make sure it’s simmering and not cooking too hard - it should be ready to eat within 2-3 hours. Perfect activity for a cool, gray vacation afternoon!Also - I’ve never written a recipe before, I apologize in advance for how verbose it is…Ingredients:Meat: I hope it goes without saying that you should only cook with humanely raised meat. The meat for this cook came from our neighborhood butcher shop, Avedanos, which supports local family farms). ~ 2.5 pounds pork shoulder ~ 2.5 pounds brisket (preferablly from the fattier end, known variously as the point or deckle) if you have your own meat grinder then buy them whole, otherwise ask your butcher to grind them as coarsely as possible 1 large onion - diced relatively smallfresh chiles 2 red bell peppers 2 poblano peppers 2 large jalapeno peppers dried chiles - I use a varying mix, this time it was: 2 chile ancho 3 dried pasilla 2 chile guajillo seasonings: you can mix all of these together as they will be added at the same timesalt (start with 1 tbs, we like it salty so usually add more to taste later in the cook) ground black pepper (1 tsp) cayenne pepper (if you want it spicy - for this cook, I added about 1/3 tsp of Penzey’s Black & Red which is a mix of black and cayenne pepper - the end result had just a very tiny bit of spicy kick) Chili Powder (3 tbs) Ground cumin seed (2 tsp) Garlic powder (1 tbs) Onion Powder (1 tbs) Steps:Roast the fresh peppers. The goal here is to char the skins so that they come off easily after steaming. I used my outdoor gas grill, but you can also do this directly over the burner of a gas range. If you don’t have gas then it sounds like you can also use an electric range or toaster oven. You want the skins to be charred black over as much of the pepper as possible, so you will need to turn them regularly; the larger peppers will probably take much longer than the small ones. Once they are nicely charred, then put them in a loosely sealed container to steam for at least 20 minutes.Roasting the fresh chiles on the backyard gas grillRoast and rehydrate the dried chiles. This will require a hot pan (I used the same Dutch oven that I will use to cook the chili) and about a quart of boiling water. Heat the pan on high and toss in the chiles, turning them regularly to prevent burning. When they start to smell roasty, place them in a heatproof bowl for soaking. Before you soak them, use some scissors to cut small holes in the side of each chili - this will make it easier to get any air out and submerge the chiles fully. After cutting the holes, pour the boiling water over the chilis. Roasting the dried chilesPrepare the meat. If your meat was ground by your butcher then you can skip this step. I like to grind the meat myself, since butchers often need time to set up their grinder for a coarse grind. I use the meat grinder attachment for our KitchenAid mixer. When grinding meat, it’s important for both the meat and grinder to be as cold as possible, so I put both of them in the freezer for about an hour before grinding the meat. Chop the meat into strips or chunks that are small enough to fit in the grinder feed; I like to leave most of the fat on and remove it later during the cook, but sometimes I will trim away large fat pieces.Action shot - grinding the brisketClean and chop the chiles. Remove the skins from the fresh chilis (they should come off easily after steaming), and also remove the stem, seeds, and membranes inside the chili. Don’t wash them! For the dried chiles, try to remove as much of the seeds and membrane as possible (don’t worry about the skins). Then chop them until they are nearing the consistency of a paste; this generally takes a lot of work.Dried chiles after roastingAnother action shot - chopping chilesTime to start cooking! Add about 2 Tbs of oil to the large pot, and cook the onions on relatively high heat until they are just starting to brown, stirring constanly.Blooming the spices - the smell is amazingAdd the spice mixture once the onions are starting to brown, and stir constantly for a minute or two. You should smell the spices bloom, especially the cumin seed.Browning the meat. After blooming the spices, add the meat and cook for several minutes until it is starting to brown. You should be able to smell the meat browning and start to see fat from the meat rendering out in the pan. Add the chili paste and mix into the meat. Then add just enough water to cover the meat; for this cook it was about 6 cups.About 3 hours in - almost done!Bring to a boil and then reduce to a simmer. The chili will then cook for at least 2 hours and preferably 3 or more hours; for this cook, it went a bit more than 3 hours.Skim extra grease. A couple of hours into the cook, there will likely be a substantial amount of grease on the top of the chili. I like to remove some of this before serving, so that the chili isn’t too greasy. There are probably fancy ways to do this, but I simply use a Chinese soup spoon to skim the fat off of the top. This time around I ended removing about 1.5 cups of fat.When you are ready to eat, taste the chili and add salt as needed to taste.Enjoy! I don’t generally like adulterating my chili with any additions, but this time I tried it with a bit of guacamole on the side, and it was really good.This recipe makes a lot of food — we usually have enough left over from this recipe for two additional meals (for two people). The chili keeps well in the freezer for at least a month, though it rarely lasts that long around here… This work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/advice-for-learning-to-code-from-scratch/index.html",
    "href": "posts/advice-for-learning-to-code-from-scratch/index.html",
    "title": "Advice for learning to code from scratch",
    "section": "",
    "text": "This was originally posted on blogger here.\nI met this week with a psychology student who was interested in learning to code but had absolutely no experience. I personally think it’s a travesty that programming is not part of the basic psychology curriculum, because doing novel and interesting research in psychology increasingly requires the ability to collect and work with large datasets and build new analysis tools, which are almost impossible without solid coding skills. Because it’s been a while since I learned to code (back when programs were stored on cassette tapes), I decided to ask my friends on the interwebs for some suggestions. I got some really great feedback, which I thought I would synthesize for others who might be in the same boat. Some of the big questions that one should probably answer before getting started are:Why do you want to learn to code? For most people who land in my office, it’s because they want to be able to analyze and wrangle data, run simulations, implement computational models, or create experiments to collect data. How do you learn best? I can’t stand watching videos, but some people swear by them. Some people like to just jump in and start doing, whereas others like to learn the concepts and theories first. Different strokes…What language should you start with? This is the stuff of religious wars. What’s important to realize, though, is that learning to program is not the same as learning to use a specific language. Programming is about how to think algorithmically to solve problems; the specific language is just an expression of that thinking. That said, languages differ in lots of ways, and some are more useful than others for particular purposes. My feeling is that one should start by learning a first-class language, because it will be easier to learn good practices that are more general. Your choice of a general purpose language should probably be driven by the field you are in; neuroscientists are increasingly turning to Python, whereas in genomics it seems that Java is very popular. I personally think that Python offers a nice mix of power and usability, and it’s the language that I encourage everyone to start with. However, if all you care about doing it performing statistical analyses, then learning R might be your first choice, whereas if you just want to build experiments for mTurk, then Javascript might be the answer. There may be some problem for which MATLAB is the right answer, but I’m no longer sure what it is. A caveat to all of this is that if you have friends or colleagues who are programming, then you should strongly consider using whatever language they are using, because they will be your best source of help.What problem do you want to solve? Some people can learn for the sake of learning, but I find that I need a problem in order to keep me motivated. I would recommend thinking of a relevant problem that you want to solve and then targeting your learning towards that problem. One good general strategy is to find a paper in your area of research interest, and try to implement their analysis. Another (suggested by Christina van Heer) is to take some data output from an experiment (e.g. in an Excel file), read it in, and compute some basic statistics. If you don’t have your own data, another alternative is to take a large open dataset (such as health data from NHANES or an openfmri dataset from openfmri.org ) and try to wrangle the data into a format that lets you ask an interesting question.OK then, so where do you look for help in getting started?The overwhelming favorite in my social media poll was codeacademy. It offers interactive exercises in lots of different languages, including Python. Another Pythonic suggestion was http://learnpythonthehardway.org/book/ which looks quite good. For those of you who prefer video courses, there were also a number of votes for online courses, including those from Coursera:“Python for Everybody”“An Introduction to Interactive Programming with Python”EdX:Introduction to R for data scienceHarvard’s Introduction to Computer ScienceAnd FutureLearn:Learn to Code for Data AnalysisIf you like video courses then these would be a good option. Other suggestions included:Learning statistics with R: A tutorial for psychologists and other beginners(free pdf book)[http://pythontutor.com](http://pythontutor.com/)http://pythontutor.com - this looks like a pretty cool tool to help see what a program is doing when it runshttp://tryr.codeschool.comcodemonkey (coding in a game environment)Swirl for RPython for Vision ResearchProgramming for Psychology in PythonDatacamplynda.com (commercial, but offers a free trial) Here are some suggested sites with various potentially useful tipsResources for Learning to Program using Python and Code Experiments Using Psychopy Reading Python source code to improve programming skillsIntroduction to R6 inspiring web sites that teach you to codeKendrick Kay’s course on Statistics and Data Analysis in MATLABSeveral people recommended Spyder as a development environment for Python (though I gave up on it because I found it to be too sluggish on the Mac)Here is a nice curated list of python tutorials for data science and machine learning9 places you can learn how to code (for free)Finally, it’s also worth keeping an eye out for local Software Carpentry workshops.If you have additional suggestions, please leave them in the comments!"
  },
  {
    "objectID": "posts/advice-for-learning-to-code-from-scratch/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/advice-for-learning-to-code-from-scratch/index.html#comments-captured-from-original-post-on-blogger",
    "title": "Advice for learning to code from scratch",
    "section": "8 comments captured from original post on Blogger",
    "text": "8 comments captured from original post on Blogger\nBreeding in Captivity said on 2016-05-20\nFor Matlab (I know, soooo old-fashioned) the easiest way to start is probably Geoff and my bookhttp://www.amazon.com/Matlab-Behavioral-Sciences-Program-Experiment/dp/0195320689/ref=sr_1_1?ie=UTF8&qid=1463778426&sr=8-1&keywords=matlab+for+the+behavioral+sciences\nJoe Orr said on 2016-05-20\nDo you have any thoughts/ recommendations on how to establish coding in the psych undergraduate curriculum?\nUnknown said on 2016-05-20\nSurely C/C++ should be mentioned? They’re hard, yes, but my God will you learn a lot.\nLeonardo said on 2016-05-22\nSure, but where to stop? Why not recommend some Lisp or Haskell? C was the first programming language I learned, but I feel its current relevance is only for performance optimization, not for science.\nUnknown said on 2016-05-24\nI also recommend the series of books by Allen Downey (e.g. Think Python), available free online at http://greenteapress.com/wp/\nUnknown said on 2016-05-31\nLate to the game here but PyQuick from Google was a great way for me to get started with Python.https://developers.google.com/edu/python/introduction\nUnknown said on 2016-07-17\nNice, I really like DJ Mannions lectures. They are really good for a Psychologist wanting to learn programming. Personally, I think that if you manage to learn Python, which is a quite easy language to learn, you can learn more advanced languages later. I also would like to add a very recent guide on how to use Python programming in Psychology. I came across this post Python programming in Psychology. In that post you get to learn how to use Python from creating your experiment to visualizing and analysing collected data.\nNickole Dinardo said on 2016-10-29\nNo doubt it’s impressive post."
  },
  {
    "objectID": "posts/2011-in-review/index.html",
    "href": "posts/2011-in-review/index.html",
    "title": "2011 in review",
    "section": "",
    "text": "This was originally posted on blogger here.\nIn the spirit of Chris Guillebeau’s Annual Review, I decided to take a few minutes this morning to review what worked well and what didn’t 2011 and look forward to 2012.Goals:Last year I set three goals for 2011. Here’s how they fared.FAIL: Work toward a travel moratorium for 2012. This started with me responding to all requests with “I’m sorry, but I’m not traveling at all for work in 2012.” At some point that became untenable, and the floodgates opened. At this point, I am still planning to travel less in 2012 than in 2011, but will still probably take 8-10 trips.SUCCESS: Improve climbing skills well enough to lead climb. My climbing skills have improved enormously over the last year, and in the summer I began lead climbing at the rock gym, where I can now lead a number of routes in the 5.9 range. I was not able to lead outside, mostly because the weather in December did not cooperate, but I plan to do so very soon.SUCCESS: No new web projects. I only purchased one new domain name this year, and that was for a project that had been hatched in 2010. We have instead focused heavily on our existing projects, particularly openfmri.org. Here are my goals for 2012:Improve my posture. Some nagging neck and back issues this year have highlighted the need to improve my posture. Who knows, I might even get some mental benefits from it as well.Improve my code management. In the last year I have started integrating source code management (using git) into my workflow (see my github repo for a tour of some of my adventures during the last year). However, it still has not become a habit for me during everyday coding. Exercise on every trip. One of the reasons that I find travel so disruptive is that it interferes with my fitness routine. I carry my yoga mat on nearly every trip, and this year I did a fairly good job of exercising while on the road, but I was not very consistent. Next year I plan to make sure that I get some exercise on every trip, even if it’s just some burpees and squats in the hotel room. I hope it’s not true that making these goals public will make them harder to achieve!StatsCountries visited: 7Miles flown: 76,162Talks given: 12Papers published: 14Grants funded: 2Property crimes (committed against me, not by me): 2Best meals:1. Tasting menu at Congress2. Lunch at Les Arcenuax, Marseille3. Tie between Franklin BBQ and JMueller BBQ4. Tasting menu at Uchi (the meal that sealed our transition to full-blown carnivores)"
  },
  {
    "objectID": "posts/2011-in-review/index.html#comments-captured-from-original-post-on-blogger",
    "href": "posts/2011-in-review/index.html#comments-captured-from-original-post-on-blogger",
    "title": "2011 in review",
    "section": "3 comments captured from original post on Blogger",
    "text": "3 comments captured from original post on Blogger\nJanet said on 2011-12-31\nGreat post. Do you think a standing desk might help with the posture issues, since (i presume) so much of your day is spent at a keyboard?\nRuss Poldrack said on 2011-12-31\nprobably would help - In fact, I have started standing at work whenever I can (e.g. conference calls).\nAce said on 2011-12-31\nSaddle chairs are a good way to improve posture & ergonomics. pricey but a good investment given time spent on computer: http://www.haginc.com/products/hag-capisco/"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "This was originally posted on blogger here.\nI’ve set up this personal site so I will have a place to spout off about the various things that I think about that don’t fall under the purview of the my Huffington Post blog (which is focused on implications of research for daily living) or the Cognitive Atlas blog (which is focused on topics related to our Cognitive Atlas project). I will probably focus mostly on science issues, productivity tools and workflows, and food and travel."
  },
  {
    "objectID": "posts/quitting-cable/index.html",
    "href": "posts/quitting-cable/index.html",
    "title": "Quitting cable",
    "section": "",
    "text": "This was originally posted on blogger here.\nI was inspired by Nathan’s post at Flowing Data to say a bit about how our experiment with giving up cable TV is going. Back in September, we turned off our U-Verse subscription, sent back the DVR, and started getting our TV solely from the computer (we use a Mac Mini as our media center PC). Here is our experience so far:We watch a lot less TV. Our TV routine has now morphed from watching 2-3 hours per night into watching a single show every night (recent favorites are The Layover and Top Chef, with Colbert Report as our fallback). In its place we are reading a lot more; in fact, much of the money that we are saving on cable is probably flowing to the Kindle store at Amazon. However, we have also recently started using the Austin Public Library’s ebook lending service which is a great way to save on ebooks. I’ve also been playing the guitar more often.Sometimes you really want live TV. The one problem with getting everything from the web is that it’s often hard to find a good live stream; we had this problem on new year’s eve. To solve this, I recently installed a solution to allow us to view live broadcast TV from the computer, using anElgato EyeTV One Computer TV Tunerwith a Mohu Leaf HDTV antenna. With this slick combination we are able to get 13 channels of over-the-air HDTV for free. The EyeTV software is really nice; it has good DVR functionality and an integrated TV Guide. It’s very much like having cable with 13 channels, except that the DVR functions are much better than any set-top DVR we ever had.Hulu Plus &gt; Netflix. We have found that Hulu Plus meets our TV viewing needs quite well. Sure we have to watch some commercials, but we are usually able to get new shows the next day after they air, and the selection is pretty good. I tried a free trial of Netflix online, but we have not found that it has much to offer us, except for an occasional movie. However, we watch movies pretty rarely, and so it probably makes more sense for us to just buy them from iTunes. For shows that are not available on the web or via Hulu (e.g., The Layover), we buy them from iTunes as well. It’s not cheap but we still come out ahead in the long run. Media center software sucks. We tried using both Plex and Boxee on the mac mini, but gave up on both after too many things just didn’t work; in particular, the Hulu integration on Plex was really frustrating, as it seems like it should work but then it never quite does. Now we just watch Hulu content through a web browser, live/recorded TV through EyeTV, and iTunes content through iTunes. The main drawback of this setup is that we can’t get remote functionality that works seamlessly across all these different interfaces, but that’s not been a problem.Overall I would rate this experiment as a success and would definitely recommend giving up cable."
  },
  {
    "objectID": "older/index.html",
    "href": "older/index.html",
    "title": "Russ Poldrack's blog",
    "section": "",
    "text": "I’m Russ Poldrack, Professor of Psychology at Stanford. This is where I blog about various things: neuroscience, psychology, reproducibility and open science, data science, and life."
  }
]